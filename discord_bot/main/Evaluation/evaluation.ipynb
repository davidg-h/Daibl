{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einleitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Notebook vergleichen wir verschiedene Ansätze, um sicherzustellen, dass unser Bot die am besten passenden Antworten generiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die 5 Fragen, die verglichen sollen folgende Eigenschaften besitzen:\n",
    "- Die Fragen sollen Fakultät übergreifend sein\n",
    "- Die Fragen sollen TH-spezifisch sein\n",
    "- Die Fragen sollen TH-Intranet-spezifisch sein\n",
    "- Die Fragen sollen Realitätsnah sein\n",
    "\n",
    "Die 5 Fragen, die verglichen werden sind:\n",
    "\n",
    "1. Wie ist die Email Adresse von Professor Gallwitz?\n",
    "2. Was soll ich beachten, wenn ich eine Prüfung anmelden will?\n",
    "3. Welche voraussetzungen, muss ich für den Mater Studiengang erfüllen?\n",
    "4. Welche Professoren gibt es an der Fakultät Soziale Arbeit?\n",
    "5. Wann und was muss ich im IT-Projekt machen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Model: Vicuna groß"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun werden fogende Parameter verglichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| LLM Modell    | Kontext Variation | Word Embedding Model | Frage 1 | Antwort 1 | Frage 2 | Antwort 2 | Frage 3 | Antwort 3 | Frage 4 | Antwort 4 | Frage 5 | Antwort 5 | Durchschnitt |\n",
    "|---------------|-------------------|----------------------|---------|-----------|---------|-----------|---------|-----------|---------|-----------|---------|-----------|--------------|\n",
    "| Vicuna groß   | 1 Dokument        | a(MiniLM)            | 2       |           | 4       |           | 5       |           | 8       |           | 7       |           | 5.2          |\n",
    "| Vicuna groß   | 1 Dokument        | b(xy)                 | 10      |           | 10      |           | 10      |           | 10      |           | 10      |           | 10           |\n",
    "| Vicuna groß   | 1 Dokument        | c(TF-IDF)             |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 1 Dokument        | d(MiniLM*TF-IDF)      |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 1 Dokument        | e(xy*TF-IDF)          |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 5 Dokumente       | a                     |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 5 Dokumente       | b                     |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 5 Dokumente       | c                     |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 5 Dokumente       | d                     |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 5 Dokumente       | e                     |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 10 Dokumente      | a                     |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 10 Dokumente      | b                     |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 10 Dokumente      | c                     |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 10 Dokumente      | d                     |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 10 Dokumente      | e                     |         |           |         |           |         |           |         |           |         |           | 0            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun soll ein script ersteltt werden, der die Antwort Spalten generiert.\n",
    "Nacher werden die Antworten manuell bewertet von allen Projekt Teilnehmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lizab\\projects\\daibl-1\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "from scrap.query_crafter import construct_prompt\n",
    "from scrap.embedding_algorithms.tdIdfDistance import get_most_similar_articles_tf_idf\n",
    "from scrap.embedding_algorithms.question_embedding_MiniLM import get_most_similar_articles_MiniLM\n",
    "from LLM.ServerCommunicator import server_get_answer\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from transformers import pipeline # loading of hf LLMs\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_articles_for_specified_embediing_model(embedding_model,question,document_amount):\n",
    "    \n",
    "    embeddings_model_dict = {\n",
    "        'MiniLM': get_most_similar_articles_MiniLM,\n",
    "        'TF-IDF': get_most_similar_articles_tf_idf\n",
    "    }\n",
    "     \n",
    "    get_most_similar_articles = embeddings_model_dict.get(embedding_model)\n",
    "    result = get_most_similar_articles(question,document_amount)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def return_prompt_anwser(model, query_prompt):\n",
    "    \"\"\" process and return answer of LLM \"\"\"\n",
    "    \n",
    "    answer = model(query_prompt, do_sample=True)\n",
    "    return answer[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_answer_from_model(model_id, query_prompt,model):\n",
    "    if model_id == 'vicuna_70b':\n",
    "        answer = server_get_answer(query_prompt)\n",
    "    elif model_id == 'vicuna_13b': \n",
    "        answer = return_prompt_answer(model,query_prompt)\n",
    "    elif model_id  == 'meta-llama/Llama-2-13b-chat-hf':\n",
    "        answer = return_prompt_answer(model,query_prompt)\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to forward the port for the vicuna_70b model.\n",
    "To do so, copy the template into the terminal:  ssh -N -L localhost:8080:localhost:8087 <musterm123>@141.75.89.6 and enter your password afterwards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_response(model_id,document_amount,embedding_model,question,model):\n",
    "    anwser=\"\"\n",
    "    documents = get_most_similar_articles_for_specified_embediing_model(embedding_model,question,document_amount)\n",
    "    query_prompt = construct_prompt(documents, question) \n",
    "    anwser = await get_anwser_from_model(model_id,query_prompt,model)\n",
    "    \n",
    "    return anwser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await generate_response(\"vicuna_70b\",1,\"MiniLM\",\"Wie ist die Email Adresse von Professor Gallwitz?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Geben Sie den Dateipfad zu Ihrer Excel-Datei an\n",
    "excel_file_path = 'C:\\\\Users\\\\lizab\\\\Desktop\\\\TH\\\\Semester6\\\\ItProjekt\\\\Evaluation.xlsx'\n",
    "\n",
    "# Laden Sie die Excel-Datei in einen Pandas DataFrame\n",
    "df = pd.read_excel(excel_file_path, sheet_name='Sheet1')\n",
    "\n",
    "# Modifizieren Sie die Zellen nach Bedarf\n",
    "# Beispiel: Setzen Sie den Wert 42 in die Zelle in der ersten Zeile und ersten Spalte\n",
    "df.at[0, 'Spaltenname'] = \"halloichbins\"\n",
    "\n",
    "# Speichern Sie die Änderungen zurück in die Excel-Datei\n",
    "df.to_excel(excel_file_path, sheet_name='Sheet1', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_load(model_id):\n",
    "  HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "  login(token=HUGGINGFACEHUB_API_TOKEN)\n",
    "  device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "  model = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto',\n",
    "    temperature=0.3,\n",
    "    top_p=0.15,\n",
    "    top_k=15,\n",
    "    repetition_penalty=1.1,\n",
    "    num_return_sequences=1,\n",
    "    max_new_tokens=128,\n",
    "    #max_length=256,\n",
    "  )\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Erstellen Sie eine leere DataFrame\n",
    "columns = [\"Model\", \"Document Amount\", \"Embeddings Model\", \"Question\", \"Response\"]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Ihre Daten\n",
    "model_ids = [\"vicuna_70b\",\"vicuna_13b\",\"meta-llama/Llama-2-13b-chat-hf\"]\n",
    "document_amounts = [1, 5, 10]\n",
    "embeddings_models=[\"MiniLM\",\"TF-IDF\"]\n",
    "questions=[\"Wie ist die Email Adresse von Professor Gallwitz?\",\n",
    "           \"Was soll ich beachten, wenn ich eine Prüfung anmelden will?\",\n",
    "           \"Welche voraussetzungen, muss ich für den Master Studiengang erfüllen?\",\n",
    "           \"Welche Professoren gibt es an der Fakultät Soziale Arbeit?\",\n",
    "           \"Wann und was muss ich im IT-Projekt machen?\"]\n",
    "\n",
    "data = []\n",
    "model= None\n",
    "for model_id in model_ids:\n",
    "    if(model_id != \"vicuna_70b\"):\n",
    "       model= model_load(model_id)\n",
    "    for document_amount in document_amounts:\n",
    "        for embeddings_model in embeddings_models:\n",
    "            for question in questions:\n",
    "                response = await generate_response(model_id, document_amount, embeddings_model, question,model)\n",
    "                data.append([model_id, document_amount, embeddings_model, question, response])\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame(data, columns=columns)], ignore_index=True)\n",
    "\n",
    "# Speichern Sie die DataFrame in eine Excel-Datei\n",
    "df.to_excel('output.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_unload(model):\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"meta-llama/Llama-2-13b-chat-hf\"\n",
    "model_id=model_load(model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
