{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the BERT Model works\n",
    "\n",
    "This Notebook trys to explain in detail, how the BERT Model works and how we use it to calculate our embeddings. It heavily debends on this Notebook (https://colab.research.google.com/drive/1yFphU6PW9Uo6lmDly_ud9a6c4RCYlwdX)\n",
    "First we import the modules required. You can safely ignore the warning stating \"Some weights of ..\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import warnings\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True) \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Start nehmen wir uns einen Beispielsatz und zerlegen ihn in Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in der bi ##bl ##iot ##he ##k gi ##bt es 40 bu ##cher zu ##m them ##a anime ##s\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"In der Bibliothek gibt es 40 Bücher zum Thema Animes\"\n",
    "test_sentence_tokens = tokenizer.tokenize(test_sentence)\n",
    "print(*test_sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes fügen wir die obligatorischen [CLS] und [SEP] tokens an den Anfang und das Ende an und berechnen dann die Token-IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 101, 1999, 4315, 12170, 16558, 25185, 5369, 2243, 21025, 19279, 9686, 2871, 20934, 7474, 16950, 2213, 2068, 2050, 8750, 2015, 102, 102]\n"
     ]
    }
   ],
   "source": [
    "test_sentence_tokens = ['[CLS]'] + test_sentence_tokens + ['[SEP]']\n",
    "attention_mask = [1 if token != \"[PAD]\" else 0  for token in test_sentence_tokens]\n",
    "token_ids = tokenizer.convert_tokens_to_ids(test_sentence_tokens)\n",
    "print(token_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann nehmen wir die Token IDs und füttern Sie dem BERT Model als Input. Das BERT Model ist ein dichtes neuronales Netzwerk mit 12 Schichten. Jede Schicht besteht dabei aus 768 verschiedenen Neuronen, die alle ein verschiedenen Wert annehmen können.\n",
    "\n",
    "Wir benutzen hier auch die python Bibliothek torch. Diese bietet viele Möglichkeiten, Maschine Learning funktionalitäten einfach umzusetzen. In diesem Fall benutzen wir Tensoren. Tensoren kann man sich vorstellen als Listen von Listen. Eine Liste (=Array) ist eine eindimensionale Struktur. Wenn jedes Element der Liste eine weitere Liste beinhaltet, nennt man dies Matrix. Eine Matrix hat 2 Dimensionen. Wenn dann jedes Element dieser Matrix nochmal eine Liste ist, nennt man dies Tensor. Tensoren sind dann ein Überbegriff für n-dimensionale Datenstrukturen.\n",
    "\n",
    "In diesem Fall erstellen wir einen Tensor aus einer Liste (token_ids). Der Tensor hat  die Dimension (len(token_ids),). Mit der funktion unsqueeze() können Wir einfach eine Dimension hinzufügen. Aus der Liste token_ids, wird eine Liste, die als erstes Element die Liste token_ids beinhaltet. Die Dimension ist jetzt (1, len(token_ids)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "attention_mask = torch.tensor(attention_mask).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 23, 768])\n"
     ]
    }
   ],
   "source": [
    "output = model(token_ids, attention_mask=attention_mask)\n",
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Output Variable speichern wir nun den output des BERT Models. Dieser Output ist sehr vielschichtig, also analysieren wir ihn etwas. \n",
    "Der Output besteht zunöchst aus 3 Elementen:\n",
    "* output[0] - torch.Size([1, 31, 768])\n",
    "* output[1] - torch.Size([1, 768])\n",
    "* output[2] - list(torch.Size([1, 41, 768]))\n",
    "\n",
    "Die ersten 2 Outputs stehen für die Ergebnisse des BERT Models die Default als Entwerte der letzten schicht (TODO überprüfen) stehen.\n",
    "\n",
    "output[2] ist für die Berechnung der Embeddings jedoch am wichtigsten. Hierbei handelt es sich um die Hidden-States des Neuronalen Netzwerkes. \n",
    "\n",
    "output[2] besteht aus einer Liste mit 13 Elementen - 12 layer plus inout encoding\n",
    "für jedes dieser Layer ist ein Tensor mit den Dimensionen (batch_size, Tokens, hidden_states) gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 1\n",
      "Number of tokens: 41\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "hidden_states = output[2]\n",
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das sind 409 344 verschiedene Werte um einen einzigen Satz zu repräsentieren!!!\n",
    "Wir können uns an dieser Stelle auch einmal die Verteilung der hidden Werte in einem Bestimmten Layer für ein bestimmtes Token anschauen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzYAAAFfCAYAAACcK1n6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAhT0lEQVR4nO3df5BV5WE//vcisqCwi6AsUhdEYwVrNBYUV22rhkoZdGQktrEmRetoTVdbXVvLViPqaLDaqtH4uxa1LaPVjFp/RMeiwbSCPzBmBEeijQxU3DVG2VVaF5T7/SMf75dVVC7scjnL6zVzZrjnnHvuex9Xdt889zy3plQqlQIAAFBg/aodAAAAYEspNgAAQOEpNgAAQOEpNgAAQOEpNgAAQOEpNgAAQOEpNgAAQOH1r3aAT1u/fn1WrVqVIUOGpKamptpxAACAKimVSnn//fczatSo9Ov3xXMy21yxWbVqVRobG6sdAwAA2EasXLkye+yxxxees80VmyFDhiT5dfi6uroqpwEAAKqls7MzjY2N5Y7wRba5YvPJ28/q6uoUGwAAYJNuUbF4AAAAUHiKDQAAUHiKDQAAUHiKDQAAUHiKDQAAUHiKDQAAUHiKDQAAUHiKDQAAUHiKDQAAUHiKDQAAUHiKDQAAUHiKDQAAUHj9qx0AAAC2lj1nPdLt8fIrplUpCT3NjA0AAFB4ig0AAFB4ig0AAFB4ig0AAFB4ig0AAFB4FRebN998M9/61rcyfPjwDBo0KF/96lfzwgsvlI+XSqVcdNFF2X333TNo0KBMnjw5r732Wo+GBgAA2FBFxea9997L4Ycfnh133DE/+tGP8sorr+Qf/uEfsssuu5TPufLKK3Pdddfl5ptvzrPPPpudd945U6ZMyYcfftjj4QEAAJIKP8fm7/7u79LY2Ji5c+eW940dO7b851KplGuvvTYXXnhhjj/++CTJXXfdlYaGhjzwwAP55je/+ZlrdnV1paurq/y4s7Oz4i8CAADYvlU0Y/Pv//7vmThxYk488cSMGDEiBx10UG677bby8TfeeCNtbW2ZPHlyeV99fX0mTZqUhQsXbvSac+bMSX19fXlrbGzczC8FAAB+/SGcG25sHyoqNr/4xS9y0003ZZ999snjjz+e73znO/mLv/iL3HnnnUmStra2JElDQ0O35zU0NJSPfVpra2s6OjrK28qVKzfn6wAAALZjFb0Vbf369Zk4cWK+973vJUkOOuigLFmyJDfffHNmzpy5WQFqa2tTW1u7Wc8FAABIKpyx2X333bPffvt12zd+/PisWLEiSTJy5MgkSXt7e7dz2tvby8cAAAB6WkXF5vDDD8+yZcu67fv5z3+eMWPGJPn1QgIjR47M/Pnzy8c7Ozvz7LPPpqmpqQfiAgAAfFZFb0U799xzc9hhh+V73/te/vAP/zDPPfdcbr311tx6661Jkpqampxzzjm57LLLss8++2Ts2LH57ne/m1GjRmX69Om9kR8AAKCyYnPwwQfn/vvvT2tray699NKMHTs21157bU4++eTyOeeff37WrFmTM844I6tXr84RRxyRxx57LAMHDuzx8AAAAEmFxSZJjj322Bx77LGfe7ympiaXXnppLr300i0KBgAAsKkquscGAABgW6TYAAAAhafYAAAAhafYAAAAhafYAAAAhafYAAAAhafYAAAAhVfx59gAAMC2Zs9Zj1Q7AlVmxgYAACg8xQYAACg8xQYAACg899gAANCnuf9m+2DGBgAAKDzFBgAAKDzFBgAAKDzFBgAAKDyLBwAAsN3acGGB5VdMq2IStpQZGwAAoPAUGwAAoPAUGwAAoPAUGwAAoPAUGwAAoPAUGwAAoPAUGwAAoPAUGwAAoPAUGwAAoPAUGwAAoPAUGwAAoPAUGwAAoPAUGwAAoPAUGwAAoPAUGwAAoPAUGwAAoPAUGwAAoPAUGwAAoPAUGwAAoPAqKjYXX3xxampqum3jxo0rH//www/T3Nyc4cOHZ/DgwZkxY0ba29t7PDQAAMCGKp6x+a3f+q289dZb5e0///M/y8fOPffcPPTQQ7n33nuzYMGCrFq1KieccEKPBgYAAPi0/hU/oX//jBw58jP7Ozo6cvvtt2fevHk5+uijkyRz587N+PHjs2jRohx66KFbnhYAAGAjKp6xee211zJq1KjstddeOfnkk7NixYokyeLFi7Nu3bpMnjy5fO64ceMyevToLFy48HOv19XVlc7Ozm4bAABAJSqasZk0aVLuuOOO7LvvvnnrrbdyySWX5Hd+53eyZMmStLW1ZcCAARk6dGi35zQ0NKStre1zrzlnzpxccsklmxUeAIDt056zHun1ay6/YlqPvwa9p6JiM3Xq1PKfDzjggEyaNCljxozJv/3bv2XQoEGbFaC1tTUtLS3lx52dnWlsbNysawEAANunLVrueejQofnN3/zNvP766xk5cmTWrl2b1atXdzunvb19o/fkfKK2tjZ1dXXdNgAAgEpsUbH54IMP8t///d/ZfffdM2HChOy4446ZP39++fiyZcuyYsWKNDU1bXFQAACAz1PRW9H+6q/+Kscdd1zGjBmTVatWZfbs2dlhhx1y0kknpb6+PqeddlpaWloybNiw1NXV5eyzz05TU5MV0QAAgF5VUbH5n//5n5x00kn51a9+ld122y1HHHFEFi1alN122y1Jcs0116Rfv36ZMWNGurq6MmXKlNx44429EhwAAOATNaVSqVTtEBvq7OxMfX19Ojo63G8DAMBG9caqaJ9mVbTqq6QbbNE9NgAAANsCxQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACg8xQYAACi8/tUOAAAAG7PnrEeqHYECMWMDAAAUnmIDAAAUnmIDAAAUnntsAABgIza8x2f5FdOqmIRNsUUzNldccUVqampyzjnnlPd9+OGHaW5uzvDhwzN48ODMmDEj7e3tW5oTAADgc212sXn++edzyy235IADDui2/9xzz81DDz2Ue++9NwsWLMiqVatywgknbHFQAACAz7NZxeaDDz7IySefnNtuuy277LJLeX9HR0duv/32XH311Tn66KMzYcKEzJ07N88880wWLVrUY6EBAAA2tFnFprm5OdOmTcvkyZO77V+8eHHWrVvXbf+4ceMyevToLFy4cKPX6urqSmdnZ7cNAACgEhUvHnD33XfnxRdfzPPPP/+ZY21tbRkwYECGDh3abX9DQ0Pa2to2er05c+bkkksuqTQGAABAWUUzNitXrsxf/uVf5l//9V8zcODAHgnQ2tqajo6O8rZy5coeuS4AALD9qKjYLF68OG+//XZ++7d/O/3790///v2zYMGCXHfddenfv38aGhqydu3arF69utvz2tvbM3LkyI1es7a2NnV1dd02AACASlT0VrSvf/3refnll7vtO/XUUzNu3Lj8zd/8TRobG7Pjjjtm/vz5mTFjRpJk2bJlWbFiRZqamnouNQAAwAYqKjZDhgzJ/vvv323fzjvvnOHDh5f3n3baaWlpacmwYcNSV1eXs88+O01NTTn00EN7LjUAAMAGKl484Mtcc8016devX2bMmJGurq5MmTIlN954Y0+/DAAAQFlNqVQqVTvEhjo7O1NfX5+Ojg732wAAbMf2nPVItSOULb9iWrUjbJcq6Qab9Tk2AAAA2xLFBgAAKLwev8cGAAD6mk+/Lc5b07Y9ZmwAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCU2wAAIDCq6jY3HTTTTnggANSV1eXurq6NDU15Uc/+lH5+Icffpjm5uYMHz48gwcPzowZM9Le3t7joQEAADZUUbHZY489csUVV2Tx4sV54YUXcvTRR+f444/P0qVLkyTnnntuHnroodx7771ZsGBBVq1alRNOOKFXggMAAHyiplQqlbbkAsOGDctVV12Vb3zjG9ltt90yb968fOMb30iSvPrqqxk/fnwWLlyYQw89dKPP7+rqSldXV/lxZ2dnGhsb09HRkbq6ui2JBgBAge0565FqR/hcy6+YVu0I24XOzs7U19dvUjfY7HtsPv7449x9991Zs2ZNmpqasnjx4qxbty6TJ08unzNu3LiMHj06Cxcu/NzrzJkzJ/X19eWtsbFxcyMBAADbqYqLzcsvv5zBgwentrY2Z555Zu6///7st99+aWtry4ABAzJ06NBu5zc0NKStre1zr9fa2pqOjo7ytnLlyoq/CAAAYPvWv9In7LvvvnnppZfS0dGR++67LzNnzsyCBQs2O0BtbW1qa2s3+/kAAAAVF5sBAwbkK1/5SpJkwoQJef755/P9738/f/RHf5S1a9dm9erV3WZt2tvbM3LkyB4LDAAA8Glb/Dk269evT1dXVyZMmJAdd9wx8+fPLx9btmxZVqxYkaampi19GQAAgM9V0YxNa2trpk6dmtGjR+f999/PvHnz8uMf/ziPP/546uvrc9ppp6WlpSXDhg1LXV1dzj777DQ1NX3uimgAAAA9oaJi8/bbb+dP/uRP8tZbb6W+vj4HHHBAHn/88fz+7/9+kuSaa65Jv379MmPGjHR1dWXKlCm58cYbeyU4AADAJ7b4c2x6WiVrVQMA0Hf5HBu2yufYAAAAbCsUGwAAoPAqXu4ZAAB6w7b81jO2fWZsAACAwlNsAACAwlNsAACAwlNsAACAwrN4AAAAVGjDhQ58ps22wYwNAABQeIoNAABQeIoNAABQeIoNAABQeIoNAABQeIoNAABQeIoNAABQeIoNAABQeD6gEwCAqtnwgy5hS5ixAQAACk+xAQAACk+xAQAACk+xAQAACk+xAQAACk+xAQAACk+xAQAACk+xAQAACk+xAQAACk+xAQAACk+xAQAACk+xAQAACk+xAQAACq9/tQMAAECR7TnrkW6Pl18xrUpJtm9mbAAAgMJTbAAAgMJTbAAAgMJTbAAAgMJTbAAAgMKrqNjMmTMnBx98cIYMGZIRI0Zk+vTpWbZsWbdzPvzwwzQ3N2f48OEZPHhwZsyYkfb29h4NDQAAsKGKis2CBQvS3NycRYsW5Yknnsi6detyzDHHZM2aNeVzzj333Dz00EO59957s2DBgqxatSonnHBCjwcHAAD4REWfY/PYY491e3zHHXdkxIgRWbx4cX73d383HR0duf322zNv3rwcffTRSZK5c+dm/PjxWbRoUQ499NCeSw4AAPD/bNE9Nh0dHUmSYcOGJUkWL16cdevWZfLkyeVzxo0bl9GjR2fhwoUbvUZXV1c6Ozu7bQAAAJXY7GKzfv36nHPOOTn88MOz//77J0na2toyYMCADB06tNu5DQ0NaWtr2+h15syZk/r6+vLW2Ni4uZEAAIDt1GYXm+bm5ixZsiR33333FgVobW1NR0dHeVu5cuUWXQ8AANj+VHSPzSfOOuusPPzww3n66aezxx57lPePHDkya9euzerVq7vN2rS3t2fkyJEbvVZtbW1qa2s3JwYAAECSCmdsSqVSzjrrrNx///158sknM3bs2G7HJ0yYkB133DHz588v71u2bFlWrFiRpqamnkkMAADwKRXN2DQ3N2fevHl58MEHM2TIkPJ9M/X19Rk0aFDq6+tz2mmnpaWlJcOGDUtdXV3OPvvsNDU1WRENAIDtwp6zHin/efkV06qYZPtSUbG56aabkiRHHnlkt/1z587NKaeckiS55ppr0q9fv8yYMSNdXV2ZMmVKbrzxxh4JCwAAsDEVFZtSqfSl5wwcODA33HBDbrjhhs0OBQAAUIkt+hwbAACAbYFiAwAAFJ5iAwAAFJ5iAwAAFJ5iAwAAFJ5iAwAAFJ5iAwAAFJ5iAwAAFJ5iAwAAFJ5iAwAAFJ5iAwAAFJ5iAwAAFJ5iAwAAFJ5iAwAAFJ5iAwAAFJ5iAwAAFJ5iAwAAFJ5iAwAAFJ5iAwAAFJ5iAwAAFJ5iAwAAFF7/agcAAIC+as9Zj3R7vPyKaVVK0veZsQEAAApPsQEAAApPsQEAAApPsQEAAArP4gEAAPQoN8xTDWZsAACAwlNsAACAwlNsAACAwlNsAACAwrN4AAAAvWrDxQQsJEBvMWMDAAAUnmIDAAAUnmIDAAAUnntsAABgK3G/Ue+peMbm6aefznHHHZdRo0alpqYmDzzwQLfjpVIpF110UXbfffcMGjQokydPzmuvvdZTeQEAAD6j4mKzZs2aHHjggbnhhhs2evzKK6/Mddddl5tvvjnPPvtsdt5550yZMiUffvjhFocFAADYmIrfijZ16tRMnTp1o8dKpVKuvfbaXHjhhTn++OOTJHfddVcaGhrywAMP5Jvf/OaWpQUAANiIHl084I033khbW1smT55c3ldfX59JkyZl4cKFG31OV1dXOjs7u20AAACV6NFi09bWliRpaGjotr+hoaF87NPmzJmT+vr68tbY2NiTkQAAgO1A1Zd7bm1tTUdHR3lbuXJltSMBAAAF06PFZuTIkUmS9vb2bvvb29vLxz6ttrY2dXV13TYAAIBK9GixGTt2bEaOHJn58+eX93V2dubZZ59NU1NTT74UAABAWcWron3wwQd5/fXXy4/feOONvPTSSxk2bFhGjx6dc845J5dddln22WefjB07Nt/97nczatSoTJ8+vSdzAwAAlFVcbF544YUcddRR5cctLS1JkpkzZ+aOO+7I+eefnzVr1uSMM87I6tWrc8QRR+Sxxx7LwIEDey41AADABiouNkceeWRKpdLnHq+pqcmll16aSy+9dIuCAQAAbKqqr4oGAACwpRQbAACg8Cp+KxoAAGyuPWc9Uu0I9FFmbAAAgMJTbAAAgMJTbAAAgMJTbAAAgMKzeAAAAFvMogBUmxkbAACg8BQbAACg8BQbAACg8BQbAACg8CweAAAAVfDpBReWXzGtSkn6BjM2AABA4Sk2AABA4Sk2AABA4Sk2AABA4Sk2AABA4Sk2AABA4Sk2AABA4Sk2AABA4fmATgAA2AZs+IGdPqyzcmZsAACAwlNsAACAwlNsAACAwlNsAACAwrN4AAAAm8TN7WzLzNgAAACFp9gAAACFp9gAAACFp9gAAACFZ/EAAIDtjEUAiueL/ptteGxjx7cXZmwAAIDCU2wAAIDCU2wAAIDCU2wAAIDCs3jAJnCDHQDbEjcKF9PW/n1iU1/vi76fPn3si55Hz9oa4/tFr1HEv1d6bcbmhhtuyJ577pmBAwdm0qRJee6553rrpQAAgO1crxSbe+65Jy0tLZk9e3ZefPHFHHjggZkyZUrefvvt3ng5AABgO9crb0W7+uqrc/rpp+fUU09Nktx888155JFH8k//9E+ZNWtWt3O7urrS1dVVftzR0ZEk6ezs7I1om2V91/+W/7wt5QJg+7Thz6XEz6ai2Nq/T3zR6336e2hDG577RedRPV/233NTv7829fugmj7JUSqVvvTcmtKmnFWBtWvXZqeddsp9992X6dOnl/fPnDkzq1evzoMPPtjt/IsvvjiXXHJJT0YAAAD6kJUrV2aPPfb4wnN6fMbmnXfeyccff5yGhoZu+xsaGvLqq69+5vzW1ta0tLSUH69fvz7vvvtuhg8fnpqamp6Ot0U6OzvT2NiYlStXpq6urtpx+iRj3PuM8dZhnHufMd46jHPvM8a9zxhvHb0xzqVSKe+//35GjRr1pedWfVW02tra1NbWdts3dOjQ6oTZRHV1df6n6GXGuPcZ463DOPc+Y7x1GOfeZ4x7nzHeOnp6nOvr6zfpvB5fPGDXXXfNDjvskPb29m7729vbM3LkyJ5+OQAAgJ4vNgMGDMiECRMyf/788r7169dn/vz5aWpq6umXAwAA6J23orW0tGTmzJmZOHFiDjnkkFx77bVZs2ZNeZW0oqqtrc3s2bM/89Y5eo4x7n3GeOswzr3PGG8dxrn3GePeZ4y3jmqPc4+vivaJH/zgB7nqqqvS1taWr33ta7nuuusyadKk3ngpAABgO9drxQYAAGBr6fF7bAAAALY2xQYAACg8xQYAACg8xQYAACg8xWYTXX755TnssMOy0047ZejQoV947q9+9avsscceqampyerVq7dKvr7gy8b4Zz/7WU466aQ0NjZm0KBBGT9+fL7//e9v/aAFtinfxytWrMi0adOy0047ZcSIEfnrv/7rfPTRR1s3aB/z85//PMcff3x23XXX1NXV5YgjjshTTz1V7Vh9ziOPPJJJkyZl0KBB2WWXXTJ9+vRqR+qzurq68rWvfS01NTV56aWXqh2nz1i+fHlOO+20jB07NoMGDcree++d2bNnZ+3atdWOVng33HBD9txzzwwcODCTJk3Kc889V+1IfcacOXNy8MEHZ8iQIRkxYkSmT5+eZcuWVSWLYrOJ1q5dmxNPPDHf+c53vvTc0047LQcccMBWSNW3fNkYL168OCNGjMi//Mu/ZOnSpbngggvS2tqaH/zgB1s5aXF92Rh//PHHmTZtWtauXZtnnnkmd955Z+64445cdNFFWzlp33Lsscfmo48+ypNPPpnFixfnwAMPzLHHHpu2trZqR+szfvjDH+bb3/52Tj311PzsZz/Lf/3Xf+WP//iPqx2rzzr//PMzatSoasfoc1599dWsX78+t9xyS5YuXZprrrkmN998c/72b/+22tEK7Z577klLS0tmz56dF198MQceeGCmTJmSt99+u9rR+oQFCxakubk5ixYtyhNPPJF169blmGOOyZo1a7Z+mBIVmTt3bqm+vv5zj994442l3/u93yvNnz+/lKT03nvvbbVsfcWXjfGG/vzP/7x01FFH9W6gPujzxvjRRx8t9evXr9TW1lbed9NNN5Xq6upKXV1dWzFh3/HLX/6ylKT09NNPl/d1dnaWkpSeeOKJKibrO9atW1f6jd/4jdI//uM/VjvKduHRRx8tjRs3rrR06dJSktJPf/rTakfq06688srS2LFjqx2j0A455JBSc3Nz+fHHH39cGjVqVGnOnDlVTNV3vf3226UkpQULFmz11zZj04NeeeWVXHrppbnrrrvSr5+h3Ro6OjoybNiwasfoMxYuXJivfvWraWhoKO+bMmVKOjs7s3Tp0iomK67hw4dn3333zV133ZU1a9bko48+yi233JIRI0ZkwoQJ1Y7XJ7z44ot58803069fvxx00EHZfffdM3Xq1CxZsqTa0fqc9vb2nH766fnnf/7n7LTTTtWOs13wc27LrF27NosXL87kyZPL+/r165fJkydn4cKFVUzWd3V0dCRJVb5v/fbdQ7q6unLSSSflqquuyujRo6sdZ7vwzDPP5J577skZZ5xR7Sh9RltbW7dSk6T82NumNk9NTU3+4z/+Iz/96U8zZMiQDBw4MFdffXUee+yx7LLLLtWO1yf84he/SJJcfPHFufDCC/Pwww9nl112yZFHHpl33323yun6jlKplFNOOSVnnnlmJk6cWO0424XXX389119/ff7sz/6s2lEK65133snHH3+80Z9tfq71vPXr1+ecc87J4Ycfnv3333+rv/52XWxmzZqVmpqaL9xeffXVTbpWa2trxo8fn29961u9nLpYenKMN7RkyZIcf/zxmT17do455pheSF4cvTXGfLFNHfdSqZTm5uaMGDEiP/nJT/Lcc89l+vTpOe644/LWW29V+8vYpm3qGK9fvz5JcsEFF2TGjBmZMGFC5s6dm5qamtx7771V/iq2fZs6ztdff33ef//9tLa2Vjty4WzO39Nvvvlm/uAP/iAnnnhiTj/99Colh8o0NzdnyZIlufvuu6vy+v2r8qrbiPPOOy+nnHLKF56z1157bdK1nnzyybz88su57777kvz6X7aSZNddd80FF1yQSy65ZIuyFlVPjvEnXnnllXz961/PGWeckQsvvHAL0vUNPTnGI0eO/MxKMe3t7eVj/P82ddyffPLJPPzww3nvvfdSV1eXJLnxxhvzxBNP5M4778ysWbO2Qtpi2tQx/qQg7rfffuX9tbW12WuvvbJixYrejNgnVPK9vHDhwtTW1nY7NnHixJx88sm58847ezFlsVX69/SqVaty1FFH5bDDDsutt97ay+n6tl133TU77LBD+WfZJ9rb2/1c62FnnXVWHn744Tz99NPZY489qpJhuy42u+22W3bbbbceudYPf/jD/N///V/58fPPP58//dM/zU9+8pPsvffePfIaRdSTY5wkS5cuzdFHH52ZM2fm8ssv77HrFllPjnFTU1Muv/zyvP322xkxYkSS5IknnkhdXV23XxrZ9HH/3//93yT5zH13/fr1K880sHGbOsYTJkxIbW1tli1bliOOOCJJsm7duixfvjxjxozp7ZiFt6njfN111+Wyyy4rP161alWmTJmSe+65J5MmTerNiIVXyd/Tb775Zo466qjyzKN7drfMgAEDMmHChMyfP7+8BPz69eszf/78nHXWWdUN10eUSqWcffbZuf/++/PjH/84Y8eOrVqW7brYVGLFihV59913s2LFinz88cfldfu/8pWvZPDgwZ8pL++8806SZPz48V/6uTf82peN8ZIlS3L00UdnypQpaWlpKb83docddujR8tSXfdkYH3PMMdlvv/3y7W9/O1deeWXa2tpy4YUXprm5+TP/SsumaWpqyi677JKZM2fmoosuyqBBg3LbbbfljTfeyLRp06odr0+oq6vLmWeemdmzZ6exsTFjxozJVVddlSQ58cQTq5yu7/j0/aODBw9Okuy9995V+9fZvubNN9/MkUcemTFjxuTv//7v88tf/rJ8zOzC5mtpacnMmTMzceLEHHLIIbn22muzZs2anHrqqdWO1ic0Nzdn3rx5efDBBzNkyJDy72f19fUZNGjQ1g2z1ddhK6iZM2eWknxme+qppzZ6/lNPPWW55wp92RjPnj17o8fHjBlT1dxFsinfx8uXLy9NnTq1NGjQoNKuu+5aOu+880rr1q2rXug+4Pnnny8dc8wxpWHDhpWGDBlSOvTQQ0uPPvpotWP1KWvXri2dd955pREjRpSGDBlSmjx5cmnJkiXVjtWnvfHGG5Z77mFz587d6N/Rfl3bctdff31p9OjRpQEDBpQOOeSQ0qJFi6odqc/4vO/ZuXPnbvUsNf8vEAAAQGF54yYAAFB4ig0AAFB4ig0AAFB4ig0AAFB4ig0AAFB4ig0AAFB4ig0AAFB4ig0AAFB4ig0AAFB4ig0AAFB4ig0AAFB4/x8h3AWDuk47PgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# For the 5th token in our sentence, select its feature values from layer 5.\n",
    "token_i = 5\n",
    "layer_i = 5\n",
    "vec = hidden_states[layer_i][batch_i][token_i]\n",
    "vec = vec.detach().numpy()\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die hidden_states sind gerade noch nch Layern gruppiert, aber es ist Sinnvoller, sie nach Tokens zu gruppieren. Die layer dimension ist gerade noch eine python Liste, deswegen fügen wir sie als 0 dimension einem Tensor hinzu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 41, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes Entfernen wir die Batches dimension, da wir sowieso nur einen Satz haben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 41, 768])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir die Dimensionen mithilfe von torchs permute vertauschen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41, 13, 768])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das finale Ziel ist, ein einziges Embedding für diesen Satz zu erstellen. im Moment haben wir für jedes Token in jedem Layer ein Embedding. also 533 mal soviele wie wir eigentlich wollen.\n",
    "\n",
    "Um dieses Problem zu lösen, gibt es verschiedene Möglichkeiten die verschiedenen Embeddings zu verbinden.\n",
    "\n",
    "Eine Methode wäre, für jedes Token das 2. bis letzte Layer einfach nur zu mitteln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "layer_vecs = torch.mean(token_embeddings, dim=0)\n",
    "print(layer_vecs.shape)\n",
    "# Calculate the average of all 41 token vectors.\n",
    "sentence_embedding = torch.mean(layer_vecs[2:], dim=0)\n",
    "print(sentence_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt noch weitere Ansätze, z.B.:\n",
    "\n",
    "nur das letzte layer\n",
    "embed_1 = layer_vecs[12]\n",
    "\n",
    "den durchschnitt aller layer\n",
    "embed_2 = torch.mean(layer_vecs[2:], dim=0)\n",
    "\n",
    "die Summe aller layer\n",
    "embed_3 = layer_vecs[2:].sum(0)\n",
    "\n",
    "die Summe der letzten 4 Layer\n",
    "embed_4 = layer_vecs[-4:].sum(0) \n",
    "\n",
    "Die Letzten 4 Layer hintereinandergehängt\n",
    "embed_5 = torch.cat([layer_vecs[i] for i in [-1,-2,-3,-4]], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Entwickler des BERT Models haben selbst die verschiedenen Ansätze evaluiert und bei Ihnen war die concatenation der letzten 4 Layer am besten. Allerdings ist dies Aufgabenabhängig und die anderen Ansätze waren marginal schlechter. Die Evaluation für unser Problem muss nun noch erfolgen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
