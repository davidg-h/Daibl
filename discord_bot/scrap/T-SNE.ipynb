{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-SNE Visualization\n",
    "This Notebook visulaizes the embeddings of the words and sentences of the th website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from db_init import db_get_df, db_save_df\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "erklärung, was und warum was man sieht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_embeddings_copy = db_get_df(\"word_embeddings\",[\"word_embeddings\"])\n",
    "print(json.loads(word_embeddings_copy[\"word_embeddings\"][0]))\n",
    "word_embeddings_copy=[json.loads(embedding) for embedding in tqdm(word_embeddings_copy[\"word_embeddings\"])]\n",
    "# Verwenden Sie 'ast.literal_eval', um Zeichenketten in Listen umzuwandeln\n",
    "#word_embeddings_copy = word_embeddings_copy.map(ast.literal_eval)\n",
    "\n",
    "# Wandeln Sie die Listen von Listen in ein Numpy-Array um\n",
    "word_embeddings = np.array(word_embeddings_copy)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "X_embedded = tsne.fit_transform(word_embeddings)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_embedded[:, 0], X_embedded[:, 1], s=5)\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Funktion zur Extraktion von Word Embeddings für die Frage\n",
    "def get_word_embedding(question, model_name='bert-base-uncased'):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    tokens = tokenizer(question, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        question_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return question_embedding\n",
    "\n",
    "# Laden der Word Embeddings\n",
    "word_embeddings_copy = db_get_df(\"word_embeddings\", [\"word_embeddings\"])\n",
    "word_embeddings_copy = [json.loads(embedding) for embedding in tqdm(word_embeddings_copy[\"word_embeddings\"])]\n",
    "word_embeddings = np.array(word_embeddings_copy)\n",
    "\n",
    "# Extrahieren der Embeddings für die Frage\n",
    "question_text = \"n?\"\n",
    "question_embedding = get_word_embedding(question_text)\n",
    "\n",
    "# Berechnen der Kosinus-Ähnlichkeit zwischen der Frage und den anderen Word Embeddings\n",
    "similarities = cosine_similarity(word_embeddings, [question_embedding])\n",
    "\n",
    "# 'similarities' ist jetzt ein Array mit den Kosinus-Ähnlichkeiten zwischen der Frage und den anderen Word Embeddings.\n",
    "\n",
    "# Kombinieren Sie die t-SNE-Komponenten mit den Kosinus-Ähnlichkeiten\n",
    "combined_features = np.column_stack((X_embedded, similarities))\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(combined_features[:, 0], combined_features[:, 1], s=5)\n",
    "plt.scatter(question_embedding[0], question_embedding[1], color='red', s=50, label='Ihre Frage')\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings with Cosine Similarity\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Annahme: X_embedded ist Ihre t-SNE-Visualisierung\n",
    "# Annahme: word_embeddings ist Ihre Matrix der Word Embeddings\n",
    "# Annahme: question_text ist Ihre Frage\n",
    "# Annahme: n_clusters ist die Anzahl der gewünschten Cluster\n",
    "question_text = \"Welche Kompetenzen hat Pr. Gallwitz?\" #wann ist der Bewerbungszeitraum  Für das Wintersemester\n",
    "\n",
    "# Schritt 1: Clustering durchführen\n",
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "cluster_labels = kmeans.fit_predict(word_embeddings)\n",
    "\n",
    "# Schritt 3: Berechnen der Ähnlichkeit zur Frage\n",
    "question_embedding = get_word_embedding(question_text)  # Verwenden Sie Ihre get_word_embedding Funktion\n",
    "similarities = cosine_similarity(word_embeddings, [question_embedding])\n",
    "\n",
    "# Schritt 4: Visualisierung aktualisieren\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(5):\n",
    "    plt.scatter(X_embedded[cluster_labels == i, 0], X_embedded[cluster_labels == i, 1], s=5, label=f'Cluster {i}')\n",
    "\n",
    "# Farben entsprechend des Clusters für die Frage aktualisieren\n",
    "question_cluster = np.argmax(similarities)\n",
    "plt.scatter(X_embedded[question_cluster, 0], X_embedded[question_cluster, 1], s=50, color='red', label='Ihre Frage')\n",
    "\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings with Clusters\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5)  # Specify the number of clusters you want\n",
    "cluster_labels = kmeans.fit_predict(word_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(len(np.unique(cluster_labels))):\n",
    "    plt.scatter(X_embedded[cluster_labels == i, 0], X_embedded[cluster_labels == i, 1], s=5, label=f'Cluster {i}')\n",
    "\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings with Clusters\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the db_get_df function to get the DataFrame\n",
    "df = db_get_df()\n",
    "\n",
    "# Print the first 2 rows of the DataFrame\n",
    "first_2_rows = df.head(2)\n",
    "print(first_2_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the indices of data points in Cluster 0\n",
    "cluster0_indices = np.where(cluster_labels == 0)\n",
    "\n",
    "# Get the corresponding rows from the DataFrame 'df'\n",
    "cluster0_data_rows = df.iloc[cluster0_indices]\n",
    "\n",
    "# Print the 'text' column for the data points in Cluster 0\n",
    "for text in cluster0_data_rows['text']:\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Annahme: Ihr DataFrame 'df' enthält eine Spalte 'text' mit den Textdaten.\n",
    "\n",
    "# Anzahl der Cluster (angenommen, es sind 5 Cluster)\n",
    "num_clusters = 5\n",
    "\n",
    "for cluster_id in range(num_clusters):\n",
    "    # Filtern Sie die Zeilen für den aktuellen Cluster\n",
    "    cluster_data_rows = df\n",
    "\n",
    "    # Laden des spaCy-Modells für die Textverarbeitung\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    # Benutzerdefinierte Stoppwortliste\n",
    "    stopwords = {'www', 'th-nuernberg', 'nürnberg', 'nuernberg', 'th', 'technische', 'hochschule', 'ohm', 'de', 'punkt', 'simon'}\n",
    "\n",
    "    # Tokenisieren und Lemmatisieren der Texte, Entfernen der Stoppwörter und Konvertieren in Strings\n",
    "    processed_texts = []\n",
    "    for text in cluster_data_rows['text']:\n",
    "        doc = nlp(text)\n",
    "        processed_tokens = []\n",
    "        for token in doc:\n",
    "            if token.text.lower() not in stopwords and token.pos_ in {'NOUN', 'PROPN'}:\n",
    "                processed_tokens.append(token.text)\n",
    "        processed_texts.append(' '.join(processed_tokens))\n",
    "\n",
    "    # Erstellen eines Wörterbuchs und einer Textkorpus für das LDA-Modell\n",
    "    text_tokens = [text.split() for text in processed_texts]\n",
    "    dictionary = gensim.corpora.Dictionary(text_tokens)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in text_tokens]\n",
    "\n",
    "    # Anwendung des LDA-Modells\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "    # Anzeigen der Hauptthemen für den aktuellen Cluster\n",
    "    print(f\"Cluster {cluster_id} Topics:\")\n",
    "    for topic_id, topic in lda_model.print_topics():\n",
    "        print(f\"Topic {topic_id}: {topic}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_text = ' '.join(processed_texts)  # 'processed_texts' ist die Liste der bereinigten Texte\n",
    "\n",
    "# Erstellen der Word Cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "\n",
    "# Anzeigen der Word Cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Erstellen einer Liste von Stoppwörtern, einschließlich der URL und der benutzerdefinierten Wörter\n",
    "stopwords = set(['www', 'th-nuernberg', 'nürnberg', 'nuernberg', 'th', 'technische', 'hochschule', 'ohm', 'de', 'punkt', 'simon','https','http','nuremberg','telefon','email','fax','Prof Dr','studium'])\n",
    "stopwords = set(word.lower() for word in stopwords)  # In Kleinbuchstaben umwandeln\n",
    "\n",
    "# Anzahl der Cluster (angenommen, es sind 5 Cluster)\n",
    "num_clusters = 5\n",
    "\n",
    "for cluster_id in range(num_clusters):\n",
    "    if cluster_id == 1:\n",
    "        continue\n",
    "    # Filter the indices of data points in the current cluster\n",
    "    cluster_indices = np.where(cluster_labels == cluster_id)\n",
    "\n",
    "    # Get the corresponding rows from the DataFrame 'df'\n",
    "    cluster_data_rows = df.iloc[cluster_indices]\n",
    "\n",
    "    # Extract and preprocess text data\n",
    "    texts = cluster_data_rows['text']\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "    processed_texts = [' '.join([token.text for token in nlp(text) if not token.is_stop and token.text.lower() not in stopwords]) for text in texts]\n",
    "\n",
    "    # Create a Word Cloud for the current cluster\n",
    "    all_text = ' '.join(processed_texts)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "\n",
    "    # Display the Word Cloud for the current cluster\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Word Cloud for Cluster {cluster_id}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Flatten the list of embeddings to a single array\n",
    "flattened_embeddings = [embedding for sublist in embeddings for embedding in sublist]\n",
    "print(flattened_embeddings)\n",
    "# Convert the list of embeddings to a NumPy array\n",
    "embeddings_array = np.array(flattened_embeddings)\n",
    "\n",
    "# Reduce the dimensions using t-SNE to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "embeddings_2d = []\n",
    "batch_size = 1000  # Adjust the batch size as needed\n",
    "\n",
    "# Apply t-SNE in batches to track progress\n",
    "for i in tqdm(range(0, len(embeddings_array), batch_size)):\n",
    "    embeddings_batch = embeddings_array[i:i + batch_size]\n",
    "    embeddings_2d_batch = tsne.fit_transform(embeddings_batch)\n",
    "    embeddings_2d.extend(embeddings_2d_batch)\n",
    "\n",
    "# Plot the 2D embeddings\n",
    "embeddings_2d = np.array(embeddings_2d)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5)\n",
    "plt.title('t-SNE Visualization of Sentence Embeddings')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming 'embeddings' contains the generated sentence embeddings\n",
    "flat_embeddings = np.array([emb for sublist in embeddings for emb in sublist])\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_2d = []\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction with progress bar\n",
    "with tqdm(total=len(flat_embeddings)) as pbar:\n",
    "    for i in range(0, len(flat_embeddings), 100):  # Process in batches for efficiency\n",
    "        embeddings_2d_batch = tsne.fit_transform(flat_embeddings[i:i + 100])\n",
    "        embeddings_2d.append(embeddings_2d_batch)\n",
    "        pbar.update(100)\n",
    "\n",
    "# Flatten the results\n",
    "embeddings_2d = np.vstack(embeddings_2d)\n",
    "\n",
    "# Plot the 2D t-SNE visualization\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], marker='.')\n",
    "plt.title('t-SNE Visualization of Sentence Embeddings')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "question_text = \"Welche Kompetenzen hat Pr. Gallwitz?\" #embeddings machen\n",
    "embeddings # schauen welche 5 am nächsten sind"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
