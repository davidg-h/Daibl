{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Processing</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ziel dieses Notebooks ist, die gescrapeten Daten so zu verarbeiten, dass Sie später thematisch durchsucht werden können. Ein Embeddingvektor liefert genau diese Funktion. Auf eine Anfrage hin wird der Abstand zwischen dem Embedding der Frage und den Embeddings aller anderen Dokumenten berechnet. Dann werden die Dokumente mit den kleinsten Abständen ausgewählt und dem LLM als Kontext mitgegeben. Mithilfe des Wissens dieser Dokumente soll dass LLM dann in der Lage sein die Frage korrekt zu beantworten.\n",
    "Für die Embeddings benutzen wir [Google Bert](https://blog.google/products/search/search-language-understanding-bert/)\n",
    "\n",
    "Beispiel:\n",
    "\n",
    "User: Welche Dozenten unterrichten das Fach Grundlagen der Informatik?\n",
    "\n",
    "System wählt besten 5 Dokumente aus \n",
    "\n",
    "    <Dokument 1>: ... betreute Prof. Dr. Löhr eine Batchelorarbeit in Grundlagen der Informatik...\n",
    "    <Dokument 2>: Prof. Dr. Weber tel.: 013882664 email: weber@th.de Raum: HQ: 403, Fächer: Grundlagen der Informatik ...\n",
    "    <Dokument 3> ...\n",
    "    <Dokument 4> ...\n",
    "    <Dokument 5> ...\n",
    "    \n",
    "\n",
    "Aus der Nutzeranfrage und den Dokumenten wird eine neue Query erstellt, die dem LLM dann final bereitgetellt wird. Diese sieht in etwa so aus:\n",
    "\n",
    "    \n",
    "    <Dokument 1>: ... betreute Prof. Dr. Löhr eine Batchelorarbeit in Grundlagen der     Informatik...\n",
    "    <Dokument 2>: Prof. Dr. Weber tel.: 013882664 email: weber@th.de Raum: HQ: 403, Fächer: Grundlagen der Informatik ...\n",
    "    <Dokument 3> ...\n",
    "    <Dokument 4> ...\n",
    "    <Dokument 5> ...\n",
    "\n",
    "    Bitte beantworte folgende Frage unter der Berücksichtigung obiger Dokumente:\n",
    "    Welche Dozenten unterrichten das Fach Grundlagen der Informatik?\n",
    "\n",
    "\n",
    "Das LLM wird daraufhin hoffentlich korrekt eine Antwort liefern die ähnlich ist zu:\n",
    "\n",
    "    A: An der TH Nürnberg Georg Simon Ohm unterichten die Professoren Prof. Dr. Löhr und Prof. Dr. Weber das Fach Grundlagen der Informatik.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cosine\n",
    "from db_init import db_get_df, db_save_df\n",
    "import json\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE \n",
    "import numpy as np  \n",
    "import json\n",
    "from db_init import db_get_df, db_save_df\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE \n",
    "import numpy as np  \n",
    "import json\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import gensim\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from question_embedding import question_embeddings\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst laden wir die Daten aus der Datenbank. Dabei besitzt jedes Dokument als Metadaten den Titel der Webseite, den filenamen und den Text. Diese speichern wir uns in einen Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"html_attrs\", [\"filename\", \"title\", \"text\"])\n",
    "\n",
    "print(df.dtypes)\n",
    "print(df[\"text\"][3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur überprüfung der Texte können wir nun einmal eine Keywordsuche starten. Dieser Ansatz wird außerdem tiefer im Notebook [spacy_keywordextraction](./spacy_keywordextraction.ipynb) verfolgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Gallwitz\"\n",
    "\n",
    "[text for text in df[\"text\"] if word in text][:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt werden wir für jedes Dokument ein eigenes Word embeddings erstellen. Dazu müssen wir zunächst das BERT Model laden.\n",
    "Das BERT Model ist ein von Google trainiertes mehrschichtiges neuronales Netz, welches ursprünglich dafür entwickelt wurde, dass ???\n",
    "es ist trainiert auf 10.000+ Büchern\n",
    "es gibt Modelle \"base\" und \"large\"\n",
    "uncased heißt ohne klein - Großschreibung\n",
    "\n",
    "Wir brauchen zur vorbereitung die zusätzlichen Token\n",
    "[SEP] um das Ende eines Satzes zu markieren\n",
    "[CLS] am Anfang des Texten\n",
    "[PAD] zum auffüllen der Token \n",
    "Außerdem\n",
    "TokenIDs\n",
    "MaskIDs - zum filtern der [PAD]\n",
    "Segment IDs um verschiedene Sätze zu unterscheiden\n",
    "Posititional Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO try better model\n",
    "model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True) \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSentence = \"In der Bibliothek gibt es 40 Bücher zum Thema Animes\"\n",
    "tokens_question = tokenizer.tokenize(testSentence)\n",
    "print(*tokens_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun erstellen wir eine neue Spalte [\"tokens\"], in der wir für jedes Dokument die Tokens abspeichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] = [tokenizer.tokenize(text) for text in tqdm(df[\"text\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der dataframe hat nun eine Spalte mehr und wir können uns ein Beispiel der Tokens ansehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "print(df[\"tokens\"][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Tokens müssen nun in IDs umgewandelt werden, damit sie das BERT Model für die erstellung eines Embedding Vectors benutzen kann. Dafür benutzen wir eine Funktion des Tokenizers convert_tokens_to_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"token_ids\"] = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tqdm(df[\"tokens\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_symbols = [\"[CLS]\", \"[SEP]\"]\n",
    "print(tokenizer.convert_tokens_to_ids(special_symbols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben jetzt also die Tokens IDs für unsere 2400 verschiedenen Dokumente gebildet. Der nächste Schritt wäre nun, diese Tokens IDs in dem BERT Model zu übergeben, sodass es uns ein Embedding daraus errechnet. Leider kann das BERT Model nur 512 Tokens (~1300 Zeichen) als Input nehmen. Die meißten der gescrapeten Webseiten sind aber wesentlich länger. Der Naheliegendste Ansatz ist dabei, die Tokens einfach in 510 token große Chunks aufzusplitten (Wir brauchen noch 2 Tokens extra für jeden Chunk) und für jeden Chunk ein extra Embedding zu erstellen. Dabei gibt es entweder die Möglichkeit die Chunks überlappend, oder einfach hard cut zu gestalten. Wir werden hier zunächst den hard cut Ansatz verfolgen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splice dokuments in 512 token chunks\n",
    "\n",
    "# Initialize an empty list to store rows for the new DataFrame\n",
    "new_rows = []\n",
    "\n",
    "# Function to split text and tokens into chunks of 512 tokens\n",
    "def split_text_and_tokens(row):\n",
    "    text = row['text']\n",
    "    tokens_ids = row['token_ids']\n",
    "    filename = row['filename']\n",
    "\n",
    "    if len(tokens_ids) > 510:\n",
    "        # Split into multiple chunks\n",
    "        for i in range(0, len(tokens_ids), 510):\n",
    "            chunk_tokens = tokens_ids[i:i + 510]\n",
    "            # adding the [CLS] and the [SEP] token\n",
    "            chunk_tokens = [101] + chunk_tokens + [102]\n",
    "            chunk_text = tokenizer.decode(chunk_tokens)\n",
    "\n",
    "            # Create a new row with a reference to the original row\n",
    "            new_row = {'filename': filename, 'chunk_id': i/510, 'chunk_text': chunk_text, 'chunk_tokens_json': json.dumps(chunk_tokens)}\n",
    "            new_rows.append(new_row)\n",
    "    else:\n",
    "        # If the row has 510 tokens or fewer, keep it as is\n",
    "\n",
    "        # adding the [CLS] and the [SEP] token\n",
    "        chunk_tokens = [101] + tokens_ids + [102]\n",
    "        text = \"[CLS]\" + text + \"[SEP]\"\n",
    "        new_row = {'filename': filename, 'chunk_id': 0, 'chunk_text': text, 'chunk_tokens_json': json.dumps(tokens_ids) }\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "# Apply the function to each row in the original DataFrame\n",
    "df.apply(split_text_and_tokens, axis=1)\n",
    "\n",
    "# Create a new DataFrame from the list of new rows\n",
    "chunk_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Reset the index of the new DataFrame if needed\n",
    "chunk_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(chunk_df.sample(2).to_markdown())\n",
    "\n",
    "# tokenizer.convert_tokens_to_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ab jetzt werden wir mit dem neuen chunk_df weiterarbeiten. Nun erstellen wir für jeden Chunk ein eigenes Embedding, welches dann die Semantik dieses chunks enthalten soll. Dafür müssen wir uns nun das BERT Model etwas genauer anschauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proccessSentence(tokens, model, tokenizer):\n",
    "    if len(tokens) == 0:\n",
    "        # Handle the case when the token list is empty, for example, return a default embedding or raise an exception.\n",
    "        # For demonstration purposes, we'll return a zero tensor as the default embedding.\n",
    "        return torch.zeros(768)\n",
    "\n",
    "    tokens = [\"CLS\"] + tokens + [\"SEP\"]\n",
    "\n",
    "    attention_mask = [1 if token != \"[PAD]\" else 0  for token in tokens]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    token_ids_tensor = torch.tensor([token_ids], dtype=torch.int64)\n",
    "    attetion_mask_tensor = torch.tensor([attention_mask], dtype=torch.int64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(token_ids_tensor, attetion_mask_tensor)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # stack the layer list \n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    # remove the batches dim\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    # average all token embeds\n",
    "    layer_vecs = torch.mean(token_embeddings, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate the average of layer 3 to 13\n",
    "    embed = torch.mean(layer_vecs[2:], dim=0)\n",
    "\n",
    "\n",
    "    return embed\n",
    "\n",
    "\n",
    "df[\"chunk_embeddings\"] = [proccessSentence(tokens).tolist() for tokens in tqdm(df[\"tokens\"])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diser Code kann sehr lange brauchen um die Embeddings zu berechnen. An dieser Stelle sollte man dann die Embeddings am besten abspeichern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "db_save_df(df, \"chunk_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"chunk_embeddings\", [\"*\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_t_sne(embed_id):\n",
    "    df_embed=[json.loads(embedding) for embedding in tqdm(df[f\"chunk_embeddings_{embed_id}\"])]\n",
    "    word_embeddings = np.array(df_embed)\n",
    "    tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "    X_embedded = tsne.fit_transform(word_embeddings)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X_embedded[:, 0], X_embedded[:, 1], s=5)\n",
    "    plt.title(f\"t-SNE Visualization of Word Embeddings {embed_id}\")\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    embed_t_sne(i +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Funktion zur Extraktion von Word Embeddings für die Frage\n",
    "def get_word_embedding(question, model_name='bert-base-uncased'):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    tokens = tokenizer(question, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        question_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return question_embedding\n",
    "\n",
    "# Laden der Word Embeddings\n",
    "df = db_get_df(\"chunk_embeddings\", [\"chunk_embeddings_2\"])\n",
    "df = [json.loads(embedding) for embedding in tqdm(df[\"chunk_embeddings_2\"])]\n",
    "word_embeddings = np.array(df)\n",
    "\n",
    "# Extrahieren der Embeddings für die Frage\n",
    "question_text = \"n?\"\n",
    "question_embedding = get_word_embedding(question_text)\n",
    "\n",
    "# Berechnen der Kosinus-Ähnlichkeit zwischen der Frage und den anderen Word Embeddings\n",
    "similarities = cosine_similarity(word_embeddings, [question_embedding])\n",
    "\n",
    "# 'similarities' ist jetzt ein Array mit den Kosinus-Ähnlichkeiten zwischen der Frage und den anderen Word Embeddings.\n",
    "\n",
    "# Kombinieren Sie die t-SNE-Komponenten mit den Kosinus-Ähnlichkeiten\n",
    "combined_features = np.column_stack((X_embedded, similarities))\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(combined_features[:, 0], combined_features[:, 1], s=5)\n",
    "plt.scatter(question_embedding[0], question_embedding[1], color='red', s=50, label='Ihre Frage')\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings with Cosine Similarity\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Annahme: X_embedded ist Ihre t-SNE-Visualisierung\n",
    "# Annahme: word_embeddings ist Ihre Matrix der Word Embeddings\n",
    "# Annahme: question_text ist Ihre Frage\n",
    "# Annahme: n_clusters ist die Anzahl der gewünschten Cluster\n",
    "question_text = \"Welche Kompetenzen hat Pr. Gallwitz?\" #wann ist der Bewerbungszeitraum  Für das Wintersemester\n",
    "\n",
    "# Schritt 1: Clustering durchführen\n",
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "cluster_labels = kmeans.fit_predict(word_embeddings)\n",
    "\n",
    "# Schritt 3: Berechnen der Ähnlichkeit zur Frage\n",
    "question_embedding = get_word_embedding(question_text)  # Verwenden Sie Ihre get_word_embedding Funktion\n",
    "similarities = cosine_similarity(word_embeddings, [question_embedding])\n",
    "\n",
    "# Schritt 4: Visualisierung aktualisieren\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(5):\n",
    "    plt.scatter(X_embedded[cluster_labels == i, 0], X_embedded[cluster_labels == i, 1], s=5, label=f'Cluster {i}')\n",
    "\n",
    "# Farben entsprechend des Clusters für die Frage aktualisieren\n",
    "question_cluster = np.argmax(similarities)\n",
    "plt.scatter(X_embedded[question_cluster, 0], X_embedded[question_cluster, 1], s=50, color='red', label='Ihre Frage')\n",
    "\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings with Clusters\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans = KMeans(n_clusters=5)  # Specify the number of clusters you want\n",
    "cluster_labels = kmeans.fit_predict(word_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(len(np.unique(cluster_labels))):\n",
    "    plt.scatter(X_embedded[cluster_labels == i, 0], X_embedded[cluster_labels == i, 1], s=5, label=f'Cluster {i}')\n",
    "\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings with Clusters\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the indices of data points in Cluster 0\n",
    "cluster0_indices = np.where(cluster_labels == 1)\n",
    "\n",
    "# Get the corresponding rows from the DataFrame 'df'\n",
    "cluster0_data_rows = df.iloc[cluster0_indices]\n",
    "\n",
    "# Print the 'text' column for the data points in Cluster 0\n",
    "for text in cluster0_data_rows['text']:\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Annahme: Ihr DataFrame 'df' enthält eine Spalte 'text' mit den Textdaten.\n",
    "\n",
    "# Anzahl der Cluster (angenommen, es sind 5 Cluster)\n",
    "num_clusters = 5\n",
    "\n",
    "for cluster_id in range(num_clusters):\n",
    "    # Filtern Sie die Zeilen für den aktuellen Cluster\n",
    "    cluster_data_rows = df\n",
    "\n",
    "    # Laden des spaCy-Modells für die Textverarbeitung\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    # Benutzerdefinierte Stoppwortliste\n",
    "    stopwords = {'www', 'th-nuernberg', 'nürnberg', 'nuernberg', 'th', 'technische', 'hochschule', 'ohm', 'de', 'punkt', 'simon'}\n",
    "\n",
    "    # Tokenisieren und Lemmatisieren der Texte, Entfernen der Stoppwörter und Konvertieren in Strings\n",
    "    processed_texts = []\n",
    "    for text in cluster_data_rows['text']:\n",
    "        doc = nlp(text)\n",
    "        processed_tokens = []\n",
    "        for token in doc:\n",
    "            if token.text.lower() not in stopwords and token.pos_ in {'NOUN', 'PROPN'}:\n",
    "                processed_tokens.append(token.text)\n",
    "        processed_texts.append(' '.join(processed_tokens))\n",
    "\n",
    "    # Erstellen eines Wörterbuchs und einer Textkorpus für das LDA-Modell\n",
    "    text_tokens = [text.split() for text in processed_texts]\n",
    "    dictionary = gensim.corpora.Dictionary(text_tokens)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in text_tokens]\n",
    "\n",
    "    # Anwendung des LDA-Modells\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "    # Anzeigen der Hauptthemen für den aktuellen Cluster\n",
    "    print(f\"Cluster {cluster_id} Topics:\")\n",
    "    for topic_id, topic in lda_model.print_topics():\n",
    "        print(f\"Topic {topic_id}: {topic}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_text = ' '.join(processed_texts)  # 'processed_texts' ist die Liste der bereinigten Texte\n",
    "\n",
    "# Erstellen der Word Cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "\n",
    "# Anzeigen der Word Cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Erstellen einer Liste von Stoppwörtern, einschließlich der URL und der benutzerdefinierten Wörter\n",
    "stopwords = set(['www', 'th-nuernberg', 'nürnberg', 'nuernberg', 'th', 'technische', 'hochschule', 'ohm', 'de', 'punkt', 'simon','https','http','nuremberg','telefon','email','fax','Prof Dr','studium'])\n",
    "stopwords = set(word.lower() for word in stopwords)  # In Kleinbuchstaben umwandeln\n",
    "\n",
    "# Anzahl der Cluster (angenommen, es sind 5 Cluster)\n",
    "num_clusters = 5\n",
    "\n",
    "for cluster_id in range(num_clusters):\n",
    "    if cluster_id == 0:\n",
    "        continue\n",
    "    # Filter the indices of data points in the current cluster\n",
    "    cluster_indices = np.where(cluster_labels == cluster_id)\n",
    "\n",
    "    # Get the corresponding rows from the DataFrame 'df'\n",
    "    cluster_data_rows = df.iloc[cluster_indices]\n",
    "\n",
    "    # Extract and preprocess text data\n",
    "    texts = cluster_data_rows['text']\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "    processed_texts = [' '.join([token.text for token in nlp(text) if not token.is_stop and token.text.lower() not in stopwords]) for text in texts]\n",
    "\n",
    "    # Create a Word Cloud for the current cluster\n",
    "    all_text = ' '.join(processed_texts)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "\n",
    "    # Display the Word Cloud for the current cluster\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Word Cloud for Cluster {cluster_id}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "question=\"Welche Kompetenzen hat Prf. Gallwitz?\"\n",
    "# Assume you have a DataFrame df with a 'text' column that contains your documents\n",
    "# Also, assume you have a 'question' string for which you want to find relevant documents.\n",
    "\n",
    "# Create a TF-IDF vectorizer with the same parameters as before\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on your documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Transform the question string into TF-IDF representation\n",
    "question_tfidf = tfidf_vectorizer.transform([question])\n",
    "\n",
    "# Calculate cosine similarities between the question and all documents\n",
    "similarities = cosine_similarity(question_tfidf, tfidf_matrix)\n",
    "\n",
    "# Create a DataFrame to store similarities and document texts\n",
    "similarity_df = pd.DataFrame({\n",
    "    'Similarity': similarities[0],\n",
    "    'Text': df['text']\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by similarity in descending order\n",
    "sorted_similarity_df = similarity_df.sort_values(by='Similarity', ascending=False)\n",
    "\n",
    "# Print the top N most relevant documents (e.g., top 5)\n",
    "top_n = 5\n",
    "relevant_documents = sorted_similarity_df.head(top_n)\n",
    "\n",
    "# Print the relevant documents and their similarities to the question\n",
    "for index, row in relevant_documents.iterrows():\n",
    "    print(f\"Similarity: {row['Similarity']}\")\n",
    "    print(row['Text'])\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database = 'discord_bot/scrap/html.sqlite'\n",
    "\n",
    "# with sqlite3.connect(database) as con:\n",
    "#     html_df.to_sql('html_with_embeddings', con, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dokument--> bert anwenden für jeden dokument\n",
    "# question-bert anwenden\n",
    "question=\"was macht Gallwitz?\"\n",
    "document=df[\"text\"][1]\n",
    "\n",
    "tokens_question = tokenizer.tokenize(question)\n",
    "tokens_document = tokenizer.tokenize(document)\n",
    "attetion_mask_question = [1] * len(tokens_question)\n",
    "attention_mask_dokument = [1] * len(tokens_document)\n",
    "\n",
    "token_idss = tokenizer.convert_tokens_to_ids(tokens_question)\n",
    "tokenDocument_idss = tokenizer.convert_tokens_to_ids(tokens_document)\n",
    "\n",
    "\n",
    "tokens_tensor = torch.tensor([token_idss])\n",
    "segments_tensors = torch.tensor([attetion_mask_question])\n",
    "\n",
    "tokensDocument_tensor = torch.tensor([tokenDocument_idss])\n",
    "segmentsDocument_tensors = torch.tensor([attention_mask_dokument])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokensDocument_tensor, segmentsDocument_tensors)\n",
    "    hiddenDocuments_states = outputs[2]\n",
    "\n",
    "# print(token_idss)\n",
    "# print(tokenDocument_idss)\n",
    "\n",
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_vecs_sum = []\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding)\n",
    "\n",
    "\n",
    "tokenDocuments_vecs = hiddenDocuments_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentenceDocument_embedding = torch.mean(tokenDocuments_vecs, dim=0)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(sentence_embedding, sentenceDocument_embedding)\n",
    "\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Fachhochschulgesetz\"\n",
    "\n",
    "df.loc[df[\"text\"].str.contains(word)][\"text\"]\n",
    "# [text for text in df[\"text\"] if word in text][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"chunk_embeddings\")\n",
    "questions = [\"Was besagt das Fachhochschulgesetz?\", \n",
    "             \"Wo befindet sich die Mensa\", \n",
    "             \"Welche Professoren gibt es an der Technischen Hochschule Nürnberg?\",\n",
    "             \"Gib mir alle Infos zum Studienstart\",\n",
    "             \"Was gibt es Neues im Bezug auf Künstliche Intelligenz an der Hochschule?\",\n",
    "             \"Themen für eine Batchelorarbeit\",\n",
    "             \"Where is the Language office?\",\n",
    "             \"Where can i find the Mensa\",\n",
    "             \"Give me information on beginning of Semester\",\n",
    "             \"How many professors are there at the TH?\"]\n",
    "question = questions[random.randint(0,9)]\n",
    "question_embedding = question_embeddings(question)\n",
    "\n",
    "df[\"distance\"] = [1 - cosine(json.loads(embedding), question_embedding) for embedding in df[\"chunk_embeddings_2\"]]\n",
    "most_similar_documents = df.nsmallest(5, \"distance\")\n",
    "# print(f\"question embedding: {question_embedding[:10]}\")\n",
    "print(question)\n",
    "print(most_similar_documents[\"chunk_text\"].to_markdown())\n",
    "\n",
    "\n",
    "df[\"distance\"].plot(kind='hist', bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"word_embeddings\", [\"filename\", \"title\", \"text\", \"tokens\"])\n",
    "df[\"token_ids\"] = [tokenizer.convert_tokens_to_ids(json.loads(tokens)) for tokens in df[\"tokens\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splice dokuments in 512 token chunks\n",
    "\n",
    "# Initialize an empty list to store rows for the new DataFrame\n",
    "new_rows = []\n",
    "\n",
    "# Function to split text and tokens into chunks of 512 tokens\n",
    "def split_text_and_tokens(row):\n",
    "    text = row['text']\n",
    "    tokens_ids = row['token_ids']\n",
    "    filename = row['filename']\n",
    "\n",
    "    if len(tokens_ids) > 512:\n",
    "        # Split into multiple chunks\n",
    "        for i in range(0, len(tokens_ids), 512):\n",
    "            chunk_tokens = tokens_ids[i:i + 512]\n",
    "            chunk_text = tokenizer.decode(chunk_tokens)\n",
    "\n",
    "            # Create a new row with a reference to the original row\n",
    "            new_row = {'filename': filename, 'chunk_id': i/512, 'chunk_text': chunk_text, 'chunk_tokens_json': json.dumps(chunk_tokens)}\n",
    "            new_rows.append(new_row)\n",
    "    else:\n",
    "        # If the row has 512 tokens or fewer, keep it as is\n",
    "        new_row = {'filename': filename, 'chunk_id': 0, 'chunk_text': text, 'chunk_tokens_json': json.dumps(tokens_ids) }\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "# Apply the function to each row in the original DataFrame\n",
    "df.apply(split_text_and_tokens, axis=1)\n",
    "\n",
    "# Create a new DataFrame from the list of new rows\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Reset the index of the new DataFrame if needed\n",
    "new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(new_df.to_markdown())\n",
    "\n",
    "# tokenizer.convert_tokens_to_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus den 2433 Dokumenten die wir eigentlich gescraped haben, sind nun 6945 chunks entstanden es hat sich fast verdreifacht. Wenn man die 787 Seiten ohne Inhalt abzieht, hat sich die Anzahl von 1646 auf 6158 fast vervierfacht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"chunk_tokens_json\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(new_df, \"chunk_word_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "database_path = os.getenv(\"DATABASE_PATH\")\n",
    "\n",
    "def merge_db_tables():\n",
    "    # Connect to your SQLite database\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Create the new table using the structure of the first table (chunk_word_embeddings_0)\n",
    "    cursor.execute('''CREATE TABLE chunk_word_embeddings_all AS SELECT * FROM chunk_word_embeddings_0 WHERE 0''')\n",
    "    # Insert data from the other tables into the new table\n",
    "    for i in range(1, 8):\n",
    "        cursor.execute(f'INSERT INTO chunk_word_embeddings_all SELECT * FROM chunk_word_embeddings_{i}')\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "merge_db_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate embeddings with SentenceTransformer from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "df = db_get_df(\"html_attrs_de\", [\"filename\", \"title\", \"text\"])\n",
    "# Load spaCy for sentence segmentation\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "#TODO speichere embeddings in databank\n",
    "sentences = []\n",
    "embeddings = []\n",
    "\n",
    "for text in df['text']:\n",
    "    # Split the text into sentences using spaCy\n",
    "    doc = nlp(text)\n",
    "    sentence_list = [sent.text for sent in doc.sents]\n",
    "    sentences.append(sentence_list)\n",
    "    \n",
    "    # Encode each sentence using Sentence Transformers\n",
    "    sentence_embeddings = [model.encode(sentence) for sentence in sentence_list]\n",
    "    embeddings.append(sentence_embeddings)\n",
    "    print(embeddings)\n",
    "\n",
    "# Now, 'sentences' is a list where each element is a list of sentences, and 'embeddings' is a list of corresponding sentence embeddings.\n",
    "# filenames = df.head(3)\n",
    "# print(filenames.to_markdown())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings with sentence-transformers/all-MiniLM-L6-v2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the pre-trained MiniLM model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Create a list to store the embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Assuming 'text' column holds your HTML content\n",
    "for text in df['text']:\n",
    "    # Encode the text and append the resulting embedding to the list\n",
    "    text_embedding = model.encode(text)\n",
    "    embeddings.append(text_embedding)\n",
    "\n",
    "# Create a new DataFrame to store the embeddings\n",
    "embeddings_df1 = pd.DataFrame(embeddings)\n",
    "\n",
    "# Now 'embeddings_df' contains the embeddings for each text in your DataFrame\n",
    "db_save_df(embeddings_df1, \"embeddings1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings with 'paraphrase-MiniLM-L6-v2' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "df = db_get_df(\"html_attrs_de\", [\"filename\", \"title\", \"text\"])\n",
    "\n",
    "# Load the 'paraphrase-MiniLM-L6-v2' model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Create a list to store the embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Assuming 'text' column holds your HTML content\n",
    "for text in df['text']:\n",
    "    # Encode the text and append the resulting embedding to the list\n",
    "    text_embedding = model.encode(text)\n",
    "    embeddings.append(text_embedding)\n",
    "\n",
    "# Create a new DataFrame to store the embeddings\n",
    "embeddings_df2 = pd.DataFrame(embeddings)\n",
    "\n",
    "# Now 'embeddings_df' contains the embeddings for each text in your DataFrame using the 'paraphrase-MiniLM-L6-v2' model\n",
    "db_save_df(embeddings_df2, \"embeddings2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings with 'sentence-transformers/msmarco-distilbert-base-tas-b' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the pre-trained Multi-QA MiniLM model\n",
    "model = SentenceTransformer('sentence-transformers/msmarco-distilbert-base-tas-b')\n",
    "\n",
    "# Create a list to store the embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Assuming 'text' column holds your HTML content\n",
    "for text in tqdm(df['text'], desc=\"Encoding Texts\"):\n",
    "    # Encode the text and append the resulting embedding to the list\n",
    "    text_embedding = model.encode(text)\n",
    "    embeddings.append(text_embedding)\n",
    "\n",
    "# Create a new DataFrame to store the embeddings\n",
    "embeddings_df3 = pd.DataFrame(embeddings)\n",
    "db_save_df(embeddings_df3, \"embeddings3\")\n",
    "\n",
    "# Now 'embeddings_df' contains the embeddings for each text in your DataFrame using the multi-qa model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings with 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the pre-trained Multi-QA MiniLM model\n",
    "model = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Create a list to store the embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Assuming 'text' column holds your HTML content\n",
    "for text in tqdm(df['text'], desc=\"Encoding Texts\"):\n",
    "    # Encode the text and append the resulting embedding to the list\n",
    "    text_embedding = model.encode(text)\n",
    "    embeddings.append(text_embedding)\n",
    "\n",
    "# Create a new DataFrame to store the embeddings\n",
    "embeddings_df4 = pd.DataFrame(embeddings)\n",
    "db_save_df(embeddings_df4, \"embeddings4\")\n",
    "\n",
    "# Now 'embeddings_df' contains the embeddings for each text in your DataFrame using the multi-qa model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = db_get_df(\"html_attrs\", [\"filename\", \"title\", \"text\"])\n",
    "# Load the pre-trained MiniLM model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Create an empty list to store similarities\n",
    "similarities = []\n",
    "\n",
    "# Given a query or question\n",
    "query = \"welche kompetenzen hat Norbert\"\n",
    "\n",
    "# Encode the query\n",
    "query_embedding = model.encode(query, show_progress_bar=True)\n",
    "text_embeddings = []\n",
    "\n",
    "# Iterate through each row in the DataFrame with a progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating similarities\"):\n",
    "    # Encode the text in each row\n",
    "    text_embedding = model.encode(row['text'])\n",
    "    text_embeddings.append({'index': index, 'text_embedding': text_embedding})\n",
    "    # Calculate similarity between the query embedding and text embedding\n",
    "    similarity = cosine_similarity([query_embedding], [text_embedding])[0][0]\n",
    "\n",
    "    # Store the similarity along with the index\n",
    "    similarities.append({'index': index, 'similarity': similarity})\n",
    "embeddings_df = pd.DataFrame(text_embeddings)\n",
    "db_save_df(embeddings_df, \"embeddings_paraphrase_MiniLM_L6_v2\")\n",
    "# Convert the list of similarities into a DataFrame\n",
    "similarities_df = pd.DataFrame(similarities)\n",
    "db_save_df(similarities_df, \"similarities1\")\n",
    "# Get the top N similar articles\n",
    "top_n = 5\n",
    "most_similar_indices = similarities_df.nlargest(top_n, 'similarity')['index']\n",
    "most_similar_articles = df.loc[most_similar_indices]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in most_similar_articles.iterrows():\n",
    "    print(f\"Article Index: {index}\")\n",
    "    print(row['text'])  # Display the text of the article\n",
    "    print(\"\\n-----------------------------------\\n\")\n",
    "    \n",
    "# Annahme: similarities_df ist ein DataFrame mit einer Spalte 'similarity'\n",
    "similarities_df['similarity'].plot(kind='hist', bins=200)\n",
    "plt.xlabel('Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Similarity Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Laden Sie embeddings_df aus der Datenbank\n",
    "embeddings_df = db_get_df(\"embeddings_paraphrase_MiniLM_L6_v2\")\n",
    "\n",
    "# Given a query or question\n",
    "query = \"welche kompetenzen hat Norbert\"\n",
    "\n",
    "# Load the pre-trained MiniLM model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Encode the query\n",
    "query_embedding = model.encode(query, show_progress_bar=True)\n",
    "\n",
    "# Create an empty list to store similarities\n",
    "similarities = []\n",
    "\n",
    "# Iterate through each row in the DataFrame with a progress bar\n",
    "for index, row in tqdm(embeddings_df.iterrows(), total=len(embeddings_df), desc=\"Calculating similarities\"):\n",
    "    # Get the pre-computed text embedding from the DataFrame\n",
    "    text_embedding_bytes = row['text_embedding']\n",
    "    \n",
    "    # Convert the bytestring to a Numpy array\n",
    "    text_embedding = pickle.loads(text_embedding_bytes)\n",
    "    \n",
    "    # Calculate similarity between the query embedding and text embedding\n",
    "    similarity = cosine_similarity([query_embedding], [text_embedding])[0][0]\n",
    "\n",
    "    # Store the similarity along with the index\n",
    "    similarities.append({'index': index, 'similarity': similarity})\n",
    "\n",
    "# Convert the list of similarities into a DataFrame\n",
    "similarities_df = pd.DataFrame(similarities)\n",
    "# Get the top N similar articles\n",
    "top_n = 5\n",
    "most_similar_indices = similarities_df.nlargest(top_n, 'similarity')['index']\n",
    "most_similar_articles = df.loc[most_similar_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the pre-trained MiniLM model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Given a query or question\n",
    "query = \"welche kompetenzen hat gallwitz\"\n",
    "\n",
    "# Encode the query\n",
    "query_embedding = model.encode(query, show_progress_bar=True)\n",
    "\n",
    "# Get article embeddings\n",
    "article_embeddings = model.encode(df['text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# Concatenate query embedding and article embeddings\n",
    "all_embeddings = np.vstack([query_embedding, article_embeddings])\n",
    "\n",
    "# Perform dimensionality reduction with t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "# Separate query embedding and article embeddings\n",
    "query_2d = embeddings_2d[0]\n",
    "article_embeddings_2d = embeddings_2d[1:]\n",
    "\n",
    "# Plot the t-SNE visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(article_embeddings_2d[:, 0], article_embeddings_2d[:, 1], label='Article Embeddings')\n",
    "plt.scatter(query_2d[0], query_2d[1], color='red', label='Query Embedding')\n",
    "plt.legend()\n",
    "plt.title('t-SNE Visualization of Query Embedding and Article Embeddings')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
