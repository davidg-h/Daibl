{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Processing</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ziel dieses Notebooks ist, die gescrapeten Daten so zu verarbeiten, dass Sie später thematisch durchsucht werden können. Ein Embeddingvektor liefert genau diese Funktion. Auf eine Anfrage hin wird der Abstand zwischen dem Embedding der Frage und den Embeddings aller anderen Dokumenten berechnet. Dann werden die Dokumente mit den kleinsten Abständen ausgewählt und dem LLM als Kontext mitgegeben. Mithilfe des Wissens dieser Dokumente soll dass LLM dann in der Lage sein die Frage korrekt zu beantworten.\n",
    "Für die Embeddings benutzen wir [Google Bert](https://blog.google/products/search/search-language-understanding-bert/)\n",
    "\n",
    "Beispiel:\n",
    "\n",
    "User: Welche Dozenten unterrichten das Fach Grundlagen der Informatik?\n",
    "\n",
    "System wählt besten 5 Dokumente aus \n",
    "\n",
    "    <Dokument 1>: ... betreute Prof. Dr. Löhr eine Batchelorarbeit in Grundlagen der Informatik...\n",
    "    <Dokument 2>: Prof. Dr. Weber tel.: 013882664 email: weber@th.de Raum: HQ: 403, Fächer: Grundlagen der Informatik ...\n",
    "    <Dokument 3> ...\n",
    "    <Dokument 4> ...\n",
    "    <Dokument 5> ...\n",
    "    \n",
    "\n",
    "Aus der Nutzeranfrage und den Dokumenten wird eine neue Query erstellt, die dem LLM dann final bereitgetellt wird. Diese sieht in etwa so aus:\n",
    "\n",
    "    \n",
    "    <Dokument 1>: ... betreute Prof. Dr. Löhr eine Batchelorarbeit in Grundlagen der     Informatik...\n",
    "    <Dokument 2>: Prof. Dr. Weber tel.: 013882664 email: weber@th.de Raum: HQ: 403, Fächer: Grundlagen der Informatik ...\n",
    "    <Dokument 3> ...\n",
    "    <Dokument 4> ...\n",
    "    <Dokument 5> ...\n",
    "\n",
    "    Bitte beantworte folgende Frage unter der Berücksichtigung obiger Dokumente:\n",
    "    Welche Dozenten unterrichten das Fach Grundlagen der Informatik?\n",
    "\n",
    "\n",
    "Das LLM wird daraufhin hoffentlich korrekt eine Antwort liefern die ähnlich ist zu:\n",
    "\n",
    "    A: An der TH Nürnberg Georg Simon Ohm unterichten die Professoren Prof. Dr. Löhr und Prof. Dr. Weber das Fach Grundlagen der Informatik.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cosine\n",
    "from db_init import db_get_df, db_save_df\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst laden wir die Daten aus der Datenbank. Dabei besitzt jedes Dokument als Metadaten den Titel der Webseite, den filenamen und den Text. Diese speichern wir uns in einen Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"html_attrs\", [\"filename\", \"title\", \"text\"])\n",
    "\n",
    "print(df.dtypes)\n",
    "print(df[\"text\"][3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur überprüfung der Texte können wir nun einmal eine Keywordsuche starten. Dieser Ansatz wird außerdem tiefer im Notebook [spacy_keywordextraction](./spacy_keywordextraction.ipynb) verfolgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Gallwitz\"\n",
    "\n",
    "[text for text in df[\"text\"] if word in text][:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt werden wir für jedes Dokument ein eigenes Word embeddings erstellen. Dazu müssen wir zunächst das BERT Model laden.\n",
    "Das BERT Model ist ein von Google trainiertes mehrschichtiges neuronales Netz, welches ursprünglich dafür entwickelt wurde, dass ???\n",
    "es ist trainiert auf 10.000+ Büchern\n",
    "es gibt Modelle \"base\" und \"large\"\n",
    "uncased heißt ohne klein - Großschreibung\n",
    "\n",
    "Wir brauchen zur vorbereitung die zusätzlichen Token\n",
    "[SEP] um das Ende eines Satzes zu markieren\n",
    "[CLS] am Anfang des Texten\n",
    "[PAD] zum auffüllen der Token \n",
    "Außerdem\n",
    "TokenIDs\n",
    "MaskIDs - zum filtern der [PAD]\n",
    "Segment IDs um verschiedene Sätze zu unterscheiden\n",
    "Posititional Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased') #TODO try better model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "testSentence = \"In der Bibliothek gibt es 40 Bücher zu Thema Animes\"\n",
    "tokens_question = tokenizer.tokenize(testSentence)\n",
    "tokens_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] = [tokenizer.tokenize(text) for text in df[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "print(df[\"tokens\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_question = ['[CLS]'] + tokens_question + ['[SEP]']\n",
    "attention_mask = [1 if token != \"[PAD]\" else 0  for token in tokens_question]\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens_question)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "\n",
    "attention_mask = torch.tensor(attention_mask).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(token_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proccessSentence(tokens):\n",
    "    if len(tokens) == 0:\n",
    "        # Handle the case when the token list is empty, for example, return a default embedding or raise an exception.\n",
    "        # For demonstration purposes, we'll return a zero tensor as the default embedding.\n",
    "        return torch.zeros(768)\n",
    "\n",
    "    # Ensure the token sequence length is no longer than the maximum sequence length the model can handle (512)\n",
    "    if len(tokens) > 512:\n",
    "        tokens = tokens[:512]\n",
    "\n",
    "    # Padding the token sequence to the maximum sequence length if it's shorter\n",
    "    if len(tokens) < 512:\n",
    "        tokens += ['[PAD]'] * (512 - len(tokens))\n",
    "\n",
    "    attention_mask = [1 if token != \"[PAD]\" else 0  for token in tokens]\n",
    "    tokenDocument_idss = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    tokensDocument_tensor = torch.tensor([tokenDocument_idss], dtype=torch.int64)\n",
    "    segmentsDocument_tensors = torch.tensor([attention_mask], dtype=torch.int64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokensDocument_tensor, segmentsDocument_tensors)\n",
    "        hiddenDocuments_states = outputs[2]\n",
    "\n",
    "    tokenDocuments_vecs = hiddenDocuments_states[-2][0]\n",
    "    sentenceDocument_embedding = torch.mean(tokenDocuments_vecs, dim=0)\n",
    "    #print(\"Our final sentence embedding vector of shape:\", sentenceDocument_embedding)\n",
    "\n",
    "    return sentenceDocument_embedding\n",
    "\n",
    "df[\"word_embeddings\"] = [proccessSentence(tokens).tolist() for tokens in tqdm(df[\"tokens\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "database_path = os.getenv(\"DATABASE_PATH\")\n",
    "\n",
    "def test_db():\n",
    "    con = sqlite3.connect(database_path)\n",
    "    # sql_query = \"\"\"SELECT count(chunk_word_embeddings_3) FROM chunk_word_embeddings_3\"\"\"\n",
    "    sql = \"\"\"SELECT distinct tbl_name from sqlite_master\"\"\"\n",
    "    # sql = \"\"\"SELECT chunk_word_embeddings_5 FROM chunk_word_embeddings_3 LIMIT 1\"\"\"\n",
    "    # sql = \"\"\"DROP TABLE chunk_word_embeddings_0\"\"\"\n",
    "\n",
    "    cursor = con.cursor()\n",
    "    cursor.execute(sql)\n",
    "    res = cursor.fetchall()\n",
    "    print(res)\n",
    "    con.close()\n",
    "test_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE \n",
    "import numpy as np  \n",
    "import json\n",
    "\n",
    "word_embeddings_copy = db_get_df(\"word_embeddings\",[\"word_embeddings\"])\n",
    "print(json.loads(word_embeddings_copy[\"word_embeddings\"][0]))\n",
    "word_embeddings_copy=[json.loads(embedding) for embedding in tqdm(word_embeddings_copy[\"word_embeddings\"])]\n",
    "# Verwenden Sie 'ast.literal_eval', um Zeichenketten in Listen umzuwandeln\n",
    "#word_embeddings_copy = word_embeddings_copy.map(ast.literal_eval)\n",
    "\n",
    "# Wandeln Sie die Listen von Listen in ein Numpy-Array um\n",
    "word_embeddings = np.array(word_embeddings_copy)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "X_embedded = tsne.fit_transform(word_embeddings)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_embedded[:, 0], X_embedded[:, 1], s=5)\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE \n",
    "import numpy as np  \n",
    "import json\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Funktion zur Extraktion von Word Embeddings für die Frage\n",
    "def get_word_embedding(question, model_name='bert-base-uncased'):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    tokens = tokenizer(question, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        question_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return question_embedding\n",
    "\n",
    "# Laden der Word Embeddings\n",
    "word_embeddings_copy = db_get_df(\"word_embeddings\", [\"word_embeddings\"])\n",
    "word_embeddings_copy = [json.loads(embedding) for embedding in tqdm(word_embeddings_copy[\"word_embeddings\"])]\n",
    "word_embeddings = np.array(word_embeddings_copy)\n",
    "\n",
    "# Extrahieren der Embeddings für die Frage\n",
    "question_text = \"n?\"\n",
    "question_embedding = get_word_embedding(question_text)\n",
    "\n",
    "# Berechnen der Kosinus-Ähnlichkeit zwischen der Frage und den anderen Word Embeddings\n",
    "similarities = cosine_similarity(word_embeddings, [question_embedding])\n",
    "\n",
    "# 'similarities' ist jetzt ein Array mit den Kosinus-Ähnlichkeiten zwischen der Frage und den anderen Word Embeddings.\n",
    "\n",
    "# Kombinieren Sie die t-SNE-Komponenten mit den Kosinus-Ähnlichkeiten\n",
    "combined_features = np.column_stack((X_embedded, similarities))\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(combined_features[:, 0], combined_features[:, 1], s=5)\n",
    "plt.scatter(question_embedding[0], question_embedding[1], color='red', s=50, label='Ihre Frage')\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings with Cosine Similarity\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Annahme: X_embedded ist Ihre t-SNE-Visualisierung\n",
    "# Annahme: word_embeddings ist Ihre Matrix der Word Embeddings\n",
    "# Annahme: question_text ist Ihre Frage\n",
    "# Annahme: n_clusters ist die Anzahl der gewünschten Cluster\n",
    "question_text = \"Welche Kompetenzen hat Pr. Gallwitz?\" #wann ist der Bewerbungszeitraum  Für das Wintersemester\n",
    "\n",
    "# Schritt 1: Clustering durchführen\n",
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "cluster_labels = kmeans.fit_predict(word_embeddings)\n",
    "\n",
    "# Schritt 3: Berechnen der Ähnlichkeit zur Frage\n",
    "question_embedding = get_word_embedding(question_text)  # Verwenden Sie Ihre get_word_embedding Funktion\n",
    "similarities = cosine_similarity(word_embeddings, [question_embedding])\n",
    "\n",
    "# Schritt 4: Visualisierung aktualisieren\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(5):\n",
    "    plt.scatter(X_embedded[cluster_labels == i, 0], X_embedded[cluster_labels == i, 1], s=5, label=f'Cluster {i}')\n",
    "\n",
    "# Farben entsprechend des Clusters für die Frage aktualisieren\n",
    "question_cluster = np.argmax(similarities)\n",
    "plt.scatter(X_embedded[question_cluster, 0], X_embedded[question_cluster, 1], s=50, color='red', label='Ihre Frage')\n",
    "\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings with Clusters\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5)  # Specify the number of clusters you want\n",
    "cluster_labels = kmeans.fit_predict(word_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(len(np.unique(cluster_labels))):\n",
    "    plt.scatter(X_embedded[cluster_labels == i, 0], X_embedded[cluster_labels == i, 1], s=5, label=f'Cluster {i}')\n",
    "\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings with Clusters\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the db_get_df function to get the DataFrame\n",
    "df = db_get_df()\n",
    "\n",
    "# Print the first 2 rows of the DataFrame\n",
    "first_2_rows = df.head(2)\n",
    "print(first_2_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the indices of data points in Cluster 0\n",
    "cluster0_indices = np.where(cluster_labels == 1)\n",
    "\n",
    "# Get the corresponding rows from the DataFrame 'df'\n",
    "cluster0_data_rows = df.iloc[cluster0_indices]\n",
    "\n",
    "# Print the 'text' column for the data points in Cluster 0\n",
    "for text in cluster0_data_rows['text']:\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Annahme: Ihr DataFrame 'df' enthält eine Spalte 'text' mit den Textdaten.\n",
    "\n",
    "# Anzahl der Cluster (angenommen, es sind 5 Cluster)\n",
    "num_clusters = 5\n",
    "\n",
    "for cluster_id in range(num_clusters):\n",
    "    # Filtern Sie die Zeilen für den aktuellen Cluster\n",
    "    cluster_data_rows = df\n",
    "\n",
    "    # Laden des spaCy-Modells für die Textverarbeitung\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    # Benutzerdefinierte Stoppwortliste\n",
    "    stopwords = {'www', 'th-nuernberg', 'nürnberg', 'nuernberg', 'th', 'technische', 'hochschule', 'ohm', 'de', 'punkt', 'simon'}\n",
    "\n",
    "    # Tokenisieren und Lemmatisieren der Texte, Entfernen der Stoppwörter und Konvertieren in Strings\n",
    "    processed_texts = []\n",
    "    for text in cluster_data_rows['text']:\n",
    "        doc = nlp(text)\n",
    "        processed_tokens = []\n",
    "        for token in doc:\n",
    "            if token.text.lower() not in stopwords and token.pos_ in {'NOUN', 'PROPN'}:\n",
    "                processed_tokens.append(token.text)\n",
    "        processed_texts.append(' '.join(processed_tokens))\n",
    "\n",
    "    # Erstellen eines Wörterbuchs und einer Textkorpus für das LDA-Modell\n",
    "    text_tokens = [text.split() for text in processed_texts]\n",
    "    dictionary = gensim.corpora.Dictionary(text_tokens)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in text_tokens]\n",
    "\n",
    "    # Anwendung des LDA-Modells\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "    # Anzeigen der Hauptthemen für den aktuellen Cluster\n",
    "    print(f\"Cluster {cluster_id} Topics:\")\n",
    "    for topic_id, topic in lda_model.print_topics():\n",
    "        print(f\"Topic {topic_id}: {topic}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_text = ' '.join(processed_texts)  # 'processed_texts' ist die Liste der bereinigten Texte\n",
    "\n",
    "# Erstellen der Word Cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "\n",
    "# Anzeigen der Word Cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Erstellen einer Liste von Stoppwörtern, einschließlich der URL und der benutzerdefinierten Wörter\n",
    "stopwords = set(['www', 'th-nuernberg', 'nürnberg', 'nuernberg', 'th', 'technische', 'hochschule', 'ohm', 'de', 'punkt', 'simon','https','http','nuremberg','telefon','email','fax','Prof Dr','studium'])\n",
    "stopwords = set(word.lower() for word in stopwords)  # In Kleinbuchstaben umwandeln\n",
    "\n",
    "# Anzahl der Cluster (angenommen, es sind 5 Cluster)\n",
    "num_clusters = 5\n",
    "\n",
    "for cluster_id in range(num_clusters):\n",
    "    if cluster_id == 0:\n",
    "        continue\n",
    "    # Filter the indices of data points in the current cluster\n",
    "    cluster_indices = np.where(cluster_labels == cluster_id)\n",
    "\n",
    "    # Get the corresponding rows from the DataFrame 'df'\n",
    "    cluster_data_rows = df.iloc[cluster_indices]\n",
    "\n",
    "    # Extract and preprocess text data\n",
    "    texts = cluster_data_rows['text']\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "    processed_texts = [' '.join([token.text for token in nlp(text) if not token.is_stop and token.text.lower() not in stopwords]) for text in texts]\n",
    "\n",
    "    # Create a Word Cloud for the current cluster\n",
    "    all_text = ' '.join(processed_texts)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "\n",
    "    # Display the Word Cloud for the current cluster\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Word Cloud for Cluster {cluster_id}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "question=\"Welche Kompetenzen hat Prf. Gallwitz?\"\n",
    "# Assume you have a DataFrame df with a 'text' column that contains your documents\n",
    "# Also, assume you have a 'question' string for which you want to find relevant documents.\n",
    "\n",
    "# Create a TF-IDF vectorizer with the same parameters as before\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on your documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Transform the question string into TF-IDF representation\n",
    "question_tfidf = tfidf_vectorizer.transform([question])\n",
    "\n",
    "# Calculate cosine similarities between the question and all documents\n",
    "similarities = cosine_similarity(question_tfidf, tfidf_matrix)\n",
    "\n",
    "# Create a DataFrame to store similarities and document texts\n",
    "similarity_df = pd.DataFrame({\n",
    "    'Similarity': similarities[0],\n",
    "    'Text': df['text']\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by similarity in descending order\n",
    "sorted_similarity_df = similarity_df.sort_values(by='Similarity', ascending=False)\n",
    "\n",
    "# Print the top N most relevant documents (e.g., top 5)\n",
    "top_n = 5\n",
    "relevant_documents = sorted_similarity_df.head(top_n)\n",
    "\n",
    "# Print the relevant documents and their similarities to the question\n",
    "for index, row in relevant_documents.iterrows():\n",
    "    print(f\"Similarity: {row['Similarity']}\")\n",
    "    print(row['Text'])\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database = 'discord_bot/scrap/html.sqlite'\n",
    "\n",
    "# with sqlite3.connect(database) as con:\n",
    "#     html_df.to_sql('html_with_embeddings', con, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dokument--> bert anwenden für jeden dokument\n",
    "# question-bert anwenden\n",
    "question=\"was macht Gallwitz?\"\n",
    "document=df[\"text\"][1]\n",
    "\n",
    "tokens_question = tokenizer.tokenize(question)\n",
    "tokens_document = tokenizer.tokenize(document)\n",
    "attetion_mask_question = [1] * len(tokens_question)\n",
    "attention_mask_dokument = [1] * len(tokens_document)\n",
    "\n",
    "token_idss = tokenizer.convert_tokens_to_ids(tokens_question)\n",
    "tokenDocument_idss = tokenizer.convert_tokens_to_ids(tokens_document)\n",
    "\n",
    "\n",
    "tokens_tensor = torch.tensor([token_idss])\n",
    "segments_tensors = torch.tensor([attetion_mask_question])\n",
    "\n",
    "tokensDocument_tensor = torch.tensor([tokenDocument_idss])\n",
    "segmentsDocument_tensors = torch.tensor([attention_mask_dokument])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokensDocument_tensor, segmentsDocument_tensors)\n",
    "    hiddenDocuments_states = outputs[2]\n",
    "\n",
    "# print(token_idss)\n",
    "# print(tokenDocument_idss)\n",
    "\n",
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_vecs_sum = []\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding)\n",
    "\n",
    "\n",
    "tokenDocuments_vecs = hiddenDocuments_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentenceDocument_embedding = torch.mean(tokenDocuments_vecs, dim=0)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(sentence_embedding, sentenceDocument_embedding)\n",
    "\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'html.sqlite'\n",
    "sql = \"\"\"\n",
    "SELECT filename, title, text, word_embeddings FROM word_embeddings\n",
    "\"\"\"\n",
    "\n",
    "con = sqlite3.connect(database)\n",
    "df = pd.read_sql_query(sql, con)\n",
    "con.close()\n",
    "\n",
    "print(df.dtypes)\n",
    "print(df[\"text\"][3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(len(df[\"text\"][4]))\n",
    "# print(json.loads(df[\"text\"][0]))\n",
    "diff_bank = 1 - cosine(json.loads(df[\"word_embeddings\"][3]), json.loads(df[\"word_embeddings\"][6]))\n",
    "# for embed in df[\"word_embeddings\"]:\n",
    "# print(embed)\n",
    "\n",
    "print(diff_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"][1746]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Fachhochschulgesetz\"\n",
    "\n",
    "df.loc[df[\"text\"].str.contains(word)][\"text\"]\n",
    "# [text for text in df[\"text\"] if word in text][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/br/Projects/IT-Ptojekt-Chatbot/daibl/discord_bot')\n",
    "from question_embedding import question_embeddings\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# TODO 10 Fragen\n",
    "# TODO TSNE\n",
    "\n",
    "df = db_get_df(\"chunk_word_embeddings_all\")\n",
    "print(df.dtypes)\n",
    "question = \"TH\"\n",
    "question_embedding = question_embeddings(question)\n",
    "\n",
    "df[\"distance\"] = [1 - cosine(json.loads(embedding), question_embedding) for embedding in df[\"chunk_word_embeddings\"]]\n",
    "most_similar_documents = df.nsmallest(5, \"distance\")\n",
    "print(f\"question embedding: {question_embedding[:10]}\")\n",
    "print(most_similar_documents[\"distance\"])\n",
    "\n",
    "\n",
    "df[\"distance\"].plot(kind='hist', bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"word_embeddings\", [\"filename\", \"title\", \"text\", \"tokens\"])\n",
    "df[\"token_ids\"] = [tokenizer.convert_tokens_to_ids(json.loads(tokens)) for tokens in df[\"tokens\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splice dokuments in 512 token chunks\n",
    "\n",
    "# Initialize an empty list to store rows for the new DataFrame\n",
    "new_rows = []\n",
    "\n",
    "# Function to split text and tokens into chunks of 512 tokens\n",
    "def split_text_and_tokens(row):\n",
    "    text = row['text']\n",
    "    tokens_ids = row['token_ids']\n",
    "    filename = row['filename']\n",
    "\n",
    "    if len(tokens_ids) > 512:\n",
    "        # Split into multiple chunks\n",
    "        for i in range(0, len(tokens_ids), 512):\n",
    "            chunk_tokens = tokens_ids[i:i + 512]\n",
    "            chunk_text = tokenizer.decode(chunk_tokens)\n",
    "\n",
    "            # Create a new row with a reference to the original row\n",
    "            new_row = {'filename': filename, 'chunk_id': i/512, 'chunk_text': chunk_text, 'chunk_tokens_json': json.dumps(chunk_tokens)}\n",
    "            new_rows.append(new_row)\n",
    "    else:\n",
    "        # If the row has 512 tokens or fewer, keep it as is\n",
    "        new_row = {'filename': filename, 'chunk_id': 0, 'chunk_text': text, 'chunk_tokens_json': json.dumps(tokens_ids) }\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "# Apply the function to each row in the original DataFrame\n",
    "df.apply(split_text_and_tokens, axis=1)\n",
    "\n",
    "# Create a new DataFrame from the list of new rows\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Reset the index of the new DataFrame if needed\n",
    "new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(new_df.to_markdown())\n",
    "\n",
    "# tokenizer.convert_tokens_to_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus den 2433 Dokumenten die wir eigentlich gescraped haben, sind nun 6945 chunks entstanden es hat sich fast verdreifacht. Wenn man die 787 Seiten ohne Inhalt abzieht, hat sich die Anzahl von 1646 auf 6158 fast vervierfacht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"chunk_tokens_json\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(new_df, \"chunk_word_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "database_path = os.getenv(\"DATABASE_PATH\")\n",
    "\n",
    "def merge_db_tables():\n",
    "    # Connect to your SQLite database\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Create the new table using the structure of the first table (chunk_word_embeddings_0)\n",
    "    cursor.execute('''CREATE TABLE chunk_word_embeddings_all AS SELECT * FROM chunk_word_embeddings_0 WHERE 0''')\n",
    "    # Insert data from the other tables into the new table\n",
    "    for i in range(1, 8):\n",
    "        cursor.execute(f'INSERT INTO chunk_word_embeddings_all SELECT * FROM chunk_word_embeddings_{i}')\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "merge_db_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
