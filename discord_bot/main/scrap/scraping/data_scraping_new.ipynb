{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Die Webseite der TH Nürnberg Scrapen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einleitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Webseite der TH-Nürnberg wirkt als Ausgangspunkt für die Wissensgrundlage des Chatbots.\n",
    "\n",
    "Conventions: \n",
    "- pandas Spaltennamen im Sigular\n",
    "- Die meißten Links sind keine URLs, da sie lokal sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import pandas as pd\n",
    "from db_init import db_get_df, db_save_df\n",
    "import glob\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapen der home Seite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Ausganspunkt für unsere Daten nutzen wir die Home Webseite der TH-Nürnberg. (https://www.th-nuernberg.de/)\n",
    "Diese Website downloaden wir und suchen alle Links auf andere Webseiten und speichern diese Links in eine Liste.\n",
    "Als nächsten Schritt rufen wir alle Links aus dieser Liste Auf und sammeln wiederum alle Links von jeder dieser Webseiten.\n",
    "In der daraus resultierenden Liste sortieren wir alle Links aus, die nicht auf die Webseite der TH verweisen.\n",
    "Dann laden wir alle Dokumente herunter und speichern sie in der Datenbank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Funktion, die eine URL als parameter nimmt und das HTML file zurückgibt, falls die Seite existiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_html_from_url(url):\n",
    "    res = requests.get(url)\n",
    "    html = \"\"\n",
    "    if res.ok:\n",
    "        html =  res.text\n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Funktion, die ein HTML file nach links durchsucht und alle gefundenen externen und internen Links zurückgibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_one_html(html):\n",
    "    soup = BeautifulSoup(html,\"lxml\")\n",
    "    urls = [a[\"href\"] for a in soup.find_all('a', href=True)]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt geben wir unsere initiale URL an und extrahieren alle Links aus dieser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.th-nuernberg.de/\"\n",
    "html = download_html_from_url(BASE_URL)\n",
    "links = get_links_from_one_html(html)\n",
    "df = pd.DataFrame({\"link\": links})\n",
    "print(*df[\"link\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links filtern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst können wir alle Links überprüfen, ob sie Parameter oder sections mit \n",
    "- *?param1=hallo*    \n",
    "- *#section*        \n",
    "\n",
    "Beide Attribute sind für den Download der Webseiten nicht notwendig und erschweren das finden von Dublikaten also schneiden wir sie hinten ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Externe Links filtern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir Mal nachschauen, auf welche externen Seiten die THN Webseite verweist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_extern_urls(urls):\n",
    "    filtered_links = []\n",
    "    for link in urls:\n",
    "        if link.startswith(\"http\"):\n",
    "            filtered_links.append(link)\n",
    "\n",
    "    return filtered_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extern_links = filter_extern_urls(df[\"link\"])\n",
    "print(\"Anzahl externer Links: \", len(extern_links))\n",
    "print(extern_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Seite verweist also auf alle gängigen Sozial Media Seiten, wie Twitter, youtube, tiktok (der nicht existiert), instagram (der nicht existiert), xing, oft auf die jobbörse mit mehreren Links ins Intranet und auf die Efi fakultät,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interne Links filtern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir filtern nun noch alle Links heraus, die keine Text-inhalte besitzen oder auf die Selbe Seite referieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_intern_links(urls):\n",
    "    filtered_links = []\n",
    "    for url in urls:\n",
    "        if url.startswith(\"#\"):\n",
    "            continue\n",
    "        elif url.startswith(\"http\"):\n",
    "            continue\n",
    "        elif url.startswith(\"mailto:\"):\n",
    "            continue\n",
    "        elif url.startswith(\"javascript:\"):\n",
    "            continue\n",
    "        elif url.startswith(\"&#\"): # is encoded mailto\n",
    "            continue\n",
    "        elif \".xml\" in url:\n",
    "            continue\n",
    "        elif \".pdf\" in url:\n",
    "            continue\n",
    "        elif url == \"/\":\n",
    "            continue\n",
    "        else:\n",
    "            filtered_links.append(url)\n",
    "    return filtered_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intern_links = filter_intern_links(df[\"link\"])\n",
    "print(\"Anzahl interner Links: \", len(intern_links))\n",
    "print(intern_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dublikate entfernen und sortieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir die Links filtern, anschließend alphabetisch sortieren und dann Dublikate URLs entfernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_and_remove_dublicates(df):\n",
    "    df = df.sort_values(\"link\")\n",
    "    df = df.drop_duplicates(subset=\"link\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intern_links = filter_intern_links(df[\"link\"])\n",
    "df = pd.DataFrame({\"link\": intern_links})\n",
    "df = sort_and_remove_dublicates(df)\n",
    "print(\"Anzahl interner Links (ohne Dublikate): \", len(intern_links))\n",
    "print(*df[\"link\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weitere Seiten nach urls scrapen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir habe also 229 Seiten., die wir jetzt alle nochmal nach links durchforsten können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in tqdm(df[\"url\"]):\n",
    "    new_urls = get_links_from_one_url(\"https://www.th-nuernberg.de\" + url)\n",
    "    new_rows = []\n",
    "    for new_url in new_urls:\n",
    "        new_rows.append({'url': new_url})\n",
    "    df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir wieder schauen, wie viele Interne und externe URLs wir bekommen haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anzahl ungefilterter URLs\", len(df))\n",
    "df = sort_and_remove_dublicates(df)\n",
    "print(\"Anzahl gefilterter URLs\", len(df))\n",
    "extern_links = filter_extern_urls(df[\"url\"])\n",
    "print(\"Anzahl externer URLs\", len(extern_links))\n",
    "intern_links = filter_intern_links(df[\"url\"])\n",
    "print(\"Anzahl interner URLs\", len(intern_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links abspeichern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für unsere weiteren Schritte werden wir immer nur interne Links verwenden, deshalb speichern wir an dieser Stelle mal die internen Links ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"only_links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloaden der files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir mit dem downloaden anfangen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Funktion Läd nun alle Html files zu den Links herunter und speichert sie im Dataframe neben den \"link\" in einer Spalte \"html\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all_urls(links):\n",
    "    htmls = []\n",
    "    for link in tqdm(links):\n",
    "        url = \"https://www.th-nuernberg.de\" + link\n",
    "        html = download_html_from_url(url)\n",
    "        htmls.append(html)\n",
    "    df = pd.DataFrame({\"link\": links, \"html\": htmls})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = download_all_urls(df[\"link\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können die Daten an dieser Stelle abspeichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df_new, \"html_iter_01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weitere Iterationsstufen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir ab diesem Abschnitt starten können wir die vorher gesammelten Daten neu laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"html_iter_01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir die heruntergeladenen HTML files nach weiteren Links durchsuchen und Sie dem Dataframe hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_links_in_html(htmls):\n",
    "    all_links = []\n",
    "    for html in tqdm(htmls):\n",
    "        links = get_links_from_one_html(html)\n",
    "        [all_links.append(link) for link in links]\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = find_all_links_in_html(df[\"html\"])\n",
    "len(all_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben jetzt also 71331 Links gesammelt, davon sind aber viele Dublikate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame({\"link\": all_links, \"html\": None})\n",
    "df = pd.concat([df, df_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sort_and_remove_dublicates(df)\n",
    "len(df[\"link\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gefiltert nach dublikaten haben wir nun also noch 4043 Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteratives Downloaden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den Daten nun weitere Webseiten hinzuzufügen, können wir für jede weitere URL schauen, ob sie schon heruntergeladen wurde. Wenn nicht, dann laden wir sie jetzt herunter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_df_with_html(df):\n",
    "    new_rows = []\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        if pd.isna(row['html']) or row['html'] == '':\n",
    "            # Call the download function with the link from the 'link' column\n",
    "            url = \"https://www.th-nuernberg.de\" + row[\"link\"]\n",
    "            html = download_html_from_url(url)\n",
    "            new_rows.append({'link': row[\"link\"], 'html': html})\n",
    "\n",
    "    if new_rows:\n",
    "        df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = update_df_with_html(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_ultimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df_ultimate, \"html_ultimate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texte extrahieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes müssen wir aus den rohen html dokumenten die unrelevanten Daten aussortieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"html_ultimate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die nachfolgende Funktion bestimmt, ob ein Beautifulsoup geparsetes html Element sichtbar ist oder nicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt bestimmen wir eine Funktion, die ein HTML als Input bekommt und daraus nur die sichtbaren Zeichen bestimmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(file):\n",
    "    soup = BeautifulSoup(file,\"lxml\")\n",
    "\n",
    "    title = soup.find(\"title\")\n",
    "    if title:\n",
    "        title = title.text\n",
    "    else:\n",
    "        title = \"\"\n",
    "\n",
    "    main = soup.find(\"main\")\n",
    "    portal = soup.find(\"div\", {'class': 'portal'})\n",
    "    page_container = soup.find(\"div\", {'class': 'page-wrap'})\n",
    "    \n",
    "    visible_texts = \"\"\n",
    "    if main:\n",
    "        # print(\"main found\")\n",
    "        container = main.find(\"div\" ,{'class': 'container'}, recursive=False)\n",
    "        if container:\n",
    "            texts = container.find_all(text=True)\n",
    "            visible_texts = filter(tag_visible, texts)\n",
    "        # else:\n",
    "        #     print(\"no container\") \n",
    "\n",
    "    elif portal:\n",
    "        # print(f\"portal found {filename}\")\n",
    "        texts = portal.find_all(text=True)\n",
    "        visible_texts = filter(tag_visible, texts)\n",
    "        # print(u\" \".join(t.strip() for t in visible_texts))\n",
    "\n",
    "    elif page_container:\n",
    "        # print(f\"page_container found {filename}\")\n",
    "        container = page_container.find(\"div\" ,{'class': 'container'}, recursive=False)\n",
    "        # container = page_container.children[2]\n",
    "        if container:\n",
    "            texts = container.find_all(text=True)\n",
    "            visible_texts = filter(tag_visible, texts)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"title\":    title,\n",
    "        \"text\":     u\" \".join(t.strip() for t in visible_texts)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_texts = []\n",
    "titles = []\n",
    "for html in tqdm(df[\"html\"]):\n",
    "    content = get_content(html)\n",
    "    parsed_texts.append(content[\"text\"])\n",
    "    titles.append(content[\"title\"])\n",
    "\n",
    "df[\"text\"] = parsed_texts\n",
    "df[\"title\"] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df[df[\"text\"] != \"\"][\"url\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
