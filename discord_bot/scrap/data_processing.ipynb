{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Processing</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ziel dieses Notebooks ist, die gescrapeten Daten so zu verarbeiten, dass Sie später thematisch durchsucht werden können. Ein Embeddingvektor liefert genau diese Funktion. Auf eine Anfrage hin wird der Abstand zwischen dem Embedding der Frage und den Embeddings aller anderen Dokumenten berechnet. Dann werden die Dokumente mit den kleinsten Abständen ausgewählt und dem LLM als Kontext mitgegeben. Mithilfe des Wissens dieser Dokumente soll dass LLM dann in der Lage sein die Frage korrekt zu beantworten.\n",
    "Für die Embeddings benutzen wir [Google Bert](https://blog.google/products/search/search-language-understanding-bert/)\n",
    "\n",
    "Beispiel:\n",
    "\n",
    "User: Welche Dozenten unterrichten das Fach Grundlagen der Informatik?\n",
    "\n",
    "System wählt besten 5 Dokumente aus \n",
    "\n",
    "    <Dokument 1>: ... betreute Prof. Dr. Löhr eine Batchelorarbeit in Grundlagen der Informatik...\n",
    "    <Dokument 2>: Prof. Dr. Weber tel.: 013882664 email: weber@th.de Raum: HQ: 403, Fächer: Grundlagen der Informatik ...\n",
    "    <Dokument 3> ...\n",
    "    <Dokument 4> ...\n",
    "    <Dokument 5> ...\n",
    "    \n",
    "\n",
    "Aus der Nutzeranfrage und den Dokumenten wird eine neue Query erstellt, die dem LLM dann final bereitgetellt wird. Diese sieht in etwa so aus:\n",
    "\n",
    "    \n",
    "    <Dokument 1>: ... betreute Prof. Dr. Löhr eine Batchelorarbeit in Grundlagen der     Informatik...\n",
    "    <Dokument 2>: Prof. Dr. Weber tel.: 013882664 email: weber@th.de Raum: HQ: 403, Fächer: Grundlagen der Informatik ...\n",
    "    <Dokument 3> ...\n",
    "    <Dokument 4> ...\n",
    "    <Dokument 5> ...\n",
    "\n",
    "    Bitte beantworte folgende Frage unter der Berücksichtigung obiger Dokumente:\n",
    "    Welche Dozenten unterrichten das Fach Grundlagen der Informatik?\n",
    "\n",
    "\n",
    "Das LLM wird daraufhin hoffentlich korrekt eine Antwort liefern die ähnlich ist zu:\n",
    "\n",
    "    A: An der TH Nürnberg Georg Simon Ohm unterichten die Professoren Prof. Dr. Löhr und Prof. Dr. Weber das Fach Grundlagen der Informatik.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lizab\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lizab\\projects\\daibl-2\\discord_bot\\scrap\\data_processing.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lizab/projects/daibl-2/discord_bot/scrap/data_processing.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msqlite3\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lizab/projects/daibl-2/discord_bot/scrap/data_processing.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lizab/projects/daibl-2/discord_bot/scrap/data_processing.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertModel, BertTokenizer\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lizab/projects/daibl-2/discord_bot/scrap/data_processing.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lizab/projects/daibl-2/discord_bot/scrap/data_processing.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1231\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\lizab\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1117\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1115\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m   1116\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1117\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(module, name)\n\u001b[0;32m   1118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lizab\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1116\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1114\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[0;32m   1115\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m-> 1116\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[0;32m   1117\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1118\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lizab\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1126\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_module\u001b[39m(\u001b[39mself\u001b[39m, module_name: \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1125\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1126\u001b[0m         \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m   1127\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1128\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1129\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1130\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1131\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lizab\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32mc:\\Users\\lizab\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdataclasses\u001b[39;00m \u001b[39mimport\u001b[39;00m dataclass\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List, Optional, Tuple, Union\n\u001b[1;32m---> 25\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcheckpoint\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n",
      "File \u001b[1;32mc:\\Users\\lizab\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:1315\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdlpack\u001b[39;00m \u001b[39mimport\u001b[39;00m from_dlpack, to_dlpack\n\u001b[0;32m   1312\u001b[0m \u001b[39m# Import experimental masked operations support. See\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[39m# [RFC-0016](https://github.com/pytorch/rfcs/pull/27) for more\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m \u001b[39m# information.\u001b[39;00m\n\u001b[1;32m-> 1315\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m masked\n\u001b[0;32m   1317\u001b[0m \u001b[39m# Import removed ops with error message about removal\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_linalg_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1319\u001b[0m     matrix_rank,\n\u001b[0;32m   1320\u001b[0m     eig,\n\u001b[0;32m   1321\u001b[0m     solve,\n\u001b[0;32m   1322\u001b[0m     lstsq,\n\u001b[0;32m   1323\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lizab\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\masked\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmaskedtensor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m is_masked_tensor, MaskedTensor\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmaskedtensor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcreation\u001b[39;00m \u001b[39mimport\u001b[39;00m as_masked_tensor, masked_tensor\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     _canonical_dim,\n\u001b[0;32m      5\u001b[0m     _generate_docstring,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     normalize,\n\u001b[0;32m     30\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1140\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1080\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1504\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1476\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1616\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1659\u001b[0m, in \u001b[0;36m_fill_cache\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cosine\n",
    "from db_init import db_get_df, db_save_df\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst laden wir die Daten aus der Datenbank. Dabei besitzt jedes Dokument als Metadaten den Titel der Webseite, den filenamen und den Text. Diese speichern wir uns in einen Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"html_attrs\", [\"filename\", \"title\", \"text\"])\n",
    "\n",
    "print(df.dtypes)\n",
    "print(df[\"text\"][3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur überprüfung der Texte können wir nun einmal eine Keywordsuche starten. Dieser Ansatz wird außerdem tiefer im Notebook [spacy_keywordextraction](./spacy_keywordextraction.ipynb) verfolgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Gallwitz\"\n",
    "\n",
    "[text for text in df[\"text\"] if word in text][:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt werden wir für jedes Dokument ein eigenes Word embeddings erstellen. Dazu müssen wir zunächst das BERT Model laden.\n",
    "Das BERT Model ist ein von Google trainiertes mehrschichtiges neuronales Netz, welches ursprünglich dafür entwickelt wurde, dass ???\n",
    "es ist trainiert auf 10.000+ Büchern\n",
    "es gibt Modelle \"base\" und \"large\"\n",
    "uncased heißt ohne klein - Großschreibung\n",
    "\n",
    "Wir brauchen zur vorbereitung die zusätzlichen Token\n",
    "[SEP] um das Ende eines Satzes zu markieren\n",
    "[CLS] am Anfang des Texten\n",
    "[PAD] zum auffüllen der Token \n",
    "Außerdem\n",
    "TokenIDs\n",
    "MaskIDs - zum filtern der [PAD]\n",
    "Segment IDs um verschiedene Sätze zu unterscheiden\n",
    "Posititional Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased') #TODO try better model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "testSentence = \"In der Bibliothek gibt es 40 Bücher zu Thema Animes\"\n",
    "tokens_question = tokenizer.tokenize(testSentence)\n",
    "tokens_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] = [tokenizer.tokenize(text) for text in df[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "print(df[\"tokens\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_question = ['[CLS]'] + tokens_question + ['[SEP]']\n",
    "attention_mask = [1 if token != \"[PAD]\" else 0  for token in tokens_question]\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens_question)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "\n",
    "attention_mask = torch.tensor(attention_mask).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(token_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proccessSentence(tokens):\n",
    "    if len(tokens) == 0:\n",
    "        # Handle the case when the token list is empty, for example, return a default embedding or raise an exception.\n",
    "        # For demonstration purposes, we'll return a zero tensor as the default embedding.\n",
    "        return torch.zeros(768)\n",
    "\n",
    "    # Ensure the token sequence length is no longer than the maximum sequence length the model can handle (512)\n",
    "    if len(tokens) > 512:\n",
    "        tokens = tokens[:512]\n",
    "\n",
    "    # Padding the token sequence to the maximum sequence length if it's shorter\n",
    "    if len(tokens) < 512:\n",
    "        tokens += ['[PAD]'] * (512 - len(tokens))\n",
    "\n",
    "    attention_mask = [1 if token != \"[PAD]\" else 0  for token in tokens]\n",
    "    tokenDocument_idss = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    tokensDocument_tensor = torch.tensor([tokenDocument_idss], dtype=torch.int64)\n",
    "    segmentsDocument_tensors = torch.tensor([attention_mask], dtype=torch.int64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokensDocument_tensor, segmentsDocument_tensors)\n",
    "        hiddenDocuments_states = outputs[2]\n",
    "\n",
    "    tokenDocuments_vecs = hiddenDocuments_states[-2][0]\n",
    "    sentenceDocument_embedding = torch.mean(tokenDocuments_vecs, dim=0)\n",
    "    #print(\"Our final sentence embedding vector of shape:\", sentenceDocument_embedding)\n",
    "\n",
    "    return sentenceDocument_embedding\n",
    "\n",
    "df[\"word_embeddings\"] = [proccessSentence(tokens).tolist() for tokens in tqdm(df[\"tokens\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE \n",
    "import numpy as np  \n",
    "\n",
    "word_embeddings_copy = df['word_embeddings'].copy()\n",
    "\n",
    "# Verwenden Sie 'ast.literal_eval', um Zeichenketten in Listen umzuwandeln\n",
    "word_embeddings_copy = word_embeddings_copy.apply(ast.literal_eval)\n",
    "\n",
    "# Wandeln Sie die Listen von Listen in ein Numpy-Array um\n",
    "word_embeddings = np.array(word_embeddings_copy.tolist())\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "X_embedded = tsne.fit_transform(word_embeddings)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_embedded[:, 0], X_embedded[:, 1], s=5)\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database = 'discord_bot/scrap/html.sqlite'\n",
    "\n",
    "# with sqlite3.connect(database) as con:\n",
    "#     html_df.to_sql('html_with_embeddings', con, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 1\n",
      "Number of tokens: 6\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "#dokument--> bert anwenden für jeden dokument\n",
    "# question-bert anwenden\n",
    "question=\"was macht Gallwitz?\"\n",
    "document=df[\"text\"][1]\n",
    "\n",
    "tokens_question = tokenizer.tokenize(question)\n",
    "tokens_document = tokenizer.tokenize(document)\n",
    "attetion_mask_question = [1] * len(tokens_question)\n",
    "attention_mask_dokument = [1] * len(tokens_document)\n",
    "\n",
    "token_idss = tokenizer.convert_tokens_to_ids(tokens_question)\n",
    "tokenDocument_idss = tokenizer.convert_tokens_to_ids(tokens_document)\n",
    "\n",
    "\n",
    "tokens_tensor = torch.tensor([token_idss])\n",
    "segments_tensors = torch.tensor([attetion_mask_question])\n",
    "\n",
    "tokensDocument_tensor = torch.tensor([tokenDocument_idss])\n",
    "segmentsDocument_tensors = torch.tensor([attention_mask_dokument])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokensDocument_tensor, segmentsDocument_tensors)\n",
    "    hiddenDocuments_states = outputs[2]\n",
    "\n",
    "# print(token_idss)\n",
    "# print(tokenDocument_idss)\n",
    "\n",
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_vecs_sum = []\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding)\n",
    "\n",
    "\n",
    "tokenDocuments_vecs = hiddenDocuments_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentenceDocument_embedding = torch.mean(tokenDocuments_vecs, dim=0)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(sentence_embedding, sentenceDocument_embedding)\n",
    "\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'html.sqlite'\n",
    "sql = \"\"\"\n",
    "SELECT filename, title, text, word_embeddings FROM word_embeddings\n",
    "\"\"\"\n",
    "\n",
    "con = sqlite3.connect(database)\n",
    "df = pd.read_sql_query(sql, con)\n",
    "con.close()\n",
    "\n",
    "print(df.dtypes)\n",
    "print(df[\"text\"][3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(len(df[\"text\"][4]))\n",
    "# print(json.loads(df[\"text\"][0]))\n",
    "diff_bank = 1 - cosine(json.loads(df[\"word_embeddings\"][3]), json.loads(df[\"word_embeddings\"][6]))\n",
    "# for embed in df[\"word_embeddings\"]:\n",
    "# print(embed)\n",
    "\n",
    "print(diff_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"][1746]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Fachhochschulgesetz\"\n",
    "\n",
    "df.loc[df[\"text\"].str.contains(word)][\"text\"]\n",
    "# [text for text in df[\"text\"] if word in text][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/br/Projects/IT-Ptojekt-Chatbot/daibl/discord_bot')\n",
    "from question_embedding import question_embeddings\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# TODO 10 Fragen\n",
    "# TODO TSNE\n",
    "\n",
    "df = db_get_df(\"chunk_word_embeddings_all\")\n",
    "print(df.dtypes)\n",
    "question = \"TH\"\n",
    "question_embedding = question_embeddings(question)\n",
    "\n",
    "df[\"distance\"] = [1 - cosine(json.loads(embedding), question_embedding) for embedding in df[\"chunk_word_embeddings\"]]\n",
    "most_similar_documents = df.nsmallest(5, \"distance\")\n",
    "print(f\"question embedding: {question_embedding[:10]}\")\n",
    "print(most_similar_documents[\"distance\"])\n",
    "\n",
    "\n",
    "df[\"distance\"].plot(kind='hist', bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"word_embeddings\", [\"filename\", \"title\", \"text\", \"tokens\"])\n",
    "df[\"token_ids\"] = [tokenizer.convert_tokens_to_ids(json.loads(tokens)) for tokens in df[\"tokens\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splice dokuments in 512 token chunks\n",
    "\n",
    "# Initialize an empty list to store rows for the new DataFrame\n",
    "new_rows = []\n",
    "\n",
    "# Function to split text and tokens into chunks of 512 tokens\n",
    "def split_text_and_tokens(row):\n",
    "    text = row['text']\n",
    "    tokens_ids = row['token_ids']\n",
    "    filename = row['filename']\n",
    "\n",
    "    if len(tokens_ids) > 512:\n",
    "        # Split into multiple chunks\n",
    "        for i in range(0, len(tokens_ids), 512):\n",
    "            chunk_tokens = tokens_ids[i:i + 512]\n",
    "            chunk_text = tokenizer.decode(chunk_tokens)\n",
    "\n",
    "            # Create a new row with a reference to the original row\n",
    "            new_row = {'filename': filename, 'chunk_id': i/512, 'chunk_text': chunk_text, 'chunk_tokens_json': json.dumps(chunk_tokens)}\n",
    "            new_rows.append(new_row)\n",
    "    else:\n",
    "        # If the row has 512 tokens or fewer, keep it as is\n",
    "        new_row = {'filename': filename, 'chunk_id': 0, 'chunk_text': text, 'chunk_tokens_json': json.dumps(tokens_ids) }\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "# Apply the function to each row in the original DataFrame\n",
    "df.apply(split_text_and_tokens, axis=1)\n",
    "\n",
    "# Create a new DataFrame from the list of new rows\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Reset the index of the new DataFrame if needed\n",
    "new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(new_df.to_markdown())\n",
    "\n",
    "# tokenizer.convert_tokens_to_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus den 2433 Dokumenten die wir eigentlich gescraped haben, sind nun 6945 chunks entstanden es hat sich fast verdreifacht. Wenn man die 787 Seiten ohne Inhalt abzieht, hat sich die Anzahl von 1646 auf 6158 fast vervierfacht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"chunk_tokens_json\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(new_df, \"chunk_word_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "database_path = os.getenv(\"DATABASE_PATH\")\n",
    "\n",
    "def merge_db_tables():\n",
    "    # Connect to your SQLite database\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Create the new table using the structure of the first table (chunk_word_embeddings_0)\n",
    "    cursor.execute('''CREATE TABLE chunk_word_embeddings_all AS SELECT * FROM chunk_word_embeddings_0 WHERE 0''')\n",
    "    # Insert data from the other tables into the new table\n",
    "    for i in range(1, 8):\n",
    "        cursor.execute(f'INSERT INTO chunk_word_embeddings_all SELECT * FROM chunk_word_embeddings_{i}')\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "merge_db_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
