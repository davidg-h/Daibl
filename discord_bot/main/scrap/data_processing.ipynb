{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Processing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einleitung und Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Einleitung"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ziel dieses Notebooks ist, die gescrapeten Daten so zu verarbeiten, dass Sie später thematisch durchsucht werden können. Ein Embeddingvektor liefert genau diese Funktion. Auf eine Anfrage hin wird der Abstand zwischen dem Embedding der Frage und den Embeddings aller anderen Dokumenten berechnet. Dann werden die Dokumente mit den kleinsten Abständen ausgewählt und dem LLM als Kontext mitgegeben. Mithilfe des Wissens dieser Dokumente soll dass LLM dann in der Lage sein die Frage korrekt zu beantworten.\n",
    "Für die Embeddings benutzen wir [Google Bert](https://blog.google/products/search/search-language-understanding-bert/)\n",
    "\n",
    "Beispiel:\n",
    "\n",
    "User: Welche Dozenten unterrichten das Fach Grundlagen der Informatik?\n",
    "\n",
    "System wählt besten 5 Dokumente aus \n",
    "\n",
    "> <Dokument 1>: ... betreute Prof. Dr. Löhr eine Batchelorarbeit in Grundlagen der Informatik... <br>\n",
    "> <Dokument 2>: Prof. Dr. Weber tel.: 013882664 email: weber@th.de Raum: HQ: 403, Fächer: Grundlagen der Informatik ... <br>\n",
    "> <Dokument 3> ... <br>\n",
    "> <Dokument 4> ... <br>\n",
    "> <Dokument 5> ... <br>\n",
    "    \n",
    "\n",
    "Aus der Nutzeranfrage und den Dokumenten wird eine neue Query erstellt, die dem LLM dann final bereitgetellt wird. Diese sieht in etwa so aus:\n",
    "\n",
    "    \n",
    "> <Dokument 1>: ... betreute Prof. Dr. Löhr eine Batchelorarbeit in Grundlagen der Informatik... <br>\n",
    "> <Dokument 2>: Prof. Dr. Weber tel.: 013882664 email: weber@th.de Raum: HQ: 403, Fächer: Grundlagen der Informatik ... <br>\n",
    "> <Dokument 3> ... <br>\n",
    "> <Dokument 4> ... <br>\n",
    "> <Dokument 5> ... <br>\n",
    "> Bitte beantworte folgende Frage unter der Berücksichtigung obiger Dokumente:\n",
    "> Welche Dozenten unterrichten das Fach Grundlagen der Informatik?\n",
    "\n",
    "\n",
    "Das LLM wird daraufhin hoffentlich korrekt eine Antwort liefern die ähnlich ist zu:\n",
    "\n",
    "> A: An der TH Nürnberg Georg Simon Ohm unterichten die Professoren Prof. Dr. Löhr und Prof. Dr. Weber das Fach Grundlagen der Informatik.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/br/Projects/IT-Ptojekt-Chatbot/daibl/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from db_init import db_get_df, db_save_df\n",
    "import json\n",
    "# import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import random\n",
    "from embedding_algorithms.question_embedding import question_embeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laden der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Laden aus der sqlite db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst laden wir die Daten aus der Datenbank. Dabei besitzt jedes Dokument als Metadaten den Titel der Webseite, den filenamen und den Text. Diese speichern wir uns in einen Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename    object\n",
      "title       object\n",
      "text        object\n",
      "dtype: object\n",
      "        Nuremberg Tech is a university with strong regional roots that understands its role in a globalized living, employment, and research community. Our aim is to offer our students degree programmes and a learning environment that provide access to applied research in today’s international context. Therefore, an international and intercultural orientation already forms a key component of life at the university; the development of this characteristic is anchored in our internationalisation strategy .  The following third-party funded projects support the implementation and continued development of the measures described in the strategy to advance internationality at Nuremberg Tech.                           Internationalisation squared (2022 – 2023)  The Internationalisation squared (INT 2 ) project implements and further develops Nuremberg Tech’s internationalisation strategy. Based on a comprehensive analysis of existing Erasmus university partnerships, three to five European partner universities will be selected with which institutionalized cooperation will be extended to the area of research and deepened in the area of education. INT 2 aims to expand the catalogue of courses conducted in English at our technical Faculties and increase the number international research projects, e.g., within the framework of Horizon Europe. The long-term goal of these activities is to establish a European university network for comprehensive cooperation in all university fields of action.         2 is funded by the German Academic Exchange Service (DAAD) in Module A of the UAS.International funding programme with funds from the Federal Ministry of Education and Research (BMBF) from 1 January 2022 to 31 December 2023. In this programme, the DAAD has been funding activities to advance internationality at universities of applied sciences since 2019. UAS.International has four funding modules; projects in Module A cover initiation and preparation purposes, which include the implementation and further development of internationalisation strategies as well as the establishment and expansion of networks.                               Project team         Project management             Uwe  Mummert     Prof. Dr.      telefon  +49 (0)911 5880 - 2760     email  uwe.mummert at th-nuernberg Punkt de     fax  +49 (0)911 5880 - 6720           Project coordinator             Jörg  Franke     Dr.-Ing.      telefon  +49 (0)911 5880 - 2875     email  joerg.franke at th-nuernberg Punkt de     fax  +49 (0)911 5880 - 6720           Courses in English - project development             Rebecca  Ehrig     M.A.      telefon  +49 (0)911 5880 - 2864     email  rebecca.ehrig at th-nuernberg Punkt de           European research projects - project development             Kerstin  Seidel     Dipl.-Volksw.      telefon  +49 (0)911 5880 - 4057     email  kerstin.seidel at th-nuernberg Punkt de     fax  +49 (0)911 5880 - 8090                  \n"
     ]
    }
   ],
   "source": [
    "df = db_get_df(\"html_attrs\", [\"filename\", \"title\", \"text\"])\n",
    "\n",
    "print(df.dtypes)\n",
    "print(df[\"text\"][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtern der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sortieren zunächst alle Dokumete aus, die keinen Text beinhalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ungefiltert sind es 1646 Dokumente\n",
      "gefiltert sind es 1646 Dokumente\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ungefiltert sind es {len(df)} Dokumente\")\n",
    "df = df[df[\"text\"].apply(len) != 0]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(f\"gefiltert sind es {len(df)} Dokumente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       data/htmlfiles/file_karriere-bei-uns>ansprechp...\n",
       "1       data/htmlfiles/file_fakultaeten>sw>forschung>l...\n",
       "2       data/htmlfiles/file_en>international>internati...\n",
       "3       data/htmlfiles/file_einrichtungen-gesamt>in-in...\n",
       "4       data/htmlfiles/file_einrichtungen-gesamt>fraun...\n",
       "                              ...                        \n",
       "1641    data/htmlfiles/file_fakultaeten>d>studium>modu...\n",
       "1642    data/htmlfiles/file_fakultaeten>sw>news-und-te...\n",
       "1643    data/htmlfiles/file_einrichtungen-gesamt>in-in...\n",
       "1644     data/htmlfiles/file_fakultaeten>bw>studium>.html\n",
       "1645    data/htmlfiles/file_einrichtungen-gesamt>in-in...\n",
       "Name: filename, Length: 1646, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"filename\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### nach Sprachen filtern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt filtern wir die Seiten noch in Englisch und deutsche Seiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en = df[df['filename'].str.startswith('data/htmlfiles/file_en')]\n",
    "len(df_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1481"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_de = df[df['filename'].str.startswith('data/htmlfiles/file_en') == False]\n",
    "len(df_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Beispiel Keywort suche"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur überprüfung der Texte können wir nun einmal eine Keywordsuche starten. Dieser Ansatz wird außerdem tiefer im Notebook [spacy_keywordextraction](./spacy_keywordextraction.ipynb) verfolgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['       Voraussetzungen / Zielgruppe      Welche Studiengänge gibt es an der Fakultät Informatik?         Welche Studiengänge gibt es an der Fakultät Informatik?  Wirtschaftsinformatik Medieninformatik Informatik abgeschlossenes Bachelorstudium in - Wirtschaftsinformatik - Information Systems and Management - Information Technology in Business Computing - Computer Science in Business Computing abgeschlossenes Bachelorstudium in - Medieninformatik - Informatik - Computer Science - Information Technology abgeschlossenes Bachelorstudium in - Informatik - Computer Science - Information Technology mindestens 210 ECTS-Punkte, Nachqualifikation möglich mindestens 210 ECTS-Punkte, Nachqualifikation möglich mindestens 210 ECTS-Punkte, Nachqualifikation möglich Sechssemestrige Bachelorstudiengänge entsprechen 180 ECTS-Punkten. Daher sind im Masterstudium zusätzliche 30 ECTS-Punkte nachzuholen. Fehlende ECTS-Punkte können durch Nachqualifikation ausgeglichen werden. Zugangsvoraussetzung für den Master ist ein guter Abschluss. Einzelne mit Erfolg abgelegte Fächer eines abgeschlossenen Diplomstudiengangs können auf das Masterstudium angerechnet werden, sofern sie den Umfang eines vergleichbaren Bachelor-Studiums überschreiten. Details dieser Zulassungs- und Anerkennungsregeln bestimmt die Prüfungskommission.           Kann ich mir Vorleistungen anerkennen lassen?         Kann ich mir Vorleistungen anerkennen lassen?  Einzelne mit Erfolg abgelegte Fächer eines abgeschlossenen Studiengangs können auf das Masterstudium angerechnet werden, sofern sie den Umfang eines vergleichbaren Bachelorstudiums überschreiten. Details dieser Anerkennungsregeln bestimmt die Prüfungskommission. Die Studienberatung berät Sie hierbei gerne!           Wie lange dauert das Studium?         Wie lange dauert das Studium?  Das Masterstudium dauert drei Semester.  Die beiden ersten Semester beinhalten die theoretische Ausbildung. Das letzte Semester dient der Anfertigung einer Masterarbeit (Master-Thesis).  Werden Ihnen einzelne Prüfungsleistungen anerkannt, verkürzt sich die Studiendauer entsprechend.               Organisation des Studiums      Wie ist das Studium aufgebaut?         Wie ist das Studium aufgebaut?  Das Studium dauert drei Semester, bei Anrechnung von Leistungen kann es kürzer ausfallen. Die ersten beiden Semester bieten Ihnen Lehrveranstaltungen entsprechend dem Studienplan ( Informatik , Medieninformatik und Wirtschaftsinformatik ). Das letzte Semester ist für die Anfertigung einer Masterarbeit vorgesehen.           Welche Lehrveranstaltungen werden angeboten?         Welche Lehrveranstaltungen werden angeboten?  Die Kurse werden überwiegend auf Deutsch abgehalten, vereinzelt auch auf Englisch. Die Teilnahme an den Lehrveranstaltungen wird erwartet.  Die zu den Masterstudiengängen vorgesehenen Kurse zeigt der jeweilige Studienplan: Studienplan Master Informatik Studienplan  Master Medieninformatik Studienplan Master Wirtschaftsinformatik Die aktuell stattfindenden Kurse zeigt das Verzeichnis der Fachwissenschaftlichen Wahlpflichtfächer: Master Informatik Master Medieninformatik Master Wirtschaftsinformatik           In welcher Sprache werden die Kurse angeboten?         In welcher Sprache werden die Kurse angeboten?  Die Studiensprache ist Deutsch. Einzelne Kurse werden in englischer   Sprache gehalten.           Wann kann ich mich anmelden?         Wann kann ich mich anmelden?  Der Anmeldezeitraum ist in der Regel 02.05. bis 31.05. für das Wintersemsester (Medieninformatik) 02.05. bis 15.06. für das Wintersemsester (Informatik und Wirtschaftsinformatik) 15.11. bis 15.12 . für das Sommersemester (Medieninformatik) 15.11. bis 15.01 . für das Sommersemester (Informatik und Wirtschaftsinformatik) Hierzu füllen Sie den Online-Zulassungsantrag aus und senden ihn mit den benötigten Unterlagen an das Studienbüro.  Dem Antrag sind folgende Unterlagen beizufügen: Hochschulzugangsberechtigung in amtlich beglaubigter Form Hochschulabschlusszeugnis und Hochschulzwischenzeugnis (z. B. Vordiplom) amtlich beglaubigt Tabellarische Aufschlüsselung aller abgelegten und noch abzulegenden Prüfungsfächer sortiert nach Fachsemester, mit Angabe der Anzahl der Semesterwochenstunden bzw. ECTS-Punkte und den bisher erreichten Prüfungsergebnissen. Falls Sie vorbestraft sind, müssen Sie ein polizeiliches Führungszeugnis beifügen.           Wo erhalte ich weitere Informationen?         Wo erhalte ich weitere Informationen?  Mit fachlichen Fragen können Sie sich gerne an die Studienberater der Fakultät wenden. Die zentrale Studienberatung berät Sie zu allen  Fragen der Qualifikation und Alternativen zu den  Informatikstudiengängen. Das Studienbüro schließlich ist ihre Anlaufstelle  für die Bewerbung um einen Studienplatz, und wird Sie während ihres  Studiums begleiten.  Eine regelmäßige Gelegenheit zur Information und für Fragen sind die   Studieninformationstage der Hochschule, die jährlich Ende September   stattfinden.  Einen Lageplan mit Hinweisen zur Anfahrt finden Sie hier .           Ist auch ein Fernstudium möglich?         Ist auch ein Fernstudium möglich?  Nein. Zwar besteht bei den meisten Veranstaltungen keine Präsenzpflicht, eine regelmäßige Teilnahme wird jedoch erwartet.           Kann ich \\'nebenbei\\' studieren?         Kann ich \\'nebenbei\\' studieren?  Kaum, denn die Studiengänge finden zu normalen Tageszeiten statt. Es gibt keine besonderen Abend- oder Wochenendveranstaltungen und keine Begleitung mit Fernsehkursen. Auch ist kein Fernstudium mit schriftlichen Aufgaben und Korrekturen geplant.           Ist dieser Master ein Weiterbildungsstudium?         Ist dieser Master ein Weiterbildungsstudium?  Nein, dieser Masterstudiengang ist ein sogenannter konsekutiver Studiengang nach einem Bachelorstudium. Das bedeutet insbesondere, dass erhebliche einschlägige Kenntnisse in Informatik bzw. Medieninformatik oder Wirtschaftsinformatik vorausgesetzt werden und zur Zulassung nötig sind.           Kann ich auch im Sommersemester beginnen?         Kann ich auch im Sommersemester beginnen?  Ja, das können Sie. Jedoch werden nicht alle Lehrveranstaltungen in jedem Semester angeboten.           An wen kann ich mich bei Fragen wenden?         An wen kann ich mich bei Fragen wenden?  Masterabschluss Wirtschaftsinformatik Masterabschluss Medieninformatik Masterabschluss Informatik Prof. Dr. Patricia Brockmann Prof. Dr. Florian Gallwitz Prof. Dr. Jörg Roth Raum: HQ.304 Raum: HQ.518 Raum: H.Q518 Telefon: 0911-5880-1170 Telefon: 0911-5880-1677 Telefon: 0911-5880-1169 patricia.brockmann at th-nuernberg Punkt de florian.gallwitz at th-nuernberg Punkt de joerg.roth at th-nuernberg Punkt de                Abschluss / Titel      Was bringt der Mastertitel?         Was bringt der Mastertitel?  Wir sehen den Masterabschluss als eine höherwertige Qualifikation in dem Sinne, dass Sie weitere anspruchsvolle Lehrveranstaltungen sowie eine Masterarbeit absolvieren. Also eine Art \"Gütesiegel\".  In diesem Zusammenhang wird oft nach der Promotionsberechtigung gefragt. Die Erfahrung zeigt, dass eine Promotion mit FH-Diplom prinzipiell möglich ist. Dazu gibt es Zusatzauflagen, die von Universität zu Universität unterschiedlich sein können. Wir gehen davon aus, dass die Lage für Masterabsolventen wegen ihrer Höherqualifikation einfacher ist, wenngleich wir nicht erwarten, dass dies in kürzester Zeit bei den Universitäten in entsprechende Regelungen formal umgesetzt wird. Also: Ohne Garantie!           Worin unterscheidet sich dieser Masterstudiengang von anderen?         Worin unterscheidet sich dieser Masterstudiengang von anderen?  Der wesentliche Unterschied liegt daran, dass er ein konsekutiver Masterstudiengang und kein Weiterbildungs-Master ist. Der Master als Weiterbildungsstudiengang wird häufiger angeboten, ein konsekutiver Master dagegen nur sehr selten.  Beim Weiterbildungs-Master wird eine Unterbrechung, d.h. Berufstätigkeit, zwischen Erststudium und Master gefordert (in Bayern eine 2-jährige Praxis nach dem ersten Abschluss). Beim konsekutiven Master dagegen ist das Studium direkt nach dem Bachelor bzw. Diplom möglich. Eine Berufstätigkeit ist jedoch nicht schädlich.  Darüber hinaus ist bei Weiterbildungs-Mastern die Zusammensetzung der Studierenden unterschiedlicher, sowohl in den Vorkenntnissen (z.B. unterschiedliche Studiengänge) als auch im zeitlichen Abstand zum letzten Studienabschluss. Die Beobachtung zeigt, dass bei einem Weiterbildungs-Master zumindest einige Zeit für die Schaffung einer einheitlichen Basis unter den Studierenden verwendet werden muss. Im konsekutiven Masterstudiengang können wir dagegen auf einer homogeneren Grundlage aufsetzen. Wir erwarten uns davon eine inhaltlich tiefere und umfassendere Ausbildung.           Welche Mastertitel werden als Abschluss verliehen?         Welche Mastertitel werden als Abschluss verliehen?  Bei Abschluss des Studiums wird den Absolventen der akademische Titel Master of Science (M.Sc.) verliehen.  Die Abschlüsse Bachelor und Master sind insbesondere im englischen Sprachraum üblich. Der Bachelor ist unter dem Diplom anzusetzen, der Master darüber.           Ist der Studiengang akkreditiert?         Ist der Studiengang akkreditiert?  Die Studiengänge sind akkreditiert.  Die Akkreditierung für Wirtschaftsinformatik Informatik Medieninformatik (erfolgt SoSe 2017) wurde von dem Akkreditierungs-, Zertifizierungs- und  Qualitätssicherungs-Instituts ACQUIN erteilt.               Kosten und Studienförderung      Wird das Bachelorstudium mit BAföG gefördert?         Wird das Bachelorstudium mit BAföG gefördert?  Dazu erhalten Sie Auskunft vom Studentenwerk .           Werden Stipendien vergeben?         Werden Stipendien vergeben?  Hier eine kurze Übersicht der Stipendien .               Campus und Umfeld der  Technischen Hochschule Nürnberg      Wie komme ich am einfachsten zur Fakultät Informatik?         Wie komme ich am einfachsten zur Fakultät Informatik?  Mit öffentlichen Verkehrsmitteln ist das Fakultätsgebäude  in der Hohfederstr. 40 mit der Straßenbahnlinie 8 ( VGN )\\xa0vom Hauptbahnhof in 7  Minuten erreichbar. Mit dem Auto ist die Fakultät Informatik ganz  gut erreichbar, jedoch ist die Parkplatzsituation nicht einfach. Viele  Studenten und Mitarbeiter kommen mit dem Fahrrad.  (siehe unter Kontakt - Standort Hohfederstr.)           Wie studiert sich\\'s in Nürnberg?         Wie studiert sich\\'s in Nürnberg?  [Von der Fachschaft Informatik verfasst:]  \"Das Studentenleben in Nürnberg ist wegen vieler unterschiedlicher   Vorzüge besonders erwähnenswert. Dank seiner Vielfalt hält Nürnberg für   jeden Geschmack ein Stück Lebensqualität bereit. Es ist eine der   grünsten Großstädte Deutschlands, vom Preisniveau entschieden niedriger   als vergleichbare Metropolen und nicht zuletzt eine der sichersten   Städte Europas.  Ein anderes Plus ist der zentrale Hochschulcampus, wo man jedes   Gebäude innerhalb von wenigen Minuten Fußweg erreichen kann. Die Lage   des Campus ist ideal, 5 min von der Altstadt, 1 min vom Stadtpark   inklusive Biergarten, der in den Sommermonaten zum Sonnetanken einlädt,   und 3 min von den U- und S- Bahnstationen entfernt.  Rundum befinden sich die Studentenwohnheime, die mit dem Fahrrad nur   selten mehr als 10 Minuten entfernt sind. Neben sämtlichen   Sporteinrichtungen beherbergt Nürnberg das größte Kino Deutschlands und   bietet auch abends mit seiner interessanten und vielfältigen   Kneipenlandschaft keinen Grund zur Langeweile. Partyleute können in   den Szene-Clubs oder auf den wöchentlichen Studentenpartys schnell  neue  Leute kennen lernen, was bei über 20.000 Studenten in Nürnberg  auch  nicht schwer ist.  Der Hightech-Standort Nürnberg ist nicht nur in Bayern bekannt. Die   wirtschaftliche Infrastruktur der Region kann für angehende Informatiker   sehr interessant sein. Firmen wie Siemens, MAN, Adidas, Puma usw.  haben  hier ihre Wurzeln und sind zu Kooperationen mit Studenten gerne  bereit.  Auch kulturell bietet die Stadt eine Menge. Oper, Schauspielhaus und   Theater bieten attraktivem Angebote. Kunstliebhaber sollten auf jeden   Fall einigen der zahlreichen Museen und Ausstellungen einen Besuch   abstatten, wie z.B. dem Germanischen Nationalmuseum, Spielzeugmuseum   oder dem Albrecht-Dürer-Haus.  Die Stadt ist verkehrstechnisch durch ICE-Verbindungen, Flughafen und   durch die Autobahnen (A9, A73 und A3) sehr gut erschlossen.  Nürnberg ist auf jeden Fall einen Besuch wert, bei dem bis jetzt   jeder schnell überzeugt wurde, dass Nürnberg eine lebenswerte Stadt   ist.\"           Wie sieht der Wohnungsmarkt in Nürnberg aus?         Wie sieht der Wohnungsmarkt in Nürnberg aus?  Sogar nahe der Hochschule sind kleine und günstige Wohnungen zu    finden. Da die Hochschule zu Fuß nicht weit  vom Stadtkern entfernt ist,   bietet  sich damit eine meist doppelt gute Lage.  Auch das Studentenwerk bietet günstige Wohnungen und Zimmer  in der Nähe an. Einige der  Wohnheime sind für ihre legendären Parties  bekannt.              ',\n",
       " '          Der Masterstudiengang Medieninformatik ist ein postgradualer Studiengang und baut inhaltlich auf dem Bachelorstudiengang Medieninformatik auf.  Er ist Technologie-orientiert und deckt zentrale Gebiete der Informationstechnik unter besonderer Berücksichtigung verteilter und vernetzter Systeme ab. Er qualifiziert die Studierenden für das Tätigkeitsfeld der angewandten und praktischen Medieninformatik und legt dabei besonderen Wert auf die Verbreiterung der theoretisch-wissenschaftlichen Grundlagen.                 Überblick  Inhalt & Ablauf  Beruf & Karriere  Zulassung  Beratung       Abschluss  Master of Science  Regelstudienzeit  3 Semester  Zulassungsbeschränkung  studiengangsspezifisch   Studienbeginn  Wintersemester\\n                oder\\n                Sommersemester   Bewerbungszeitraum  Für das Wintersemester:\\n                    02.05.2023 bis 15.06.2023 Für das Sommersemester:\\n                    15.11.2023 bis 15.01.2024   Hier geht’s zur Bewerbung    Zuständige Fakultät  Informatik          Module  Die folgenden Modulbeschreibungen geben Ihnen einen Eindruck von den   konkreten Studieninhalten. Die für jedes Semester aktuelle und   vollständige Modulbeschreibung samt Detailinformationen finden Sie im Modulhandbuch.        Modulgruppe 2: Interaktive Medien      Automatische Spracherkennung         Lernziele  Kenntnisse der Grundlagen der automatischen Spracherkennung, Fähigkeit  zur Implementierung von einfachen Klassifikationsalgorithmen, Fähigkeit  zur Durchführung einfacher Klassifikationsexperimente, Verständnis für  die aktuellen Entwicklungen im Bereich \"Deep Learning\"  Inhalt  Dieser Kurs ist eine Einführung in die Automatische Spracherkennung und  besteht aus einem Vorlesungsteil mit praktischen Übungen im Labor.  Theorie: Eigenschaften gesprochener Sprache Merkmalgewinnung Statistische Mustererkennung Neuronale Netze und \"Deep Learning\" Wortmodellierung Grammatische Sprachmodellierung Dekodierung kontinuierlicher Sprache Praktische Übungen in Projektgruppen von 2 bis 4 Studenten, z.B. Einzelworterkennung mit Dynamischer Zeitverzerrung Textbasierte Sprachen-Erkennung mit Bigramm-Sprachmodellen Sprecher-Erkennung           Advanced HCI         Lernziele  Die Studierenden können sich eigenständig in ein aktuelles Thema der HCI  (Human Computer Interaction) einarbeiten und den aktuellen Stand der  Technik hierzu identifizieren. Ausgehend hiervon ist es Ihnen möglich,  bekannte Ansätze zu klassifizieren und zu diskutieren. Durch das  Strukturieren der Publikationen und Formulieren eines Überblicksartikels  werden die Kenntnisse im wissenschaftlichen Schreiben vertieft.  Inhalt  Themen werden unter anderem sein: Touch-Interaktion Gestenbasierte Interaktion Natürlichsprachliche Interaktion Haptische Interaktion Holographische Displays Low-Cost 3D-Drucker (Übersicht, aktueller Stand und Trends) Gehirn-Computer-Schnittstellen (Brain-Computer-Interfaces) Anwendungsfall Personenerkennung (Aktueller Stand und Trends) Anwendungsfall Navigationslösungen (Aktueller Stand und Trends)           Digitale Bildbearbeitung         Lernziele  Erwerb der folgenden Kenntnisse und Fähigkeiten: Kenntnis der für Fotografie relevanten Dateiformate mit ihren jeweiligen Einschränkungen Verständnis der Inhalte von Bildern im raw-Format Entwicklungsprozess  von raw-Bildern am Beispiel des Programms  Adobe Lightroom (Histogramm,  Farbtemperatur, Tönen, Schärfen,  perspektivische Korrektur,  Beseitigung von chromatischen Aberrationen,  (selektive)  Rauschentfernung Erstellung von HDR Bildern und die Bedeutung des Tonemappings Erstellen von Panoramen Focus Stacking, Rauschreduktion durch Tracking Intelligentes Masking und selektive Bearbeitung Partielle Erweiterung des Dynamikumfangs Frequenztrennung und Retusche Color Balancing Compositing Inhalt  Seit Jahren dominieren digitale Fotoapparate den Markt. Während   früher in Dunkelkammern Bilder entwickelt und ihnen dabei „der letzte   Schliff“ gegeben wurde, haben nun sehr mächtige Softwaresysteme an   Bedeutung gewonnen. In dieser Veranstaltung sollen die dabei bestehenden   Möglichkeiten zur Entwicklung, Bearbeitung und Komposition von Bildern   erlernt werden.  In dieser Veranstaltung werden sich theoretische   Wissensvermittlungseinheiten mit praktischen Phasen abwechseln, in denen   die Teilnehmer die Techniken auf eigene oder vorgegebene  Beispielbilder  anwenden. Diese Bearbeitungsschritte werden in  Kurzvorträgen den  anderen Teilnehmern vorgestellt, so dass die  Teilnehmer aus allen  Projektarbeiten lernen.           3D-Echtzeitrendering - Alternative Verfahren         Lernziele  Kompetenz unterschiedliche und innovative Ansätze zur Modeliierung und  Darstellung virtueller 3D Modelle zu erläutern und zu bewerten.  Kenntnisse hinsichtlich der Konzeption der Kernkomponenten einer  „nicht-traditionellen“ 3D-Grafik-Engine. Fähigkeiten zur prototypischen  Umsetzung eines alternativen Echtzeitrendering-Verfahrens und Ansätze  zur Integration dessen in eine bereits bestehende Software-Lösung.  Inhalt Abgrenzung „traditionelles“ 3D-Echtzeitrendering Kernkomponenten einer typischen 3D-Grafik-Engine Übersicht „alternative“ Modellierungsverfahren Voxel-, punkt- und bildbasierte 3D-Modellierung Übersicht „alternative“ 3D-Darstellungstechniken Raytracing, Splatting und Image-based Rendering Hierarchische und sequentielle Beschleunigungsdatenstrukturen            Analyse und Visualisierung medizinischer Bilddaten         Lernziele  Die Studierenden können die grundlegenden Verfahren zur Verarbeitung,  Analyse und Visualisierung medizinischer Bilddaten sowie deren  praktischen Nutzen in der klinischen Anwendung beurteilen und erklären,  in welchem Kontext diese verwendet werden. Sie können die besonderen  Herausforderungen, die sich durch das interdiziplinäre Umfeld ergeben,  aufzeigen und erhalten einen Überblick über aktuelle wissenschaftliche  Problemstellungen. Dadurch werden sie befähight, auf Basis des  vermittelten Wissens eigene Lösungen für spezifische Problemstellungen  zu entwickeln.  Inhalt  Die Flut und Komplexität medizinischer Bilddaten sowie die klinischen  Anforderungen an Genauigkeit und Effizienz\\xa0erfordern leistungsfähige und  robuste Konzepte der Datenverarbeitung. Auf Grund der Vielfalt an  Bildinformation und\\xa0ihrer klinischen Relevanz spielt der Übergang von  der Bildgebung zur medizinischen Analyse und Interpretation  eine\\xa0wichtige Rolle. Ergänzt durch Verfahren der Bildanalyse bildet die  Visualisierung die grundlegende\\xa0Schnittstelle zwischen den Daten und dem  Benutzer.  In der Veranstaltung werden folgende Themen behandelt: Regelkreis zur Verarbeitung medizinischer Bilddaten Überblick über bildgebende Verfahren Grundlagen der Bildvorverarbeitung Grundlagen der Segmentierung Explizite und implizite Methoden der Registrierung Visualisierung (2D, 3D) von Skalar- und Vektordaten           Programmierung von Grafik-Shadern         Lernziele  Kenntnis von physik-basierten und anderen Beleuchtungsmodellen.    Fähigkeit, graphische Anwendungen und ihre Shader zu analysieren und zu  programmieren.   Kenntnis von und Fähigkeit zur GPGPU Programmierung  Inhalt  Wiederholung und Auffrischung der Grundlagen der Computergraphik und der  Graphik-Pipeline. Shader in der Pipeline, Implementierung des  Phong-Blinn-Modells, Physik-basiertes Shading, Effekte, GPGPU  Programming, Entwicklungsumgebungen und Debugging.                 Modulgruppe 3: Internet und Mobilität      Mobile Computing         Lernziele  Untersuchung einer exemplarischen Beispiel-Aufgabenstellung aus dem  Gebiet des Mobile Computings und die Bearbeitung in einer Kleingruppe  (Projekt). Dabei Erwerb der folgenden Kenntnisse und Fähigkeiten: Erstellen von Anwendungen für mobile Geräte Kennenlernen von typischen spezifischen Anforderungen bei der  Erstellung von mobilen Anwendungen (z.B. Vorteile und Problemstellungen  von Stift- oder Touchbedienung) Inhalt  Seit einiger Zeit gibt es leistungsfähige mobile Computer (Tablet PCs,  Pocket PCs, Ultra Mobile PCs), welche auch handschriftliche Eingaben, z.  B. mit einem Stift, erlauben. Besonders interessant ist hierbei, dass  die handschriftlichen Eingaben bei diesen Rechnersystemen\\xa0nicht als  Bitmap-Schriftzug, sondern als völlig neuer Datentyp \\'digitale Tinte\\'  mit sehr hoher Auflösung und vielfältigen Verwendungsmöglichkeiten zur  Verfügung gestellt werden. Hierdurch sind neue Anwendungen denkbar, die  weit über die Möglichkeiten eines normalen Stifts hinausgehen.  Ebenso werden in der Veranstaltung andere alternative  Kommunikationsformen mit dem Computer angeprochen, die alle im Gebiet  Mobile Computing Anwendung finden können. Dies können z. B.  Gestensteuerung (Touchless), Sprachsteuerung oder Surface Computing  sein.  In dieser Veranstaltung sollen solche Eingabe- und Steuerungskonzepte  sowie auch sonstige Verwendungsmöglichkeiten von mobilen Computern in  Form von Projekten genauer untersucht werden.  Das Ziel der Veranstaltung ist dabei auch, dass alle Teilnehmer aus  allen Projektarbeiten lernen. Daher sollen sich Projektarbeiten und  regelmäßige Berichterstattungen aus den einzelnen Gruppen abwechseln.           Ubiquitäre Informationsdienste         Lernziele  Fähigkeit, schriftliche Berichte so zu erstellen, dass sie den  grundlegenden Anforderungen akademischen Schreibens genügen. Kenntnis  grundlegender Konzepte kontextualisierter und situationsbezogener  Informationsdienstleistungen. Fähigkeit, Kontext-Parameter einer  gegebenen Situation zu identifizieren und Technologien zur Erfassung  auszuwählen, die anwendungsspezifischen Anforderungen an Qualität und  Quantität des erfassten Kontexts genügen. Anwendung ausgewählter  Technologien zur Modellierung von Kontext und zur Abbildung von Kontext  auf Information. Verständnis des Aufbaus typischer Architekturen zur  Kontextverarbeitung und Analyse von Anwendungsszenarien hinsichtlich  architekturrelevanter Merkmale. Kenntnis der Anforderungen an  Interaktion mit ubiquitär verfügbaren Inhalten und Fähigkeit, anhand von  Anwendungsanforderungen geeignete Interaktionsformen zu konzipieren.  Inhalt Akademisches Schreiben (Grundlagen) Kontextquellen Inhalt und Kontext verbinden Datenmodelle, z.B., für Kontext, Sensoren, Sammlungen, Datenherkunft Systemarchitekturen für Kontext-Management und Einbindung physikalischer Umgebung EInsatz von Kontext in der Mensch-Maschine-Interaktion           Techniken des Pervasive Computings         Lernziele  Kenntnis der typischen Herausforderungen für die technische Realisierung  des \"Ubiquitous Computings\", auch \"Pervasive Computing\" genannt, mit  dem visionären Ziel \"Internet of Things\"; Kenntnis der typischen  Einsatzszenarien, Verständnis der technischen Grundlagen von  Sensorknoten und des Informationstransports in Adhoc-Netzen sowie der  speziellen Anforderungen der drahtlosen Kommunikation; Kenntnis der  typischen Betriebssysteme und Laufzeitumgebungen für Sensorknoten,  Erstellen von Anwendungen auf Basis dieser Betriebssysteme, Gestaltung  einfacher Sensornetze anhand von Simulationen und realer Hardware,  Analyse von aktuellen Entwicklungen in Bezug auf technische und soziale  Belange  Inhalt Sensornetze: \\tTechniken Hardware Algorithmen \\tfür das Rechnen in Sensornetzen Betriebssysteme \\tfür Sensornetze Selbstorganisation Nahfeldkommunikation RFID Mobile \\tComputing Einsatz von \\tUbicomp für Ambient Intelligence               Modulgruppe 4: Frei wählbare Module der Informatik und Wirtschaftsinformatik      Grundlagen der stochastischen Simulation         Lernziele  Kenntnis der grundlegenden mathematischen Modelle. Fähigkeit, für reale Probleme - bei welchen der Zufall eine Rolle spielt  - stochastische Simulationsmodelle aufzustellen, auszuführen und die  Ergebnisse zu interpretieren.  Inhalt Erzeugung von Zufallszahlen Homogene und nichthomogene Poissonprozesse Warteschlangen Statistische Datenanalyse simulierter Daten Varianzreduktion Anwendungen            Informationstheorie         Lernziele  Kenntnis der Grundlagen und Fähigkeit der Implementierung von  Algorithmen zur Datenkompression und\\xa0zur sicheren Datenübertragung.  Inhalt  Shannons Informationstheorie (Modellierung von Datenquellen und von  Übertragungskanälen). Verlustlose- und verlustbehaftete Datenkomprimierung  (z. B. Huffman Codes, arithmetische Codes, Kompression von Bilddaten). Fehlererkennende und fehlerkorrigierende Codes (z. B. Hamming Codes,  CRC-Codes).           Komplexe vernetzte Systeme         Lernziele  Nach erfolgreichem Abschluss des Moduls: können die Studierenden formale mathematische Beschreibungsmittel und  Beschreibungsheuristiken anwenden. Sie sind in der Lage komplexe Systeme  zu analysieren, Beschreibungs- und Verhaltensmodelle für diese zu  erzeugen und ihr Verhalten durch Simulation zu evaluieren. Aus den  Ergebnissen können ggf. Strukturentscheidungen abgeleitet und  Verhaltensänderungen begründet werden  Inhalt  Ein System ist ein Verbund von Elementen / Komponenten, die  miteinander in Beziehung stehen und interagieren. Das System tritt über  definierte Schnittstellen mit seiner Umwelt (anderen Systemen, Akteuren)  in Verbindung. Die Interaktion umfasst Informations-, Energie- und  Materieaustausch.  Ein Komplexes System ist ein System, dessen Verhalten sich durch die  gegenseitige Beeinflussung seiner Komponenten ergibt. Dabei ist es nicht  einfach aus einer Kombination des jeweiligen Verhaltens seiner  Komponenten beschreibbar, sondern besitzt durch Synergieeffekte darüber  hinausreichende Eigenschaften (Emergenz).  Unter die Def. von komplexen Systemen fallen technische und natürliche  Information ver\\xadar\\xadbei\\xadtende Systeme, ökologische Systeme, sowie soziale  Systeme, z.B. betriebliche und politische Organisationen, soziale  Gruppen.  Die Veranstaltung ist darauf ausgerichtet, informatische bzw.  mathematische Beschreibungsmittel und Beschreibungsheuristiken zu  erfassen und zu wiederzugeben und diese exemplarisch zur  Verhaltenssimulation bzw. -Prognose auf konkrete Situationen anzuwenden.             Kryptographische Protokolle         Lernziele Höhere kryptographische Protokolle für verschiedene Anwendungsbereiche kennen                          und verstehen. Eignung kryptographischer Primitive analysieren und bewerten können. Verschiedene kryptographische Primitive zu einer Problemlösung kombinieren                          können. Kryptographische Protokolle und ihre theoretischen Grundlagen  auf                          wissenschaftlichem Niveau mündlich und  schriftlich beschreiben können. Inhalt  In vielen Bereichen der Informatik sind anspruchsvolle Aufgaben zur                       Informationssicherheit zu lösen, wie z.B. Identitäts- und Rechtemanagement Zugangskontrolle zu Systemen sichere Ende-zu-Ende-Kommunikation in verteilten Anwendungssystemen Bezahlsysteme in B2C- und B2B-E-Commerce-Anwendungen Zur Lösung dieser Aufgaben werden die Grundbausteine der  Kryptographie                      (symmetrische und asymmetrische  Verschlüsselung, digitale Signaturen,                       kryptographische Hashfunktionen, MACs, Zertifikate), auch  kryptographische                      Primitive genannt, zu  kryptographischen                      Protokollen kombiniert. Inhalt  dieser Lehrveranstaltung sind wichtige und                      typische  Protokolle und die ihnen zugrunde liegenden theoretischen Konzepte. Schlüsselaustausch Authentifizierungs- und Identifizierungsprotokolle Interaktive Beweissysteme Zero-Knowledge-Protokolle Commitments Secret Sharing, Secure Multiparty Computation digitales Bargeld Kryptowährungen, Blockchain-Technologie Micropayment elektronische Wahlen. Schwerpunkte im Wintersemester 2018 sollen u.a. sein: Authentifizierung  und Zero                  Knowledge, neuer Hashstandard SHA-3,  Kryptowährungen - Bitcoin, digitales Geld,                   Blockchain-Technologie und -Anwendungen, Internetwahlen.           Seminar Informationssicherheit         Lernziele  Erarbeitung und Kenntnis grundlegender Konzepte zur Sicherstellung der  Informationssicherheit in ausgewählten Anwendungsbereichen.  Inhalt  Typische Themen sind: Online Banking (HBCI-Standard, EC-Karte / PIN-Generierung) Zahlungssysteme (Micropayment, Elektronisches Bargeld) Virtuale Private Network (VPN)-Lösungen mit PPTP, L2TP,\\xa0 IPSec, SSL Wireless LAN – WEP, WAP, WPA , IEEE 802.11i Protokolle und Systeme für elektronische Wahlen Verifizierbare MIXes, anonyme Kanäle Sicherheit beim Mobilfunk Secure Socket Layer SSL / Transport Layer Security TLS Internet Protocol Security: IPSec Zufall für kryptographische Anwendungen: Soft- und Hardware-Generatoren Trusted Computing - Trusted Platform Modules Vertraulichkeit und Authentifizierung in Betriebssystemen XML-Sicherheit\\xa0 / Sicherheit von Web Services Föderierte Identität / Identity Management / Single-Sign-On Digital Rights Management Geeignete Themen können auch von den Studierenden vorgeschlagen werden.           Zuverlässigkeit und Risikobewertung         Lernziele  Nach erfolgreichem Abschluss können die Studierenden wesentliche formale  Analysetechniken zur Bewertung der Zuverlässigkeit von Systemen  anwenden. Aus ermittelten Zuverlässigkeitskennzahlen kann die Evaluation von Risiken erfolgen.  Inhalt  Zuverlässigkeitstechnik - Wahrscheinlichkeit von Fehlern, Verfügbarkeit, Verlässlichkeit; Modelle für Fehler, Einfluss von Reparatur; Vorhersage Fehlertoleranz-Techniken Recovery-Techniken Risikobewertung - Definition von Risiko Risiko-Kategorien, Risiko-Management Fallanalyse, Anwendung, Risikoprognose               Ausgewählte Themen der Korrektheit und Semantik in Programmiersprachen         Lernziele  Bei Abschluss des Lernprozesses wird der erfolgreiche Studierende in der  Lage sein, Anforderungen an Software mit Mitteln der formalen Logik zu  analysieren, zu spezifizieren, korrekte Implementationen zu entwickeln  und die Korrektheit gegen die Spezifikation zu beweisen.  Inhalt  Embedded Devices dringen in nahezu jeden Bereich des täglichen Lebens  vor. Damit nimmt die Bedeutung der Sicherheit, Zuverlässigkeit und  Fehlerfreiheit von Software immer mehr Raum ein. Bei höchsten  Anforderungen an die Fehlerfreiheit ist der Einsatz von mathematischen  Werkzeugen zum Nachweis der Korrektheit eines Programms heute Stand der  Wissenschaft.  Anhand des Buchs “Software Foundations” von Benjamin Pierce oder  \"Certified Programming with Dependent Types\" von Adam Chlipala, werden  die Techniken untersucht mit denen Eigenschaften von Programmen  beschrieben und mit Werkzeugunterstützung nachgewiesen werden können: Anwendung von Logik zur Spezifikation und Rechtfertigung von Programmeigenschaften Anwendung von Beweisassistenten im Nachweis von Programmeigenschaften Funktionales Programmieren als Bindeglied zwischen Logik und Programmiersprachen Typen zur Spezifikation von Programmeigenschaften und deren Nachweis durch Typ-Prüfung des Compilers. Der Schwerpunkt liegt auf praktischen Aspekten: Die erlernten Techniken  werden im Labor mit einem Beweisassistenten, z.B. Coq eingeübt.           Automotive Software Engineering         Lernziele  Bei Abschluss des Lernprozesses wird der erfolgreiche Studierende in der  Lage sein, Techniken und Methoden des Automotive  Softwareentwicklungsprozesses zu kennen und diese im Hinblick und  Automotive Software Engineering Prinzipien zu bewerten. Weiterhin werden  die Studierenden Automotive Modellierungssprachen für konkrete  Szenarien auswählen, anwenden und damit kleinere Automotive Software  Systeme prototypisch entwickeln können.  Inhalt  Dieser Kurs vermittelt ein grundlegendes Verständnis von Methoden und  Prinzipien der automobilen Software-Entwicklung. Dazu gehören Kenntnisse  über die Entwicklung von eingebetteten Systemen sowie über den  Automotive Softwareentwicklungsprozess inklusive Anforderungsmanagement  im Allgemeinen sowie zu AUTOSAR (www.autosar.org), als der zentralen  Standardisierungsinitiative in der Automotive Domäne (dabei insbesondere  die implementierungsnahen Beschreibungsmittel) und FlexRay  (www.flexray.com), als ein zentraler, moderner Vertreter automobiler  Bussysteme, im Besonderen. Die Lehrinhalte werden, neben der  theoretischen Aufbereitung und Diskussion in den Vorlesungsteilen, zudem  im Rahmen von Laborübungen von den Studierenden praktisch angewandt.  Des Weiteren gestalten die Studierenden selbstständig Fachvorträge  inklusive Übungseinheiten, in denen vertiefende Themen des Themenfeldes  Automotive in strukturierter Form aufbereitet werden sollen.           Automotive Systems Modelling         Lernziele  Bei Abschluss des Lernprozesses wird der erfolgreiche Studierende in der  Lage sein, grundlegenden Modellierungskonzepte der Automotive Domäne zu  kennen und deren Nutzung für unterschiedliche Abstraktionsebenen  beginnend auf der abstrakten Gesamtsystemebene (EAST-ADL) bis zu  detaillierten Ebenen (AUTOSAR) bewerten zu können. Die Studierenden  werden unterschiedliche moderne Ansätze der Automotive  Systemmodellierung wie Software-Produktlinien und Security Modellierung  auswählen und ein kleines Automotive System modellieren können.  Inhalt  Dieser Kurs vermittelt grundlegendes Verständnis der Modellierung von automobilen Systemen. Dazu gehören Kenntnisse über die Entwicklung von eingebetteten Systemen sowie über den Automotive Softwareentwicklungsprozess im Allgemeinen, sowie AUTOSAR (siehe auch den Kurs \"Automotive Software Engineering\"), EAST-ADL (http://east-adl.info/), als einer Standardisierungsinitiative in der Automotive Domäne zur Beschreibung der abstrakten Systemarchitektur, Software-Produktlinien, als ein zentraler und moderner Wiederverwendungsansatz von Softwareartefakten komplexer Systeme, und Echtzeitmodellierung ensprechend TADL Standard im Speziellen. Die Lehrinhalte werden durch Anwendung in praktischen Laborübungen, sowie durch analytische und gestalterische Tätigkeiten im Rahmen einer Literaturarbeit vertieft.           Big Data Systeme         Lernziele  Nach der erfolgreichen Teilnahme an diesem Modul kennen die Studierenden  die fachlichen und technischen Herausforderungen, die im Einsatz von  Big Data begründet sind. Die Studierenden können die Potenziale und  Grenzen aktueller Technologien, Architekturkonzepte und  Analyse-Verfahren im Big-Data-Umfeld beschreiben und sind dadurch in der  Lage, Anwendungsfälle zu systematisch zu analysieren und Lösungsansätze  zu entwickeln.  Inhalt  Durch die allgemeine Digitalisierung sowohl im geschäftlichen wie auch im privaten Umfeld entstehen gigantische Datenmengen. Mobile Commerce, Smart Home, Connected Car, Internet of Things sind einige Schlagworte aus diesem Umfeld. Unternehmen möchten diese \"Big Data\" nutzen, um Geschäftsprozesse zu optimieren, Risiken zu senken, Probleme zu erkennen und nicht zuletzt Informationen über Kunden und deren Vorlieben gewinnbringend zu nutzen. In der Vorlesung werden fachliche und technische Fragestellungen, Herausforderungen und Lösungen diskutiert.  Die Lehrveranstaltung beginnt mit einem Vorlesungsteil, um das Thema zu motivieren sowie grundlegende Technologien vorzustellen. Ergänzt wird dieser Teil durch praktische Übungen. Die Themen in diesem Block sind: Einführung zu Big Data Verteilte Datenhaltung und Datenverarbeitung - Partitionierung, Replikation, Konsistenzsicherung - Verteilte Anfrageverabeitung und -optimierung - Map-Reduce Skalierbare Datenverarbeitung mit Apache Hadoop und Spark Grundlagen maschineller Lernverfahren Der zweite Teil wird durch studentische Vorträge und Ausarbeitungen gestaltet. Dabei werden ausgewählte fachliche, technische und methodische Fragestellungen aus dem Bereich Big Data vertieft. Themen in diesem Block können beispielsweise sein: Social Media Analytics Text Mining Knowledge Extraction Ethische Aspekte von Big Data Big Data Security Datenstrom-Verarbeitung In-Memory-Systeme u.v.m           Echtzeitsysteme im Automobil         Lernziele Kenntnis der theoretischen Grundlagen von Echtzeitsystemen und Synchronisationsmechanismen Kenntnis von spezifischen Echtzeitsystemen im Automobil Echtzeit-Anforderungen eines Systems analysieren und bewerten Echtzeitsysteme  entwerfen und entsprechende Konzepte anwenden Inhalt  Dieser Kurs vermittelt die Grundlagen von Echtzeitsystemen, mit speziellem Fokus auf das Automobil. Echtzeitsysteme finden sich in vielen Bereichen des täglichen Lebens, bei der Steuerung von zeitkritischen Prozessen, z.B. im Auto (Airbag, ABS, ESP, ...) in Flugzeugen (Triebwerksteuerung, fly-by-wire, ...) bei der Mobil-Kommunikation (Sprachübertragung) Ein Echtzeitsystem ist dadurch gekennzeichnet, dass es neben seiner funktionalen Korrektheit auch zeitlichen Anforderungen gerecht werden muss. Der Begriff \"echtzeifähig\" wird häufig als Synonym  für Schnelligkeit interpretiert: Multimedia-Daten werden \"in Echtzeit\"  verarbeitet, Roboter reagieren \"in Echtzeit\" auf Ereignisse in ihrer  Umgebung, Aktienkurse werden \"in Echtzeit\" aktualisiert, etc. Dabei  bedeutet \"echtzeitfähig\" aber nicht \"besonders schnell\", sondern  vielmehr \"schnell genug\". Ein Echtzeitsystem muss sich an die zeitlichen  Bedingungen seiner Umwelt anpassen und seine Berechnungen und  Ergebnisse immer zum richtigen Zeitpunkt, also rechtzeitig liefern.  Der Kurs besteht aus einem Vorlesungsteil und praktischen Übungen (Programmieraufgaben) im Labor.  Inhalte der Vorlesung: Echtzeit-Scheduling: Verfahren und Analyse Echtzeitbetriebssysteme im Automobil (OSEK, AUTOSAR) Echtzeitkommunikation im Automobil (CAN, FlexRay) Synchronisationsmechanismen / verteilte Systeme Software-Architekturen mit Echtzeitanforderungen im Automobil Design von Echtzeitsystemen Praktische Übungen (in Gruppen von 2 bis 4 Studierenden), z.B. Implementation von Scheduling-Algorithmen Design und Implementation einfacher Echtzeitsysteme ...           Formale Methoden im Softwareengineering         Lernziele  Bei Abschluss des Lernprozesses wird der erfolgreiche Studierende in der  Lage sein, formale Methoden für die Softwareentwicklung problembezogen  auszuwählen und mit automatischen und halbautomatischen Werkzeugen den  Nachweis bestimmter Eigenschaften einer Implementierung zu entwickeln.  Inhalt  Methode formaler Softwarespezifikation, Anwendung von Beweisassistenten  und automatischen Theorembeweisern in der Korrektheit von Software,  Programmverifikation, Sicherheit von mobilem Code und  Protokollverifikation           Interpretierer und ähnliche Programmierwerkzeuge         Lernziele  Der erfolgreiche Studierende wird nach Abschluss der Veranstaltung in  der Lage sein zu beurteilen, wann Interpretierer und wann Compiler  vorzugsweise einzusetzen sind und insbesondere auch vorschlagen und  begründen können, welche Methoden verwendet werden können, um geeignete  Werkzeuge zu erstellen, die zur Unterstützung beim Entwickeln und Testen  von Programmierprojekten empfehlenswert sind.  Inhalt  Komponenten von Interpretern lexikalische, syntaktische und semantische Analyse; Laufzeitsysteme. In den Übungen wird ein Interpretierer vorgestellt, der um einige Programmkonstrukte zu erweitern ist.           Systementwurf und Systemdokumentation mit UML und SysML         Lernziele  Bei\\xa0Abschluss\\xa0des\\xa0Lernprozesses\\xa0wird\\xa0der\\xa0erfolgreiche\\xa0Studierende\\xa0in\\xa0der\\xa0\\xa0    Lage\\xa0sein, komplexe IT-Systeme mittels UML\\xa0und SysML zu modellieren,  zu dokumentieren, zu analysieren und zu entwerfen.  Inhalt  Die Unified Modeling Language (UML) ist die gängigste und als  Standard akzeptierte Notation zur Beschreibung von Computersystemen  sowie der dynamischen Abläufe in Computersystemen. Dies umfasst sowohl  betriebswirtschaftliche Geschäftsprozesse wie auch technische  Interaktionen, die auf Arbeitsplatzrechnern oder auch in verteilten  Systemen ablaufen. Die UML dient zur Modellierung, Dokumentation,  Spezifizierung und Visualisierung komplexer Systeme, indem sie die  Darstellung statischer und dynamischer Modelle während der Analyse-,  Design-, Implementierungs- und Installations-Phase ermöglicht.  Die Object Management Group (OMG, www.omg.org )  organisiert und koordiniert – neben weiteren Standards – die UML als  Hersteller-neutralen Industrie-Standard und verabschiedete im Jahre 2005  die Version UML 2.  Die SysML (Systems Modeling Language)\\xa0ist eine junge  Weiterentwicklung der UML, die zum Design und zur Dokumentation  komplexer Systeme dient, die nicht notwendigerweise ihren Schwerpunkt im  Software-Bestandteil besitzen. Beispiele dafür sind weit verbreitete  Hilfsmittel wie PDAs, GPS-Geräte, aber auch Automobile, Flugzeuge, etc.  Im Rahmen der Lehrveranstaltung werden Notation und Semantik der  verschiedenen UML- und SysML-Diagramme diskutiert und verschiedene  Einsatzgebiete werden im Rahmen von Projekten untersucht.           Verteilt-kooperative Informationsverarbeitung         Lernziele  Kenntnisse und Fähigkeiten erwerben, Anwendungen aus kooperierenden,  eigenständigen Komponenten zu erstellen (Agenten); Analysieren der  Sicherheit derartiger Anwendungen; Kodierung von Wissen und Organisation  des Austauschs von Wissen zwischen Komponenten eines Systems; Bewertung  von Nutzen und Risiken selbstorganisierender Vorgänge; Transfer auf  verwandte Themen (Sensornetze); Entwurf und Realisierung  problemspezifischer kooperierender Systeme  Inhalt Was sind \\tAgenten? Grundlagen \\tvon Agentenplattformen Mobile \\tAgenten Das \\tAgentensystem JADE Kommunikation \\tzwischen Agenten Wissenstransfer Intelligente \\tAgenten Koordination Simulation mit Agenten Selbstorganisation \\tund Emergenz           Anwendungssysteme der Versicherungswirtschaft            Business Analytics         Business Analytics  Lernziele  Am Ende der Lehrveranstaltung sollen die Studierenden über fundierte  Kenntnisse zu den u.a. Themenfeldern verfügen. Die Studierenden sollen  relevante Methoden und Begriffe kennen und sicher mit ihnen umgehen  können. Ferner sollen die Studierenden das Erlernte in der Arbeitswelt  anwenden können. Durch Teamarbeiten soll darüber hinaus die  Sozialkompetenz der Studierenden gestärkt werden.  Inhalt  Die Studierenden erhalten in dem Kurs einen vertiefenden Einblick zu folgenden Themenfeldern: Ausgewählte Aspekte des Data Mining, z.B. Predictive Analytics Ausgewählte Methoden des Business Analytics, z. B. Regressionen, Time-Series-Analyses, Fuzzy Linear Programming Anwendungen aus verschiedenen Domains, z. B. Wertpapierhandel, Finanzplanung Business Analytics Software Werkzeuge           Business Intelligence         Lernziele  Am Ende der Lehrveranstaltung sollen die Studierenden über fundierte  Kenntnisse zu den in den Lehrinhalten angegebenen Themenfeldern  verfügen. Die Studierenden sollen relevante Begriffe kennen und die  Fähigkeit haben,\\xa0sicher mit ihnen umgehen können. Ferner sollen die  Studierenden das Erlernte in der Arbeitswelt anwenden können. Durch  Teamarbeiten soll darüber hinaus die Sozialkompetenz der Studierenden  gestärkt werden. Die Studierenden sollen in der Lage sein, sich die  Kursinhalte mittels unterschiedlicher Lernmethoden anzueignen (z. B.  Gruppenarbeit, praktische Erprobung von theoretischen Erkenntnissen am  SAP System).  Inhalt  Die Studierenden erhalten in dem Kurs einen vertiefenden Einblick zu folgenden Themenfeldern: Aufzeigen von Nutzungsmöglichkeiten in Unternehmen Vermittlung von Anwenderwissen im Bereich SAP Business Intelligence Analysemöglichkeiten mit Business Intelligence.           Customer Relationship Analytics         Lernziele  Ziel der Veranstaltung ist es die analytischen Methoden des Customer  Relationship Management und E-Commerce für spezifische Szenarien, wie  z.B. Warenkorbanalysen, Kundenwertanalysen, Kundengruppenanalysen,  Retourenanalyse und Collaborative Filtering zu kennen und eine Methode  aus dem oben genannten Feldern i.d.R. Recommender Algorithmen verstehen,  implementieren und evaluieren zu können. In der Veranstaltung geht es  weniger um die Frage des technischen Aufbaus einer Business Intelligence  Lösung, sondern viel mehr um das Aufbrechen und Verstehen der  \"Black-Box\", die auf Knopfdruck aus Zahlen Analyseergebnisse liefert.\\xa0  Dabei kommt u.a. die Analysesoftware R zum Einsatz.  Inhalt  Die Veranstaltung gliedert sich in drei Abschnitte:  Der erste Teil findet als Präsenzveranstaltung statt. Dabei  werden Sie mit CRM, CRM Analytics, statistischen Grundlagen, der  Analysesoftware R und dem Verfassen von wissenschaftlichen Artikeln  vertraut gemacht. Im zweiten Teil bearbeiten Sie ein Thema zu CRM Analytics, i.d.R  Recommender Algorithmen. Die Dozenten unterstüzen Sie in dieser Phase  mit Beratung und Feedback. Im dritten Teil präsentieren Sie Ihre Ergebnisse.           E-Government         Lernziele  Kenntnis von Electronic Government und des Umfeldes; Fähigkeit zur  Beurteilung der Bedeutung und des Umfangs von Anwendungen im  E-Government; Kenntnis und Fähigkeit zur Analyse praktischer Maßnahmen  des E-Governments anhand von Beispielen; Fähigkeit Konzepte für  E-Government-Lösungen zu erarbeiten.  Inhalt Was bedeutet E-Government ? Welche Bereiche umfasst E-Government ? Bearbeitung einer Fallstudie oder eines Projektes Anhand von Referaten und Fallstudien oder im Rahmen eines Projektes  erarbeiten wir in diesem Seminar das Thema und Umfeld von E-Government.             Gamification von Informations- und Anwendungssystemen         Lernziele  Identifizieren und Beschreiben motivierender Spielmechanismen und Erwerb  der Fähigkeit, diese Mechansimen hinsichtlich ihrer Wirkung und Eignung  kritisch zu beurteilen und auszuwählen, um motivierender Informations-  und Anwendungssysteme zu gestalten  Inhalt  Theoretische Grundlagen: Psychologische Motivationstheorien zur Wirkung motivationaler Anreizsysteme Gamification Frameworks zum Einsatz von Spielformen, Spielelementen und Spielertypen Gamification Patterns: Identifikation und Beschreibung motivierender Spielmechanismen als generische Entwurfsmuster (Gamification Patterns) Vorgehensmodell zur Anwendung der Entwurfsmuster bei der Gestaltung motivierender Informations- und Anwendungssysteme Praktische Übung und Vortrag: Kritische Analyse gamifizierter Informations- und Anwendungssysteme in unterschiedlichen Domänen Gestaltung eines eigenen gamifizierten Informations- oder  Anwendungssystems unter Anwendung der identifizierten Gamification  Patterns           Global Software Engineering         Global Software Engineering  Lernziele  Die Globalisierung der Softwareentwicklung verlangt für das erfolgreiche  Projektmanagement neue Fähigkeiten. Spezielle\\xa0 Methoden zur  Unterstützung dieser neuen, globalen Herausforderungen des  Projektmanagers werden erforscht.  Inhalt  Simulation eines verteilten, globalen Software-Engineering-Projekts in Echtzeit Studierenden arbeiten in Projektgruppen, bevorzugt mit ausländischen  Partnern, um ein gemeinsames Software-Engineering-Projekts zu  realisieren.           Information Management Challenge         Lernziele  Im Rahmen der Veranstaltung wird zusammen mit einem Unternehmen eine    aktuelle IT relevante Problemstellung des Unternehmens von den   Studierenden  analysiert und ein Lösungsansatz erarbeitet. Die   Studierenden arbeiten dabei in miteinander um den besten  Lösungsansatz   konkurrierenden Gruppen (Information Management  Competition). Am Ende   der Veranstaltung verfügen die Studierenden über folgende Kompetenzen:  Fachkompetenz: Die Studierenden erhalten einen Einblick in eine   konkrete Unternehmenssituation, der zugehörigen IT Landschaft sowie der   Information Management Herausforderung ( z.B. Social CRM, Enterprise  IT  Architekturen,  Wissensmanagement, variiert mit den aktuellen   Fragestellungen des beteiligten Unternehmens). Für dieses Themenfeld der   Wirtschaftsinformatik (Information Management Herausforderung)  erhalten  und erarbeiten sich die Studierenden vertiefte Kentnisse  hinsichtlich  Geschäftsprozessen, IT Systemen und organisatorischen  Abhängigkeiten. Methodenkompetenz: Fähigkeit eine IT relevante Problemstellung   im Unternehmenskontext zu analysieren, dafür eine unter   betriebswirtschaftlichen, organisatorischen und IT Aspekten sinnvolle   und nutzenstiftende Lösung zu entwickeln und dabei relevante Werkzeuge   wie Projektmanagement und Web 2.0 zielgerichtet einzusetzen.   Darüberhinaus sollen die Studierenden lernen aus der im Internet zur   Verfügung stehenden Informationsflut die für die Aufgabenstellung   relevanten Informationen herauszuarbeiten und für die   Management-/Entscheiderebene im Unternehmen aufzubereiten\\xa0 (\"digital   content curation\"). Soziale Kompetenz: Organisation, Planung, Aufgabenverteilung und Entscheidungsfindung im Team Inhalt  Die Veranstaltung findet in Zusammenarbeit mit  Siemens GS IT statt.  Dabei werden u.a. Dozenten von Siemens GS  IT einführende Vorträge  geben, so dass Sie den firmenspezifischen  Hintergrund und die  relevanten IT Systeme besser einordnen und verstehen  können.  Die Challenge hat das Thema (Kurssprache ist deutsch):  Im Rahmen der Veranstaltung wird zusammen mit einem Unternehmen eine   aktuelle Problemstellung des Unternehmens von den Studierenden   analysiert und ein Lösungsansatz erarbeitet. Je nach Problemstellung   kann der Lösungsansatz ein Konzept, ein Software-Prototyp oder beides   sein. Die Studierenden arbeiten dabei in miteinander um den besten   Lösungsansatz konkurrierenden Gruppen (Information Management   Competition). Das notwendige Hintergrundwissen wird den Studierenden zu   Beginn der Veranstaltung in Theorieblöcken durch die Dozenten und   Unternehmensvertreter vermittelt. Außerdem erhalten sie   projektbegleitend Coaching durch die Dozenten. Jede studentische   Arbeitsgruppe stellt am Ende des Semesters ihr Ergebnis vor, das von   einer Jury aus Dozenten und Unternehmensvertretern zusammen mit der   schriftlichen Ausarbeitung bewertet und in eine Rangordnung gebracht   wird.             Informationssystemmodellierung und Erkenntnistheorie         Lernziele  Kenntnis erkenntnistheoretischer Bedingungen der Modellierung von Informationssystemen Kenntnis kognitiver, psychologischer und sozialer Einflüsse auf Software-Entwicklung und -Einsatz (\"human factor\") Kenntnis psychologischer und sozialer Auswirkungen der Informations-Technologie Fähigkeit mit den unerwünschten Konsequenzen der kognitiven  Randbedingungen der Modellierung betrieblicher Informationssysteme  bewusst und professionell umzugehen  Inhalt  Interdisziplinäre Blicke auf die Entwicklung von Informationssystemen (nicht aus der Perspektive der Informatik)  Eine Auswahl der folgenden Themen: Aufwandsschätzung von IT-Projekten auf erkenntnistheoretischer Grundlage Analogisches Denken Multiperspektivität Grundsätze ordnungsmäßiger Modellierung (Jörg Becker) Erkenntnistheoretische Beurteilung der Geschäftsprozessmodellierung Erkenntnistheoretische Beurteilung der objektorientierten Modellierung Prädisposition (Vorwissen, Psyche) des Systemanalytikers als Erkenntnissubjekt Besonderheiten der Erkenntnisobjekte (Einsatzbereiche) der Wirtschaftsinformatik Beziehung von Erkenntnissubjekt und Erkenntnisobjekt bei der Modellentwicklung Kritischer Realismus und evolutionäre Erkenntnistheorie Changed / Creeping Requirements Management Qualitätsvergleich von Modellierungstools ISO 9000 Software-Entwicklung und menschliches Problemlöseverhalten Software-Entwicklung und Kreativität(sförderung), Bewusstseinszustände Software-Entwicklung und der Programmieranfänger Software-Entwicklung im Unterricht an Hochschulen (SEUH Tagungsreihe) Software-Entwicklung und Unterricht in Systemanalyse, im Programmieren Software-Entwicklung und Organisations/Arbeitspsychologie/Gruppendynamik Software-Entwicklung und die Kommunikation Anwender - Entwickler Software-Einsatz und Ängste/Akzeptanz beim Benutzer Software-Einsatz und Organisations/Arbeitspsychologie/Gruppendynamik Software-Einsatz und Gedächtnis/Lern/Wahrnehmungspsychologie Software-Einsatz und Ergonomie           IT-Supplier Relationship Management         Lernziele Fachkompetenz: Die Teilnehmer erwerben Kenntnisse über die  wichtigsten Handlungsfelder eines IT Vendor Managers, wie z.B. IT  Sourcing und\\xa0laufende Führung von IT-Dienstleistern. Methodenkompetenz: Vermittlung von monetären und nicht monetären  Methoden zur Steuerung und Positionierung des IT\\xa0Sourcings in einem  Unternehmen Inhalt  Die Veranstaltung widmet sich dem breiten Spektrum an  IT-Outsourcing-Möglichkeiten in strukturierter Weise und zeigt  Handlungsoptionen und Steuerungsinstrumente für auslagernde Unternehmen  auf. Folgende Themenschwerpunkte werden abgedeckt:  1. Grundlagen und Definitionen, Zugrundeliegende Theorien 2. Chancen und Risiken im Outsourcing des IT Betriebes 3. IT Provider Lebenszyklus 4. Vertragswesen: Due Diligence & Vertragsgovernanc 5. Supplier Relationship Management & Provider Governance 6. Besonderheiten von Nearshore- und Offshore Beziehungen 7. SRM Systeme für das Management des Provider Lebenszyklus             Logistische Informationssysteme         Lernziele komplexe logistische System- und Ablaufstrukturen selbständig analysieren und wissenschaftlich einordnen operationale Optimierungsmodelle und -verfahren für logistische  Gestaltungs- und Steuerungsprobleme - unter Berücksichtigung bestehender  Interdependenzen zwischen Entscheidungen – auswählen, anwenden und ggf.  weiterentwickeln innovative Konzepte und Methoden der Logistik analytisch  beurteilen und ihre Einsatzchancen und ‑grenzen kritisch reflektieren  und bewerten Wirkungszusammenhänge für Optimierungsberechnungen und Simulationsstudien modellieren und quantifizieren Inhalt  Behandelt werden Ausschnitte aus folgenden Themenkreisen: Entscheidungs-, Koordinations- und Abwicklungsebenen der computergestützten Unternehmenslogistik Innovative Logistikkonzeptionen und –strategien (z. B. Supply Chain Management, Quick-Response-Logistik, Postponementstrategien) Systemtypen (z. B. Warenwirtschaftssysteme, Advanced Planning Systems) Einsatzbereiche und Fallbeispiele (z. B. Tourenplanung, Bestandsdisposition, Standortoptimierung)            Operations Management         Lernziele inner- und zwischenbetriebliche Gestaltungs- und  Steuerungsprobleme des Funktionalbereichs Operations selbständig  analysieren und wissenschaftlich einordnen quantitative Modelle und Verfahren hinsichtlich der  Einsatzpotentiale und Anwendungsgrenzen beurteilen und für praktische  Fragestellungen adaptieren und anwenden operationale Optimierungsmodelle für komplexe betriebliche Entscheidungsprobleme auswählen, anpassen oder ggf. neu entwickeln Inhalt  Behandelt werden Ausschnitte aus folgenden Themenkreisen: Operations Management: Management von Produktions- und Dienstleistungsprozessen Klassische Transportmodelle für ein- und mehrstufige Systeme Standortoptimierung Ausgewählte Probleme des Bestandsmanagements Supply-Chain-Konzepte und -Strategien            Strategisches IT-Management         Lernziele Fachkompetenz: Die Teilnehmer erwerben Kenntnisse über die  wichtigsten Handlungsfelder eines CIOs, wie z.B. IT-Strategie, IT  Architektur, IT Portfoliomanagement und IT Sourcing, Methodenkompetenz: Vermittlung von monetären und nicht monetären  Methoden zur Steuerung und Positionierung der IT Funktion in einem  Unternehmen, Sonstige Kompetenzen: Übertragung der klassischen Elemente der Unternehmenssteuerung in den Bereich IT Management. Inhalt  Die IT hat sich besonders in jüngerer Zeit von einer »einfachen«  Ressource zu einem strategischen Erfolgsfaktor entwickelt. Strategisches  IT-Management ist daher eine vergleichsweise junge Managementaufgabe,  die zum Ziel hat, den Wertbeitrag der IT zum Unternehmenserfolg zu  steigern und gleichzeitig die mit der IT verbundenen Risiken und Kosten  zu minimieren. Um dieses Ziel zu erreichen, müssen u.a. die folgenden  Herausforderungen gelöst werden:  Aufbau einer effektiven und effizienten IT Organisation Nachweis des Wertbeitrags oder zumindest der Wirtschaftlichkeit der IT, Abstimmung von IT- und Unternehmensstrategie (IT-Business-Alignement) Einhalten gesetzlicher Vorgaben und anderer Regulierungsanforderungen (Compliance) Festlegen des IT-Outsourcinggrades und der Strategien zur Lieferanten-/Dienstleistungsauswahl (Sourcing)            Workflow Systeme         Lernziele  Kenntnis der Architektur und Konzeption von Workflow-Systemen und  verwandten Systemen\\xa0zur Geschäftsprozessautomatisierung (Business  Process Management)\\xa0und Workflow-Entwicklung Einfache Workflows mit einem Workflow-System entwickeln können  Inhalt Am praktischen Beispiel wird Workflow als Mittel der DV-technischen Unterstützung von Geschäftsprozessen behandelt Die praktischen Übungen werden mit SAP Business Workflow und SAP NetWeaver Business Process Management durchgeführt               Weitere Module      IT-Projekt         Lernziele  Einblick in die Vielgestaltigkeit von Anwendungen der  Informationstechnologie. Fähigkeit zur Umsetzung einer Problemstellung  in eine IT-Lösung mit Teamarbeit.  Inhalt  Die Studierenden bearbeiten in Teams von ca. 4 Mitgliedern jeweils ein  Thema aus der angewandten Informatik. Sie werden dabei von einem  Dozenten betreut. Typischerweise ist für eine ausgewählte\\xa0  Problemstellung eine IT-Lösung in Form einer Software-Anwendung zu  planen, zu entwerfen und zu implementieren, oder es sind implementierte  Lösungen zu beurteilen und anzupassen.           Interkulturelle Kommunikation         Lernziele Effektives Formulieren auf Englisch in beruflichen und fachlichen Situationen Verfassen einer technischen Veröffentlichung auf Englisch Inhalt  Jeder Kursteilnehmer wird ein eigenes technisches Thema als Fokus für den Kurs wählen. Es gibt drei Aufgaben: Ein fünfzehnminütiges Interview, in dem die Kursteilnehmer das Thema vertiefen und etwaige Fragen beantworten Ein schriftlicher achtseitiger (ca. 5000 Worte) Forschungsbericht mit dem vorgegebenen Format einer technischen Veröffentlichung Ein fünfzehnminütiges Referat zu den Forschungsergebnissen               Masterarbeit  Das dritte Semester dient der Anfertigung einer Masterarbeit mit einem  Bearbeitungsumfang von 30 Leistungspunkten. Der Studierende soll mit der  Masterarbeit seine Fähigkeit nachweisen, die im Studium erworbenen  Kenntnisse und Fähigkeiten in einer selbständigen wissenschaftlichen  Arbeit auf komplexe Aufgabenstellungen anzuwenden.        Auslandsstudium  Ein Auslandsstudium wird ab dem 3. Semester empfohlen, es sei denn, die Planungen beginnen bereits in der Endphase des Bachelorstudiums. Es kann nur ein Studiensemester (ggf. mit Einschluss der Masterarbeit) sein. Ein Studiensemester im Ausland braucht einen Vorlauf von 9 Monaten. Die geplanten Lehrveranstaltungen sind vorab mit dem Beauftragten für Hochschulkooperationen und Auslandsbeziehungen der Fakultä t  abzusprechen, dann werden im Ausland erbrachte Leistungen großzügig anerkannt. Organisatorische Details sind mit dem International Office zu klären.        Wichtige Dokumente      Studien- und Prüfungsordnung  (117 KB)  Die Studien- und Prüfungsordnung ist die rechtliche Grundlage für die Ausgestaltung des Studienganges (inkl. Inhalten und zu absolvierenden Prüfungsleistungen).       Studienplan  (138 KB)  Der Studienplan veranschaulicht die Verteilung der einzelnen Fächer, Praktika, Praxissemester und Abschlussarbeiten auf die einzelnen Studienabschnitte / Semester       Modulhandbuch  (429 KB)  Das Modulhandbuch listet die zu absolvierenden Fächer auf und beschreibt diese detailliert bezüglich Inhalten, Lernzielen, zeitlichem Arbeitsaufwand und Prüfungsleistungen.                 Tätigkeitsfelder und Berufsbilder  Ein Absolvent des Masterstudiengangs „Medieninformatik“ ist in besonderem Maße zu folgenden Tätigkeitsschwerpunkten befähigt: Computergrafik und Visualisierung (z.B. Medizintechnik, CAD, Navigationssysteme, Computerspiele) Bild-, Video- und Audioverarbeitung (z.B. medizinische Diagnostik, Automatisierungstechnik, Unterhaltungselektronik) Mensch-Maschine-Interaktion (grafische Benutzerschnittstellen, Web-Interfaces, multimodale Bedienkonzepte, mobile Anwendungen) Medieninformatiker sind besonders qualifiziert als Bindeglieder in der arbeitsteilig organisierten Softwareentwicklung. Das Verständnis für Benutzerschnittstellen qualifiziert Medieninformatiker darüberhinaus für eine Tätigkeit in den Phasen der Anforderungs- und Funktionsdefinition von Softwaresystemen und der Optimierung von computergestützten Prozessen.  Absolventen des Studiengangs können von exzellenten Berufsperspektiven in zahlreichen unterschiedlichen Branchen ausgehen. In der Metropolregion Nürnberg sind neben der vor Ort stark vertretenen IT-Industrie besonders die Automobilzulieferindustrie und die Medizintechnik als mögliche Beschäftigungsfelder zu nennen.        Weitere Qualifikationsmöglichkeiten nach dem Masterabschluss  Das Masterstudium kann für den Kreis stärker wissenschaftlich orientierter Studierender die Befähigung für ein Promotionsstudium an einer deutschen oder ausländischen Universität bieten.              Informationen zur Zulassung   Eignungsfeststellung  - vgl. dazu die Studien- und Prüfungsordnung    Erforderliche Sprachnachweise:  Die Unterrichtssprache in diesem Studiengang ist Deutsch. Bewerberinnen und Bewerber mit einer anderen Muttersprache, die keinen deutschen Bachelorabschluss haben und auch keine deutschsprachige Ausbildung an einer höheren Schule abgeschlossen haben, müssen eines der folgenden Zertifikate nachweisen:   Deutsche Sprachprüfung für den Hochschulzugang ausländischer Bewerber und Bewerberinnen (DSH-Stufe 2)    Test Deutsch als Fremdsprache mit überdurchschnittlichem Ergebnis (TestDaF; mindestens Niveaustufe 4 in allen 4 Prüfungsteilen)    Telc Deutsch C1 Hochschule   Überblick über alle anerkannten Sprachnachweise (pdf)                Studienberatung   Sie suchen individuelle Beratung und Unterstützung bei Studienorientierung und Studienwahl. Dann können Sie sich gerne an unser Team der zentralen Studienberatung wenden.  Zentrale Studienberatung  Studienfachberatung   Sie möchten sich detaillierter über Inhalte einzelner Fächer des Masterstudiengangs Medieninformatik informieren? Dann ist die Studienfachberatung die richtige Anlaufstelle für Sie.           Florian  Gallwitz      Prof. Dr.-Ing.      telefon  +49 (0)911 5880 - 1677     email  florian.gallwitz at th-nuernberg Punkt de     fax  +49 (0)911 5880 - 5666     Raum HQ.518                  Fragen Sie uns    studierendenservice at th-nuernberg Punkt de Telefon: +49 911 5880 4004 (Mo, Di, Do: 9:00\\u2009–\\u200916:00 Uhr, Mi: 9:00\\u2009–\\u200914:00 Uhr, Fr: 9:00\\u2009–\\u200913:00 Uhr)    Studieren im Ausland    Mehr als 160 Partnerhochschulen weltweit stehen Studierenden der TH Nürnberg für einen Auslandsaufenthalt offen. Das International Office informiert Sie über mögliche Wege ins Ausland.    Dokumente      Studien- und Prüfungsordnung     Studienplan     Modulhandbuch       zurück         ',\n",
       " '       Allgemeine Studienberatung  Allgemeine Beratung erhalten Sie von der zentralen  Studienberatung. Die persönlichen Anforderungen, die für ein erfolgreiches Studium wichtig sind, haben wir zusammengestellt.        Studienfachberatung Informatik, Medieninformatik und Wirtschaftsinformatik  Wir wollen, dass Sie erfolgreich studieren! Wir beraten Sie vor Beginn und während des Studiums. Wir informieren Sie in regelmäßigen Veranstaltungen. Wir vermitteln Ansprechpartner wie Professoren, Kommissionen, Externe. Sie erreichen uns zu den ausgehängten Sprechzeiten bzw. über Telefon oder per E-Mail.            Bachelor - Studiengang Informatik - Studiengang Medieninformatik       Bachelor - Studiengang Informatik - Studiengang Medieninformatik         Name  Kontakt            Christian  Schiedermeier     Prof. Dr.-Ing.      telefon  +49 (0)911 5880 - 1179     email  christian.schiedermeier at th-nuernberg Punkt de     fax  +49 (0)911 5880 - 5666               Bachelor - Studiengang Wirtschaftsinformatik       Bachelor - Studiengang Wirtschaftsinformatik         Name  Kontakt            Rainer  Groß     Prof. Dr. rer. pol.      telefon  +49 (0)911 5880 - 1660     email  rainer.gross at th-nuernberg Punkt de     fax  +49 (0)911 5880 - 5666               Master - Studiengang Informatik       Master - Studiengang Informatik         Name  Kontakt            Jörg  Roth     Prof. Dr.      telefon  +49 (0)911 5880 - 1169     email  joerg.roth at th-nuernberg Punkt de     fax  +49 (0)911 5880 - 5666               Master - Studiengang Medieninformatik       Master - Studiengang Medieninformatik         Name  Kontakt            Florian  Gallwitz     Prof. Dr.-Ing.      telefon  +49 (0)911 5880 - 1677     email  florian.gallwitz at th-nuernberg Punkt de     fax  +49 (0)911 5880 - 5666               Master - Studiengang Wirtschaftsinformatik       Master - Studiengang Wirtschaftsinformatik         Name  Kontakt            Patricia  Brockmann     Prof. Dr.      telefon  +49 (0)911 5880 - 1170     email  patricia.brockmann at th-nuernberg Punkt de     fax  +49 (0)911 5880 - 5666               Grundpraktikum / Praktisches Semester       Grundpraktikum / Praktisches Semester         Name  Kontakt            Wolfgang  Bremer     Prof. Dr.      telefon  +49 (0)911 5880 - 1285     email  wolfgang.bremer at th-nuernberg Punkt de     fax  +49 (0)911 5880 - 5666                  ',\n",
       " \"       Publikationen der Arbeitsgruppe (peer-reviewed)  2022 Feitl, S., Kreimeier, J., & Götzelmann, T. Accessible Electrostatic Surface Haptics: Towards an Interactive Audiotactile Map Interface for People With Visual Impairments. Proc. of WS on Enabling Technologies for People with Disabilities (ENABLE) ; ACM:New York, NY, USA, 2022. https://doi.org/10.1145/3529190.3534781 . Oumard, C., Kreimeier, J., & Götzelmann, T. Implementation and  Evaluation of a Voice User Interface with Offline Speech Processing for  People who are Blind or Visually Impaired. In The 15th PErvasive Technologies Related to Assistive Environments Conference; PETRA '22; ACM:New York, NY, USA, 2022. https://doi.org/10.1145/3529190.3529197 . Thi Ngan Ly, K., Karg, P., Kreimeier, J., & Götzelmann, T. Development and Evaluation of a Low-cost Wheelchair Simulator for the Haptic Rendering of Virtual Road Conditions.   In The 15th PErvasive Technologies Related to Assistive Environments Conference; PETRA '22; ACM:New York, NY, USA, 2022. https://doi.org/10.1145/3529190.3529195 . Oumard, C., Kreimeier, J., & Götzelmann, T. Pardon? An Overview of the Current State and Requirements of Voice User Interfaces for Blind and Visually Impaired Users. In Computers Helping People with Special Needs; ICCHP'22; wird erscheinen, 2022. https://arxiv.org/abs/2203.05848 https://arxiv.org/abs/2203.05848 https://arxiv.org/abs/2203.05848 .   2021 Götzelmann, T., Kreimeier, J., Schwabl, J., Karg, P., Oumard, C., Büttner, F. AmI-VR: An Accessible Building Information System as Case Study Towards the Applicability of Ambient Intelligence in Virtual Reality. In Mensch und Computer 2021; MuC '21; ACM:New York, NY, USA, 2021. https://doi.org/10.1145/3473856.3474032 . Karg, P., Kreimeier, J., & Götzelmann, T. Build-and-Touch: A  Low-Cost, DIY, Open-Source Approach Towards Touchable Virtual Reality.  In The 14th PErvasive Technologies Related to Assistive Environments Conference; PETRA '21; ACM:New York, NY, USA, 2021. https://doi.org/10.1145/3453892.3462217 . Oumard, C., Kreimeier, J., & Goetzelmann, T. Preliminary Analysis on  Interaction Characteristics with Auditive Navigation in Virtual  Environments . In The 14th PErvasive Technologies Related to Assistive Environments Conference; PETRA '21; ACM:New York, NY, USA, 2021. https://doi.org/10.1145/3453892.3461654 . 2020 Götzelmann, T.; Kreimeier, J. Participation of Elderly People in Smart City Planning by Means of Virtual Reality. In Proc. 13th ACM International Conference on PErvasive Technologies Related to Assistive Environments ; ACM: New York, NY, USA, 2020. https://doi.org/10.1145/3389189.3397649 . Götzelmann,  T.; Kreimeier, J. Optimization of Navigation Considerations of People  with Visual Impairments through Ambient Intelligence. In Proc. 2nd International Workshop on Accessibility and the Smart City: Technological Challenges and Open Accessibility Issues ; ACM: New York, NY, USA, 2020. https://doi.org/10.1145/3389189.3398009 . Kreimeier, J.; Ullmann, D.; Kipke, H.; Götzelmann, T. Initial Evaluation  of Different Types of Virtual Reality Locomotion Towards a Pedestrian  Simulator for Urban and Transportation Planning. In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems (SIGCHI’20) ; CHI EA ’20; Association for Computing Machinery: Honolulu, HI, USA, 2020; pp 1–6. https://doi.org/10.1145/3334480.3382958 . Kreimeier, J.; Götzelmann, T. Two Decades of Touchable and Walkable  Virtual Reality for Blind and Visually Impaired People: A High-Level  Taxonomy. Multimodal Technologies and Interaction 2020, 4 (4), 79. https://doi.org/10.3390/mti4040079 . Ullmann, D.; Kreimeier, J.; Götzelmann, T.; Kipke, H. BikeVR: A Virtual  Reality Bicycle Simulator towards Sustainable Urban Space and Traffic  Planning. In Proceedings of Mensch und Computer ; MuC ’20; Association for Computing Machinery: New York, NY, USA, 2020; pp 511–514. https://doi.org/10.1145/3404983.3410417 . Kreimeier, J.; Karg, P.; Götzelmann, T. Tabletop Virtual Haptics: Feasibility Study for the Exploration of 2.5D Virtual Objects by Blind and Visually Impaired with Consumer Data Gloves. In Proceedings of the 13th ACM International Conference on PErvasive Technologies Related to Assistive Environments ; PETRA ’20; ACM: New York, NY, USA, 2020. https://doi.org/10.1145/3389189.3389194 . Kreimeier, J.; Karg, P.; Götzelmann, T. BlindWalkVR: Formative Insights into Blind and Visually Impaired People’s VR Locomotion Using Commercially Available Approaches. In Proceedings of the 13th ACM International Conference on PErvasive Technologies Related to Assistive Environments ; PETRA ’20; ACM: New York, NY, USA, 2020. https://doi.org/10.1145/3389189.3389193 . Götzelmann, T.; Kreimeier, J. Towards the Inclusion of Wheelchair Users in Smart City Planning through Virtual Reality Simulation. In Proc. 2nd International Workshop on Accessibility and the Smart City: Technological Challenges and Open Accessibility Issues ; ACM: New York, NY, USA, 2020. https://doi.org/10.1145/3389189.3398008 . Kreimeier, J.; Karg, P.; Götzelmann, T. BlindScanLine: Preliminary  Implementation and Evaluation of a Cross-Platform Line Scanning  Sonification Approach Comparing Frequency and Amplitude Modulation. In Proceedings of the 13th ACM International Conference on PErvasive Technologies Related to Assistive Environments ; PETRA ’20; ACM: New York, NY, USA, 2020. https://doi.org/10.1145/3389189.3393742 . 2019 Kreimeier, J.; Götzelmann, T. First Steps Towards Walk-In-Place  Locomotion and Haptic Feedback in Virtual Reality for Visually Impaired.  In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems (SIGCHI’19) ; ACM Press: Glasgow, Scotland Uk, 2019; pp 1–6. https://doi.org/10.1145/3290607.3312944 . Kreimeier, J.; Hammer, S.; Friedmann, D.; Karg, P.; Bühner, C.; Bankel, L.; Götzelmann, T. Evaluation of Different Types of Haptic Feedback Influencing the Task-Based Presence and Performance in Virtual Reality. In Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments ; ACM Press: Rhodes, Greece, 2019; pp 289–298. https://doi.org/10.1145/3316782.3321536 .   2018 Kreimeier, J.; Bielmeier, T.; Götzelmann, T. Evaluation of Capacitive  Markers Fabricated by 3D Printing, Laser Cutting and Prototyping. Inventions 2018, 3 (1), 9. https://doi.org/10.3390/inventions3010009 . Götzelmann, T. Autonomous Selection and Printing of 3D Models for People Who Are Blind. ACM Trans. Access. Comput. 2018, 11 (3), 14:1-14:25. https://doi.org/10.1145/3241066 . Kreimeier, J.; Götzelmann, T. Real World VR Proxies to Support Blind People in Mobility Training. In Gesellschaft für Informatik e.V. ; 2018. https://doi.org/10.18420/muc2018-demo-0484 . Kreimeier, J.; Götzelmann, T. FeelVR: Haptic Exploration of Virtual Objects. In Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference on\\xa0  - PETRA ’18 ; ACM Press: Corfu, Greece, 2018; pp 122–125. https://doi.org/10.1145/3197768.3201526 . Götzelmann, T. Visually Augmented Audio-Tactile Graphics for Visually Impaired People. ACM Trans. Access. Comput. 2018, 11 (2), 8:1-8:31. https://doi.org/10.1145/3186894 .   2017 Götzelmann, T. 3D-Druck für blinde Menschen: Vom statischen Druck zu interaktiven Objekten. Informatik Spektrum 2017, 40 (6), 511–515. https://doi.org/10.1007/s00287-017-1068-8 . Götzelmann, T.; Branz, L.; Heidenreich, C.; Otto, M. A Personal Computer-Based Approach for 3D Printing Accessible to Blind People. In Proceedings of the 10th International Conference on PErvasive Technologies Related to Assistive Environments - PETRA ’17 ; ACM Press: Island of Rhodes, Greece, 2017; pp 1–4. https://doi.org/10.1145/3056540.3064954 . Götzelmann, T. A 3D Printable Hand Exoskeleton for the Haptic Exploration of Virtual 3D Scenes. In Proceedings of the 10th International Conference on PErvasive Technologies Related to Assistive Environments\\xa0 - PETRA ’17 ; ACM Press: Island of Rhodes, Greece, 2017; pp 63–66. https://doi.org/10.1145/3056540.3064950 . Kröner, A.; Götzelmann, T. Fitts’ Gesetz als programmtechnische und  empirische Aufgabe für Studierende der Informatik; Gesellschaft für  Informatik, Bonn, 2017; Vol. Lecture Notes of Informatics (LNI), pp  295–305. https://doi.org/10.18420/IN2017_23 .   2016 Götzelmann, T. LucentMaps: 3D Printed Audiovisual Tactile Maps for Blind and Visually Impaired People. In Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility ; ASSETS ’16; Association for Computing Machinery: Reno, Nevada, USA, 2016; pp 81–90. https://doi.org/10.1145/2982142.2982163 . Götzelmann, T.; Schneider, D. CapCodes: Capacitive 3D Printable Identification and On-Screen Tracking for Tangible Interaction. In 9th Nordic Conference on Human-Computer Interaction (NordiCHI’16) ; ACM, 2016; pp 1–4. https://doi.org/10.1145/2971485.2971518 . Götzelmann, T.; Eichler, L. BlindWeb Maps – An Interactive Web Service for the Selection and Generation of Personalized Audio-Tactile Maps. In Computers Helping People with Special Needs: 15th International Conference, ICCHP 2016, Linz, Austria, July 13-15, 2016, Proceedings, Part II ; Miesenberger, K., Bühler, C., Penaz, P., Eds.; Springer International Publishing: Cham, 2016; pp 139–145. Götzelmann, T.; Althaus, C. TouchSurfaceModels: Capacitive Sensing Objects through 3D Printers. In 9th International Conference on PErvasive Technologies Related to Assistive Environments (PETRAE’16) ; ACM, 2016. https://doi.org/10.1145/2910674.2910690 . Götzelmann, T. CapMaps- Capacitive Sensing 3D Printed Audio-Tactile Maps. In Computers Helping People with Special Needs: 15th International Conference, ICCHP 2016, Linz, Austria, July 13-15, 2016, Proceedings, Part II ; Miesenberger, K., Bühler, C., Penaz, P., Eds.; Springer International Publishing: Cham, 2016; pp 146–152.   2015 Götzelmann, T.; Winkler, K. SmartTactMaps: A Smartphone-Based Approach to Support Blind Persons in Exploring Tactile Maps. In Proceedings of the 8th International Conference on PErvasive Technologies Related to Assistive Environments (PETRAE’15) ; ACM, 2015; p Article No. 2, 1-8. https://doi.org/10.1145/2769493.2769497 . Götzelmann, T.; Vázquez, P.-P. InclineType: An Accelerometer-Based Typing Approach for Smartwatches. In Proceedings of the 16th International Conference on Human Computer Interaction ; ACM Press, 2015; p 59:1-59:4. https://doi.org/10.1145/2829875.2829929 .   2014 Götzelmann, T.; Pavkovic, A. Towards Automatically Generated Tactile Detail Maps by 3D Printers for Blind Persons. In Lecture Notes in Computer Science (Volume 8548): 14th International Conference on Computers Helping People with Special Needs (ICCHP’14) ; Springer, 2014; pp 1–7. https://doi.org/10.1007/978-3-319-08599-9_1 . Dotenco, S.; Götzelmann, T.; Gallwitz, F. Smartphone Input Using Its Integrated Projector and Built-In Camera. In Proceedings of 16th International Conference on Human-Computer Interaction (HCII’14) ; Springer, 2014; pp 124–133. https://doi.org/10.1007/978-3-319-07227-2_13 . Schäff, C.; Pugliese, G.; Götzelmann, T. Behavior Based Web User Identification. Lecture Notes in Informatics (LNI) 2014, Volume Seminars (13), 201–204. Götzelmann, T. Interactive Tactile Maps for Blind People Using Smartphones’ Integrated Cameras. In Proceedings of the Ninth ACM International Conference on Interactive Tabletops and Surfaces (ITS’14) ; ACM: Dresden, Germany, 2014; pp 381–385. https://doi.org/10.1145/2669485.2669550 .            Ältere Publikationen (peer-reviewed)         Götzelmann, T. Concept of the Joint Use of Smartphone Camera and Projector for Keyboard Inputs. In 3rd International Symposium on Computing in Science & Engineering (ISCSE 2013) ; Gediz, Turkey, 2013; pp 52–57. Götzelmann, T.; Katzer, J. Challenges and Perspectives for True–3D in Car Navigation. In True 3D in Cartography - Autostereoscopic and Solid Visualization of Geodata (Lecture Notes in Geoinformation and Cartography) ; Buchroithner, M., Ed.; Lecture Notes in Geoinformation and Cartography; Springer Berlin Heidelberg: Berlin, Heidelberg, 2012; pp 357–366. https://doi.org/10.1007/978-3-642-12272-9_25 . Götzelmann, T. Towards Non-Photorealistic Rendering in Car Navigation. Journal of Computer Engineering & Information Technology 2012, 1 (1). https://doi.org/10.4172/2324-9307.1000e104 . Guercke, R.; Götzelmann, T.; Brenner, C.; Sester, M. Aggregation of LoD 1 Building Models as an Optimization Problem. ISPRS Journal of Photogrammetry and Remote Sensing  2011 , 66 (2), 209–222. https://doi.org/10.1016/j.isprsjprs.2010.10.006 . Götzelmann, T.; Guercke, R.; Brenner, C.; Sester, M. Terrain-Dependent Aggregation of 3D City Models. In International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, Volume XXXVIII-2/W11 ; 2009; p 6. Vázquez, P.-P.; Götzelmann, T.; Hartmann, K.; Nürnberger, A. An Interactive 3D Framework for Anatomical Education. Int J CARS 2008, 3 (6), 511–524. https://doi.org/10.1007/s11548-008-0251-4 . Götzelmann, T.; Vázquez, P.-P.; Hartmann, K.; Germer, T.; Nürnberger, A.; Strothotte, T. Mutual Text-Image Queries. In ACM SIGGRAPH Proceedings of the 23rd Spring Conference on Computer Graphics ; ACM, 2007; pp 139–146. https://doi.org/10.1145/2614348.2614368 . Götzelmann, T.; Vázquez, P.-P.; Hartmann, K.; Nürnberger, A.; Strothotte, T. Correlating Text and Images: Concept and Evaluation. In Smart Graphics ; Butz, A., Fisher, B., Krüger, A., Olivier, P., Owada, S., Eds.; Lecture Notes in Computer Science; Springer Berlin Heidelberg: Berlin, Heidelberg, 2007; Vol. 4569, pp 97–109. https://doi.org/10.1007/978-3-540-73214-3_9 Götzelmann, T.; Hartmann, K.; Strothotte, T. Annotation of Animated 3D Objects. In Proceedings of Simulation and Visualization ; 2007; Vol. 7, pp 209–222. Götzelmann, T.; Hartmann, K.; Nürnberger, A.; Strothotte, T. 3D Spatial Data Mining on Document Sets for the Discovery of Failure Causes in Complex Technical Devices. In Proceedings of the Second International Conference on Computer Graphics Theory and Applications ; SciTePress - Science and and Technology Publications: Barcelona, Spain, 2007; pp 137–145. https://doi.org/10.5220/0002082101370145 . Götzelmann, T.; Götze, M.; Ali, K.; Hartmann, K.; Strothotte, T. Annotating Images through Adaptation: An Integrated Text Authoring and Illustration Framework. Journal of WSCG 2007, 15 (1–3), 115–122. Götzelmann, T. Correlating Illustrations and Text through Interactive Annotation ; Ph.D. thesis. VDM Verlag: Saarbrücken, Germany, 2007. Hartmann, K.; Götzelmann, T.; Ali, K.; Strothotte, T. Mapped Pictures: Aesthetic Image Annotations. In Dagstuhl Seminar Proceedings - Abstracts Collection ; 2006. Götzelmann, T.; Hartmann, K.; Strothotte, T. Contextual Grouping of Labels. In Proceedings of Simulation and Visualization ; 2006; pp 245–258. Götzelmann, T.; Hartmann, K.; Strothotte, T. Agent-Based Annotation of Interactive 3D Visualizations. In Smart Graphics ; Butz, A., Fisher, B., Krüger, A., Olivier, P., Eds.; Lecture Notes in Computer Science; Springer: Berlin, Heidelberg, 2006; pp 24–35. https://doi.org/10.1007/11795018_3 . Götzelmann, T.; Götze, M.; Ali, K.; Hartmann, K.; Strothotte, T. Practical Illustration of Text: Customized Search, View Selection, and Annotation. In Mensch und Computer 2006 ; Heinecke, H. M., Paul, H., Eds.; Oldenbourg Wissenschaftsverlag: München, 2006. https://doi.org/10.1524/9783486841749.437 . Germer, T.; Götzelmann, T.; Spindler, M.; Strothotte, T. SpringLens – Distributed Nonlinear Magniﬁcations. In Proceedings of the 26th International Conference on Eurographics ; Eurographics Association, 2006; pp 123–126. Hartmann, K.; Götzelmann, T.; Ali, K.; Strothotte, T. Metrics for Functional and Aesthetic Label Layouts. In Smart Graphics ; Butz, A., Fisher, B., Krüger, A., Olivier, P., Eds.; Hutchison, D., Kanade, T., Kittler, J., Kleinberg, J. M., Mattern, F., Mitchell, J. C., Naor, M., Nierstrasz, O., Pandu Rangan, C., Steffen, B., Sudan, M., Terzopoulos, D., Tygar, D., Vardi, M. Y., Weikum, G., Series Eds.; Lecture Notes in Computer Science; Springer Berlin Heidelberg: Berlin, Heidelberg, 2005; Vol. 3638, pp 115–126. https://doi.org/10.1007/11536482_10 . Götzelmann, T.; Ali, K.; Hartmann, K.; Strothotte, T. Adaptive Labeling for Illustrations. In Proceedings of 13th Pacific Graphics Conference ; 2005; pp 64–66. Götzelmann, T.; Ali, K.; Hartmann, K.; Strothotte, T. Form Follows Function: Aesthetic Interactive Labels; The Eurographics Association, 2005; p 8. http://dx.doi.org/10.2312/COMPAESTH/COMPAESTH05/193-200 .           Weitere Publikationen (ohne peer-review)         Götzelmann, T. 3D-Druck Mit Löslichen Stützstrukturen Für Blinde Menschen. Berichte der Lehrforschung. TH Nürnberg Schriftenreihe 2016 . Nürnberg 2016, Issue 24, S. 48-54. Götzelmann, T. Taktile Karten Für Blinde Menschen Durch 3D-Drucker. Berichte der Lehrforschung. TH Nürnberg Schriftenreihe 2016 . Nürnberg 2015, Issue 23, S. 1-6. Götzelmann, T. Supporting Mobility and Navigation Abilities in Children and Young People with Visual Impairment. Mary Kitzinger Trust Newsletter . Nürnberg 2014, Issue 2 (November 2014). Götzelmann, T. BMBF Verbundvorhaben Intelligentes Cockpit (InteCo) ; Nationales Luftfahrtforschungsprogramm; Final report FKZ 20H0804B; 2011. Götzelmann, T.; Vázquez, P.-P.; Germer, T.; Hartmann, K.; Nürnberger, A.; Strothotte, T. Mutual Text Image Queries ; Technical Report 1/2007; Department of Computer Science, Otto-von-Guericke University of Magdeburg, 2007. Götzelmann, T.; Hartmann, K.; Strothotte, T. Labeling Agents ; Technical Report 11/2005; Department of Computer Science, Otto-von-Guericke University of Magdeburg, 2005.              \",\n",
       " '       Funktionsträger der Fakultät Informatik     Allgemeinwissenschaftliche Wahlpflichtfächer  Prof. Dr. B. Hauer - Fak. AMP    Dekan  Prof. Dr. T. Voit    Bibliotheksangelegenheiten  Prof. Dr. R. Weber    Fachwissenschaftliche Wahlpflichtfächer  Prof. Dr. M. Teßmann    FB-Informationssystem  Dipl. Inf. (FH) R. Ochsenkühn    Fakultätsreferentin  Dipl.-Betriebswirtin (FH) C. Theelke Lisa Betz    Frauenbeauftragte Stv. Frauenbeauftragte  Prof. Dr. P. Brockmann Prof. Dr. H. Schuhbauer    Haushalt  Prof. Dr. H. Schuhbauer Dipl.-Betriebswirtin (FH) C. Theelke    Hochschulkooperationen und Auslandsbeziehungen  Prof. Dr. H. Löhr Prof. Dr. R. Petrlic    IN-Homepage Support  Dipl.-Ing. (FH) T. Ulrich    Unternehmenskontakte  Prof. Dr. M. Lang    Labororganisation  Prof. Dr. Wienkop / Prof. Dr. Zapf / Prof. Dr. Kröner / Prof. Dr. Gallwitz    Linux Support  Dipl.- Math. Th. Strickroth    Moodle  M. Spiegel    Öffentlichkeitsarbeit  Prof. Dr. R. Weber    Praxissemester  Prof. Dr. W. Bremer    Prodekan  Prof. Dr. J. Albrecht    Prüfungskommission Informatik  Prof. Dr. P. Trommler    Prüfungspläne  Prof. Dr. F. Stappert    Sekretariat  M. Otsa, Margot Knoll    Stipendien  Prof. Dr. P. Trommler    Studienberatung - Bachelor Informatik - Bachelor Wirtschaftsinformatik - Bachelor Medieninformatik - Master Informatik - Master Wirtschaftsinformatik - Master Medieninformatik  Prof. Dr. C. Schiedermeier Prof. Dr. Rainer Groß Prof. Dr. C. Schiedermeier Prof. Dr. J. Roth Prof. Dr. P. Brockmann Prof. Dr. F. Gallwitz    Studienbüro - BA Informatik + MA IN/MIN/WIN - BA Wirtschaftsinformatik + MIN  G. Bauer K. Lang    Studienzuschüsse  Prof. Dr. H. Schuhbauer    Studiendekan  Prof. Dr. K. Riedhammer    Stundenpläne  Prof. Dr. F. Stappert    Webredakteur / Webmaster  Dipl.-Ing. (FH) T. Ulrich    Windows Support  T. Karanatsios Dipl.-Ing. (FH) R. Fischer N. Tschinkel          ']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"Gallwitz\"\n",
    "\n",
    "[text for text in df[\"text\"] if word in text][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings berechnen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Einleitung zu BERT Tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt werden wir für jedes Dokument ein eigenes Word embeddings erstellen. Dazu müssen wir zunächst das BERT Model laden. <br>\n",
    "Das BERT Model ist ein von Google trainierter Encoder, welcher ursprünglich dafür Entwickelt wurde, dass er maskierte Wörter erraten kann (Masked Language Modelling) oder Vorhersagen kann, ob ein Satz auf einen anderen Satz folgt.(Next Sentence Prediction)\n",
    "- es ist trainiert auf 10.000+ Büchern\n",
    "- es gibt Modelle \"base\" und \"large\"\n",
    "uncased heißt ohne klein - Großschreibung\n",
    "\n",
    "Wir brauchen zur Vorbereitung die zusätzlichen Token\n",
    "- [SEP] um das Ende eines Satzes zu markieren\n",
    "- [CLS] am Anfang des Texten\n",
    "- [PAD] zum auffüllen der Token \n",
    "Außerdem\n",
    "TokenIDs\n",
    "MaskIDs - zum filtern der [PAD]\n",
    "Segment IDs um verschiedene Sätze zu unterscheiden\n",
    "Posititional Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Laden des BERT Modells und des BERT tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO try better model\n",
    "# BertModel.from_pretrained('bert-base-german-cased',output_hidden_states = True) \n",
    "model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True) \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testen des Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSentence = \"In der Bibliothek gibt es 40 Bücher zum Thema Animes\"\n",
    "tokens_question = tokenizer.tokenize(testSentence)\n",
    "print(*tokens_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokens im dataframe speichern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun erstellen wir eine neue Spalte [\"tokens\"], in der wir für jedes Dokument die Tokens abspeichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] = [tokenizer.tokenize(text) for text in tqdm(df[\"text\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der dataframe hat nun eine Spalte mehr und wir können uns ein Beispiel der Tokens ansehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "print(df[\"tokens\"][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokens in IDs umwandeln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Tokens müssen nun in IDs umgewandelt werden, damit sie das BERT Model für die Erstellung eines Embedding Vectors benutzen kann. Dafür benutzen wir eine Funktion des Tokenizers ```tokenizer.convert_tokens_to_ids()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"token_ids\"] = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tqdm(df[\"tokens\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_symbols = [\"[CLS]\", \"[SEP]\", \"[PAD]\"]\n",
    "print(tokenizer.convert_tokens_to_ids(special_symbols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokens splitten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben jetzt also die Tokens IDs für unsere 2400 verschiedenen Dokumente gebildet. Der nächste Schritt wäre nun, diese Token IDs dem BERT Model zu übergeben, sodass es uns ein Embedding daraus errechnet. Leider kann das BERT Model nur 512 Tokens (~1300 Zeichen) als Input nehmen. Die meißten der gescrapeten Webseiten sind aber wesentlich länger. <br> \n",
    "Der Naheliegendste Ansatz ist dabei, die Tokens einfach in 510 token große Chunks aufzusplitten (Wir brauchen noch 2 Tokens extra für jeden Chunk) und für jeden Chunk ein extra Embedding zu erstellen. <br> \n",
    "Dabei gibt es entweder die Möglichkeit die Chunks überlappend, oder einfach hard cut zu gestalten. <br>\n",
    "\n",
    "Wir werden hier zunächst den hard cut Ansatz verfolgen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_text_and_tokens(row):\n",
    "    text = row['text']\n",
    "    tokens_ids = row['token_ids']\n",
    "    filename = row['filename']\n",
    "\n",
    "    if len(tokens_ids) > 510:\n",
    "        for i in range(0, len(tokens_ids), 510):\n",
    "            chunk_tokens = tokens_ids[i:i + 510]\n",
    "            # adding the [CLS] and the [SEP] token\n",
    "            chunk_tokens = [101] + chunk_tokens + [102]\n",
    "            chunk_text = tokenizer.decode(chunk_tokens)\n",
    "\n",
    "            new_row = {'filename': filename, 'chunk_id': i/510, 'chunk_text': chunk_text, 'chunk_tokens_json': json.dumps(chunk_tokens)}\n",
    "            new_rows.append(new_row)\n",
    "    else:\n",
    "        # adding the [CLS] and the [SEP] token\n",
    "        tokens_ids = [101] + tokens_ids + [102]\n",
    "        tokens_ids = tokens_ids + [0] * (512 -len(tokens_ids))\n",
    "        text = \"[CLS]\" + text + \"[SEP]\"\n",
    "        new_row = {'filename': filename, 'chunk_id': 0, 'chunk_text': text, 'chunk_tokens_json': json.dumps(tokens_ids) }\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "new_rows = []\n",
    "df.apply(split_text_and_tokens, axis=1)\n",
    "chunk_df = pd.DataFrame(new_rows)\n",
    "chunk_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns an dieser Stelle die gesplitteten chunks anschauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunk_df.sample(2).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ab jetzt werden wir mit dem neuen chunk_df weiterarbeiten. Nun erstellen wir für jeden Chunk ein eigenes Embedding, welches dann die Semantik dieses chunks enthalten soll. Dafür müssen wir uns nun das BERT Model etwas genauer anschauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proccessSentence(tokens, model, tokenizer):\n",
    "    if len(tokens) == 0:\n",
    "        return torch.zeros(768)\n",
    "\n",
    "    tokens = [\"CLS\"] + tokens + [\"SEP\"]\n",
    "\n",
    "    attention_mask = [1 if token != \"[PAD]\" else 0  for token in tokens]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    token_ids_tensor = torch.tensor([token_ids], dtype=torch.int64)\n",
    "    attetion_mask_tensor = torch.tensor([attention_mask], dtype=torch.int64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(token_ids_tensor, attetion_mask_tensor)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # stack the layer list \n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    # remove the batches dim\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    # average all token embeds\n",
    "    layer_vecs = torch.mean(token_embeddings, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate the average of layer 3 to 13\n",
    "    embed = torch.mean(layer_vecs[2:], dim=0)\n",
    "\n",
    "\n",
    "    return embed\n",
    "\n",
    "df[\"chunk_embeddings\"] = [proccessSentence(tokens).tolist() for tokens in tqdm(df[\"tokens\"])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diser Code kann sehr lange brauchen um die Embeddings zu berechnen. Der Code An dieser Stelle sollte man dann die Embeddings am besten abspeichern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "db_save_df(df, \"chunk_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"chunk_embeddings\", [\"*\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_t_sne(embed_id):\n",
    "    df_embed=[json.loads(embedding) for embedding in tqdm(df[f\"chunk_embeddings_{embed_id}\"])]\n",
    "    word_embeddings = np.array(df_embed)\n",
    "    tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "    X_embedded = tsne.fit_transform(word_embeddings)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X_embedded[:, 0], X_embedded[:, 1], s=5)\n",
    "    plt.title(f\"t-SNE Visualization of Word Embeddings {embed_id}\")\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    embed_t_sne(i +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Extraktion von Word Embeddings für die Frage\n",
    "def get_word_embedding(question, model_name='bert-base-uncased'):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    tokens = tokenizer(question, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        question_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return question_embedding\n",
    "\n",
    "# Laden der Word Embeddings\n",
    "df = db_get_df(\"chunk_embeddings\", [\"chunk_embeddings_2\"])\n",
    "df = [json.loads(embedding) for embedding in tqdm(df[\"chunk_embeddings_2\"])]\n",
    "word_embeddings = np.array(df)\n",
    "\n",
    "# Extrahieren der Embeddings für die Frage\n",
    "question_text = \"n?\"\n",
    "question_embedding = get_word_embedding(question_text)\n",
    "\n",
    "# Berechnen der Kosinus-Ähnlichkeit zwischen der Frage und den anderen Word Embeddings\n",
    "similarities = cosine_similarity(word_embeddings, [question_embedding])\n",
    "\n",
    "# 'similarities' ist jetzt ein Array mit den Kosinus-Ähnlichkeiten zwischen der Frage und den anderen Word Embeddings.\n",
    "\n",
    "# Kombinieren Sie die t-SNE-Komponenten mit den Kosinus-Ähnlichkeiten\n",
    "combined_features = np.column_stack((X_embedded, similarities))\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(combined_features[:, 0], combined_features[:, 1], s=5)\n",
    "plt.scatter(question_embedding[0], question_embedding[1], color='red', s=50, label='Ihre Frage')\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings with Cosine Similarity\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annahme: X_embedded ist Ihre t-SNE-Visualisierung\n",
    "# Annahme: word_embeddings ist Ihre Matrix der Word Embeddings\n",
    "# Annahme: question_text ist Ihre Frage\n",
    "# Annahme: n_clusters ist die Anzahl der gewünschten Cluster\n",
    "question_text = \"Welche Kompetenzen hat Pr. Gallwitz?\" #wann ist der Bewerbungszeitraum  Für das Wintersemester\n",
    "\n",
    "# Schritt 1: Clustering durchführen\n",
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "cluster_labels = kmeans.fit_predict(word_embeddings)\n",
    "\n",
    "# Schritt 3: Berechnen der Ähnlichkeit zur Frage\n",
    "question_embedding = get_word_embedding(question_text)  # Verwenden Sie Ihre get_word_embedding Funktion\n",
    "similarities = cosine_similarity(word_embeddings, [question_embedding])\n",
    "\n",
    "# Schritt 4: Visualisierung aktualisieren\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(5):\n",
    "    plt.scatter(X_embedded[cluster_labels == i, 0], X_embedded[cluster_labels == i, 1], s=5, label=f'Cluster {i}')\n",
    "\n",
    "# Farben entsprechend des Clusters für die Frage aktualisieren\n",
    "question_cluster = np.argmax(similarities)\n",
    "plt.scatter(X_embedded[question_cluster, 0], X_embedded[question_cluster, 1], s=50, color='red', label='Ihre Frage')\n",
    "\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings with Clusters\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans = KMeans(n_clusters=5)  # Specify the number of clusters you want\n",
    "cluster_labels = kmeans.fit_predict(word_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(len(np.unique(cluster_labels))):\n",
    "    plt.scatter(X_embedded[cluster_labels == i, 0], X_embedded[cluster_labels == i, 1], s=5, label=f'Cluster {i}')\n",
    "\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings with Clusters\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the indices of data points in Cluster 0\n",
    "cluster0_indices = np.where(cluster_labels == 1)\n",
    "\n",
    "# Get the corresponding rows from the DataFrame 'df'\n",
    "cluster0_data_rows = df.iloc[cluster0_indices]\n",
    "\n",
    "# Print the 'text' column for the data points in Cluster 0\n",
    "for text in cluster0_data_rows['text']:\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Annahme: Ihr DataFrame 'df' enthält eine Spalte 'text' mit den Textdaten.\n",
    "\n",
    "# Anzahl der Cluster (angenommen, es sind 5 Cluster)\n",
    "num_clusters = 5\n",
    "\n",
    "for cluster_id in range(num_clusters):\n",
    "    # Filtern Sie die Zeilen für den aktuellen Cluster\n",
    "    cluster_data_rows = df\n",
    "\n",
    "    # Laden des spaCy-Modells für die Textverarbeitung\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    # Benutzerdefinierte Stoppwortliste\n",
    "    stopwords = {'www', 'th-nuernberg', 'nürnberg', 'nuernberg', 'th', 'technische', 'hochschule', 'ohm', 'de', 'punkt', 'simon'}\n",
    "\n",
    "    # Tokenisieren und Lemmatisieren der Texte, Entfernen der Stoppwörter und Konvertieren in Strings\n",
    "    processed_texts = []\n",
    "    for text in cluster_data_rows['text']:\n",
    "        doc = nlp(text)\n",
    "        processed_tokens = []\n",
    "        for token in doc:\n",
    "            if token.text.lower() not in stopwords and token.pos_ in {'NOUN', 'PROPN'}:\n",
    "                processed_tokens.append(token.text)\n",
    "        processed_texts.append(' '.join(processed_tokens))\n",
    "\n",
    "    # Erstellen eines Wörterbuchs und einer Textkorpus für das LDA-Modell\n",
    "    text_tokens = [text.split() for text in processed_texts]\n",
    "    dictionary = gensim.corpora.Dictionary(text_tokens)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in text_tokens]\n",
    "\n",
    "    # Anwendung des LDA-Modells\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "    # Anzeigen der Hauptthemen für den aktuellen Cluster\n",
    "    print(f\"Cluster {cluster_id} Topics:\")\n",
    "    for topic_id, topic in lda_model.print_topics():\n",
    "        print(f\"Topic {topic_id}: {topic}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_text = ' '.join(processed_texts)  # 'processed_texts' ist die Liste der bereinigten Texte\n",
    "\n",
    "# Erstellen der Word Cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "\n",
    "# Anzeigen der Word Cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Erstellen einer Liste von Stoppwörtern, einschließlich der URL und der benutzerdefinierten Wörter\n",
    "stopwords = set(['www', 'th-nuernberg', 'nürnberg', 'nuernberg', 'th', 'technische', 'hochschule', 'ohm', 'de', 'punkt', 'simon','https','http','nuremberg','telefon','email','fax','Prof Dr','studium'])\n",
    "stopwords = set(word.lower() for word in stopwords)  # In Kleinbuchstaben umwandeln\n",
    "\n",
    "# Anzahl der Cluster (angenommen, es sind 5 Cluster)\n",
    "num_clusters = 5\n",
    "\n",
    "for cluster_id in range(num_clusters):\n",
    "    if cluster_id == 0:\n",
    "        continue\n",
    "    # Filter the indices of data points in the current cluster\n",
    "    cluster_indices = np.where(cluster_labels == cluster_id)\n",
    "\n",
    "    # Get the corresponding rows from the DataFrame 'df'\n",
    "    cluster_data_rows = df.iloc[cluster_indices]\n",
    "\n",
    "    # Extract and preprocess text data\n",
    "    texts = cluster_data_rows['text']\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "    processed_texts = [' '.join([token.text for token in nlp(text) if not token.is_stop and token.text.lower() not in stopwords]) for text in texts]\n",
    "\n",
    "    # Create a Word Cloud for the current cluster\n",
    "    all_text = ' '.join(processed_texts)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "\n",
    "    # Display the Word Cloud for the current cluster\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Word Cloud for Cluster {cluster_id}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "question=\"Welche Kompetenzen hat Prf. Gallwitz?\"\n",
    "# Assume you have a DataFrame df with a 'text' column that contains your documents\n",
    "# Also, assume you have a 'question' string for which you want to find relevant documents.\n",
    "\n",
    "# Create a TF-IDF vectorizer with the same parameters as before\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on your documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Transform the question string into TF-IDF representation\n",
    "question_tfidf = tfidf_vectorizer.transform([question])\n",
    "\n",
    "# Calculate cosine similarities between the question and all documents\n",
    "similarities = cosine_similarity(question_tfidf, tfidf_matrix)\n",
    "\n",
    "# Create a DataFrame to store similarities and document texts\n",
    "similarity_df = pd.DataFrame({\n",
    "    'Similarity': similarities[0],\n",
    "    'Text': df['text']\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by similarity in descending order\n",
    "sorted_similarity_df = similarity_df.sort_values(by='Similarity', ascending=False)\n",
    "\n",
    "# Print the top N most relevant documents (e.g., top 5)\n",
    "top_n = 5\n",
    "relevant_documents = sorted_similarity_df.head(top_n)\n",
    "\n",
    "# Print the relevant documents and their similarities to the question\n",
    "for index, row in relevant_documents.iterrows():\n",
    "    print(f\"Similarity: {row['Similarity']}\")\n",
    "    print(row['Text'])\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database = 'discord_bot/scrap/html.sqlite'\n",
    "\n",
    "# with sqlite3.connect(database) as con:\n",
    "#     html_df.to_sql('html_with_embeddings', con, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dokument--> bert anwenden für jeden dokument\n",
    "# question-bert anwenden\n",
    "question=\"was macht Gallwitz?\"\n",
    "document=df[\"text\"][1]\n",
    "\n",
    "tokens_question = tokenizer.tokenize(question)\n",
    "tokens_document = tokenizer.tokenize(document)\n",
    "attetion_mask_question = [1] * len(tokens_question)\n",
    "attention_mask_dokument = [1] * len(tokens_document)\n",
    "\n",
    "token_idss = tokenizer.convert_tokens_to_ids(tokens_question)\n",
    "tokenDocument_idss = tokenizer.convert_tokens_to_ids(tokens_document)\n",
    "\n",
    "\n",
    "tokens_tensor = torch.tensor([token_idss])\n",
    "segments_tensors = torch.tensor([attetion_mask_question])\n",
    "\n",
    "tokensDocument_tensor = torch.tensor([tokenDocument_idss])\n",
    "segmentsDocument_tensors = torch.tensor([attention_mask_dokument])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokensDocument_tensor, segmentsDocument_tensors)\n",
    "    hiddenDocuments_states = outputs[2]\n",
    "\n",
    "# print(token_idss)\n",
    "# print(tokenDocument_idss)\n",
    "\n",
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_vecs_sum = []\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding)\n",
    "\n",
    "\n",
    "tokenDocuments_vecs = hiddenDocuments_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentenceDocument_embedding = torch.mean(tokenDocuments_vecs, dim=0)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(sentence_embedding, sentenceDocument_embedding)\n",
    "\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Fachhochschulgesetz\"\n",
    "\n",
    "df.loc[df[\"text\"].str.contains(word)][\"text\"]\n",
    "# [text for text in df[\"text\"] if word in text][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"chunk_embeddings\")\n",
    "questions = [\"Was besagt das Fachhochschulgesetz?\", \n",
    "             \"Wo befindet sich die Mensa\", \n",
    "             \"Welche Professoren gibt es an der Technischen Hochschule Nürnberg?\",\n",
    "             \"Gib mir alle Infos zum Studienstart\",\n",
    "             \"Was gibt es Neues im Bezug auf Künstliche Intelligenz an der Hochschule?\",\n",
    "             \"Themen für eine Batchelorarbeit\",\n",
    "             \"Where is the Language office?\",\n",
    "             \"Where can i find the Mensa\",\n",
    "             \"Give me information on beginning of Semester\",\n",
    "             \"How many professors are there at the TH?\"]\n",
    "question = questions[random.randint(0,9)]\n",
    "question_embedding = question_embeddings(question)\n",
    "\n",
    "df[\"distance\"] = [1 - cosine(json.loads(embedding), question_embedding) for embedding in df[\"chunk_embeddings_2\"]]\n",
    "most_similar_documents = df.nsmallest(5, \"distance\")\n",
    "# print(f\"question embedding: {question_embedding[:10]}\")\n",
    "print(question)\n",
    "print(most_similar_documents[\"chunk_text\"].to_markdown())\n",
    "\n",
    "\n",
    "df[\"distance\"].plot(kind='hist', bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"word_embeddings\", [\"filename\", \"title\", \"text\", \"tokens\"])\n",
    "df[\"token_ids\"] = [tokenizer.convert_tokens_to_ids(json.loads(tokens)) for tokens in df[\"tokens\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splice dokuments in 512 token chunks\n",
    "\n",
    "# Initialize an empty list to store rows for the new DataFrame\n",
    "new_rows = []\n",
    "\n",
    "# Function to split text and tokens into chunks of 512 tokens\n",
    "def split_text_and_tokens(row):\n",
    "    text = row['text']\n",
    "    tokens_ids = row['token_ids']\n",
    "    filename = row['filename']\n",
    "\n",
    "    if len(tokens_ids) > 512:\n",
    "        # Split into multiple chunks\n",
    "        for i in range(0, len(tokens_ids), 512):\n",
    "            chunk_tokens = tokens_ids[i:i + 512]\n",
    "            chunk_text = tokenizer.decode(chunk_tokens)\n",
    "\n",
    "            # Create a new row with a reference to the original row\n",
    "            new_row = {'filename': filename, 'chunk_id': i/512, 'chunk_text': chunk_text, 'chunk_tokens_json': json.dumps(chunk_tokens)}\n",
    "            new_rows.append(new_row)\n",
    "    else:\n",
    "        # If the row has 512 tokens or fewer, keep it as is\n",
    "        new_row = {'filename': filename, 'chunk_id': 0, 'chunk_text': text, 'chunk_tokens_json': json.dumps(tokens_ids) }\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "# Apply the function to each row in the original DataFrame\n",
    "df.apply(split_text_and_tokens, axis=1)\n",
    "\n",
    "# Create a new DataFrame from the list of new rows\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Reset the index of the new DataFrame if needed\n",
    "new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(new_df.to_markdown())\n",
    "\n",
    "# tokenizer.convert_tokens_to_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus den 2433 Dokumenten die wir eigentlich gescraped haben, sind nun 6945 chunks entstanden es hat sich fast verdreifacht. Wenn man die 787 Seiten ohne Inhalt abzieht, hat sich die Anzahl von 1646 auf 6158 fast vervierfacht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"chunk_tokens_json\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(new_df, \"chunk_word_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "database_path = os.getenv(\"DATABASE_PATH\")\n",
    "\n",
    "def merge_db_tables():\n",
    "    # Connect to your SQLite database\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Create the new table using the structure of the first table (chunk_word_embeddings_0)\n",
    "    cursor.execute('''CREATE TABLE chunk_word_embeddings_all AS SELECT * FROM chunk_word_embeddings_0 WHERE 0''')\n",
    "    # Insert data from the other tables into the new table\n",
    "    for i in range(1, 8):\n",
    "        cursor.execute(f'INSERT INTO chunk_word_embeddings_all SELECT * FROM chunk_word_embeddings_{i}')\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "merge_db_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate embeddings with SentenceTransformer from huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### paraphrase-MiniLM-L6-v2 with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "df = db_get_df(\"html_attrs_de\", [\"filename\", \"title\", \"text\"])\n",
    "# Load spaCy for sentence segmentation\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "#TODO speichere embeddings in databank\n",
    "sentences = []\n",
    "embeddings = []\n",
    "\n",
    "for text in df['text']:\n",
    "    # Split the text into sentences using spaCy\n",
    "    doc = nlp(text)\n",
    "    sentence_list = [sent.text for sent in doc.sents]\n",
    "    sentences.append(sentence_list)\n",
    "    \n",
    "    # Encode each sentence using Sentence Transformers\n",
    "    sentence_embeddings = [model.encode(sentence) for sentence in sentence_list]\n",
    "    embeddings.append(sentence_embeddings)\n",
    "    print(embeddings)\n",
    "\n",
    "# Now, 'sentences' is a list where each element is a list of sentences, and 'embeddings' is a list of corresponding sentence embeddings.\n",
    "# filenames = df.head(3)\n",
    "# print(filenames.to_markdown())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### embeddings with sentence-transformers/all-MiniLM-L6-v2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the pre-trained MiniLM model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Create a list to store the embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Assuming 'text' column holds your HTML content\n",
    "for text in df['text']:\n",
    "    # Encode the text and append the resulting embedding to the list\n",
    "    text_embedding = model.encode(text)\n",
    "    embeddings.append(text_embedding)\n",
    "\n",
    "# Create a new DataFrame to store the embeddings\n",
    "embeddings_df1 = pd.DataFrame(embeddings)\n",
    "\n",
    "# Now 'embeddings_df' contains the embeddings for each text in your DataFrame\n",
    "db_save_df(embeddings_df1, \"embeddings1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### embeddings with 'sentence-transformers/msmarco-distilbert-base-tas-b' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the pre-trained Multi-QA MiniLM model\n",
    "model = SentenceTransformer('sentence-transformers/msmarco-distilbert-base-tas-b')\n",
    "\n",
    "# Create a list to store the embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Assuming 'text' column holds your HTML content\n",
    "for text in tqdm(df['text'], desc=\"Encoding Texts\"):\n",
    "    # Encode the text and append the resulting embedding to the list\n",
    "    text_embedding = model.encode(text)\n",
    "    embeddings.append(text_embedding)\n",
    "\n",
    "# Create a new DataFrame to store the embeddings\n",
    "embeddings_df3 = pd.DataFrame(embeddings)\n",
    "db_save_df(embeddings_df3, \"embeddings3\")\n",
    "\n",
    "# Now 'embeddings_df' contains the embeddings for each text in your DataFrame using the multi-qa model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### embeddings with 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the pre-trained Multi-QA MiniLM model\n",
    "model = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Create a list to store the embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Assuming 'text' column holds your HTML content\n",
    "for text in tqdm(df['text'], desc=\"Encoding Texts\"):\n",
    "    # Encode the text and append the resulting embedding to the list\n",
    "    text_embedding = model.encode(text)\n",
    "    embeddings.append(text_embedding)\n",
    "\n",
    "# Create a new DataFrame to store the embeddings\n",
    "embeddings_df4 = pd.DataFrame(embeddings)\n",
    "db_save_df(embeddings_df4, \"embeddings4\")\n",
    "\n",
    "# Now 'embeddings_df' contains the embeddings for each text in your DataFrame using the multi-qa model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate embeddings with 'paraphrase-MiniLM-L6-v2' model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir laden das Modell \"paraphrase-MiniLM-L6-v2\" von HuggingFace herunter. Daraus werden Embeddings für die jeweiligen TH-Webseiten berechnet. Das Modell erstellt Embeddings der Größe 384 Floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = db_get_df(\"html_attrs\", [\"filename\", \"title\", \"text\"])\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "text_embeddings = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating embeddings\"):\n",
    "    text_embedding = model.encode(row['text'])\n",
    "    text_embeddings.append({'filename': row['filename'],\n",
    "                            'title': row['title'],\n",
    "                            'text': row['text'],\n",
    "                            'text_embedding': text_embedding})\n",
    "\n",
    "embeddings_df = pd.DataFrame(text_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sqlite kann keine Listen speichern, daher werde hier json Objekte erstellt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df[\"text_embedding_json\"] = [json.dumps(text_embedding.tolist()) for text_embedding in embeddings_df[\"text_embedding\"]]\n",
    "db_save_df(embeddings_df[[\"filename\",\"title\",\"text\",\"text_embedding_json\" ]], \"embeddings_paraphrase_MiniLM_L6_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarities(query,model,embeddings_df):\n",
    "    query_embedding = model.encode(query, show_progress_bar=True)\n",
    "    similarities = []\n",
    "\n",
    "    for index, row in tqdm(embeddings_df.iterrows(), total=len(embeddings_df), desc=\"Calculating similarities\"):\n",
    "        \n",
    "        similarity = cosine_similarity([query_embedding], [row[\"text_embedding\"]])[0][0]\n",
    "\n",
    "        similarities.append({'filename': row['filename'],\n",
    "                            'title': row['title'],\n",
    "                            'text': row['text'], \n",
    "                            'similarity': similarity})\n",
    "\n",
    "    similarities_df = pd.DataFrame(similarities)\n",
    "    return similarities_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarities(similarities_df):\n",
    "    similarities_df['similarity'].plot(kind='hist', bins=200)\n",
    "    plt.xlabel('Similarity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Similarity Distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir schauen uns die similarities für unterschiedliche Fragen an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "embeddings_df=db_get_df(\"embeddings_paraphrase_MiniLM_L6_v2\")\n",
    "# Convert back to lists.\n",
    "embeddings_df[\"text_embedding\"] = [json.loads(text_embedding_json) for text_embedding_json in embeddings_df[\"text_embedding_json\"]]\n",
    "queries = [\"Was besagt das Fachhochschulgesetz?\", \n",
    "             \"Wo befindet sich die Mensa\", \n",
    "             \"Welche Professoren gibt es an der Technischen Hochschule Nürnberg?\",\n",
    "             \"Gib mir alle Infos zum Studienstart\",\n",
    "             \"Was gibt es Neues im Bezug auf Künstliche Intelligenz an der Hochschule?\",\n",
    "             \"Themen für eine Batchelorarbeit\",\n",
    "             \"Where is the Language office?\",\n",
    "             \"Where can i find the Mensa\",\n",
    "             \"Give me information on beginning of Semester\",\n",
    "             \"How many professors are there at the TH?\"]\n",
    "\n",
    "most_similar_articles=\"\"\n",
    "\n",
    "for query in queries:\n",
    "    similarities_df=calculate_similarities(query,model,embeddings_df)\n",
    "    most_similar_articles = similarities_df.nlargest(5, 'similarity')['text']\n",
    "    print(\"most_similar_articles: \\n\"+most_similar_articles.to_markdown())\n",
    "    plot_similarities(similarities_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
