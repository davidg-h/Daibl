{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Die Webseite der TH Nürnberg Scrapen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einleitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Webseite der TH-Nürnberg wirkt als Ausgangspunkt für die Wissensgrundlage des Chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from \"webdriver-manager\" import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import glob\n",
    "import json\n",
    "import sys\n",
    "import tqdm\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausganslage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Ausganspunkt für unsere Daten nutzen wir die Home Webseite der TH-Nürnberg. (https://www.th-nuernberg.de/)\n",
    "Diese Website downloaden wir und suchen alle Links auf andere Webseiten und speichern diese Links in eine Liste.\n",
    "Als nächsten Schritt rufen wir alle Links aus dieser Liste Auf und sammeln wiederum alle Links von jeder dieser Webseiten.\n",
    "In der daraus resultierenden Liste sortieren wir alle Links aus, die nicht auf die Webseite der TH verweisen.\n",
    "Dann laden wir alle Dokumente herunter und speichern sie in der Datenbank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst laden Wir die Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links():\n",
    "    print(sys.path)\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get(\"https://www.google.com\")\n",
    "    links = driver.find_elements(By.CSS_SELECTOR, \"a\")\n",
    "    print(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_content(file):\n",
    "    soup = BeautifulSoup(file,\"lxml\")\n",
    "\n",
    "    filename = file.name\n",
    "    title = soup.find(\"title\")\n",
    "    if title:\n",
    "        title = title.text\n",
    "    else:\n",
    "        title = \"\"\n",
    "\n",
    "\n",
    "\n",
    "    main = soup.find(\"main\")\n",
    "    portal = soup.find(\"div\", {'class': 'portal'})\n",
    "    page_container = soup.find(\"div\", {'class': 'page-wrap'})\n",
    "    \n",
    "    visible_texts = \"\"\n",
    "    if main:\n",
    "        # print(\"main found\")\n",
    "        container = main.find(\"div\" ,{'class': 'container'}, recursive=False)\n",
    "        if container:\n",
    "            texts = container.find_all(text=True)\n",
    "            visible_texts = filter(tag_visible, texts)\n",
    "        # else:\n",
    "        #     print(\"no container\") \n",
    "\n",
    "    elif portal:\n",
    "        # print(f\"portal found {filename}\")\n",
    "        texts = portal.find_all(text=True)\n",
    "        visible_texts = filter(tag_visible, texts)\n",
    "        # print(u\" \".join(t.strip() for t in visible_texts))\n",
    "\n",
    "    elif page_container:\n",
    "        # print(f\"page_container found {filename}\")\n",
    "        container = page_container.find(\"div\" ,{'class': 'container'}, recursive=False)\n",
    "        # container = page_container.children[2]\n",
    "        if container:\n",
    "            texts = container.find_all(text=True)\n",
    "            visible_texts = filter(tag_visible, texts)\n",
    "        # else:\n",
    "        #     print(\"no container\") \n",
    "        # print(u\" \".join(t.strip() for t in visible_texts))\n",
    "\n",
    "    # else:\n",
    "    #     if \".pdf\" not in filename and \".xml\" not in filename:\n",
    "            # print(f\"nothing found {filename}\")\n",
    "\n",
    "        \n",
    "    # print(u\" \".join(t.strip() for t in visible_texts))\n",
    "\n",
    "    return dict(\n",
    "        title=title,\n",
    "        text=u\" \".join(t.strip() for t in visible_texts),\n",
    "        filename=filename\n",
    "    ) \n",
    "\n",
    "def create_video_parameters_json(path, outfile):\n",
    "    html_files = glob.glob(path)\n",
    "\n",
    "    # Ausgabe-Datei\n",
    "    filename = outfile\n",
    "    videosParameter = []\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for html_file in tqdm.tqdm(html_files):\n",
    "            \n",
    "            with open(html_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = get_content(file)\n",
    "                videosParameter.append(data)\n",
    "        if videosParameter:\n",
    "            # print(json.dumps(videosParameter))\n",
    "\n",
    "            outfile.write(json.dumps(videosParameter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_video_parameters_json(\"htmlfiles/*\", \"html_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "json_file = 'html_data.json'\n",
    "videos_df = pd.read_json(json_file)\n",
    "\n",
    "\n",
    "database = 'html.sqlite'\n",
    "\n",
    "with sqlite3.connect(database) as con:\n",
    "    videos_df.to_sql('html_attrs', con, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter only german artciles for better embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_init import db_get_df, db_save_df\n",
    "\n",
    "df = db_get_df(\"html_attrs\", [\"filename\", \"title\", \"text\"])\n",
    "\n",
    "df_en = df[df['filename'].str.contains('file_en')]\n",
    "df_de = df[~df['filename'].str.contains('file_en')]\n",
    "db_save_df(df_de,\"html_attrs_de\")\n",
    "db_save_df(df_en,\"html_attrs_en\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
