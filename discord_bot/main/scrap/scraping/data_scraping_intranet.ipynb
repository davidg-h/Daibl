{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>\n",
    "\n",
    "- [Die Webseite der TH Nürnberg- Intranet Scrapen](#toc1_)\n",
    "  - [Einleitung](#toc1_1_)\n",
    "\n",
    "- [Scrapen der home Seite](#toc1_2_)\n",
    "  - [Links Filtern](#toc1_3_)\n",
    "  - [Dublikate entfernen und sortieren](#toc1_4_)\n",
    "  - [Links abspeichern](#toc1_5_)\n",
    "  - [Downloaden der Files](#toc1_6_)\n",
    "  - [Weitere Iterationstufen](#toc1_7_)\n",
    "  - [Iteration](#toc1_8_)\n",
    "  - [Texte extrahieren](#toc1_9_)<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Die Webseite der TH Nürnberg- Intranet Scrapen](#toc_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Einleitung](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Webseite der TH-Nürnberg wirkt als Ausgangspunkt für die Wissensgrundlage des Chatbots.\n",
    "\n",
    "Conventions:\n",
    "\n",
    "- pandas Spaltennamen im Sigular\n",
    "- Die meißten Links sind keine URLs, da sie lokal sind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import pandas as pd\n",
    "from db_init import db_get_df, db_save_df\n",
    "import glob\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "import urllib3\n",
    "# Suppress only the single InsecureRequestWarning from urllib3 needed\n",
    "urllib3.disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Um diesen Notebook auszuführen, braucht man eine VPN Verbindung zur TH-Intranet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Scrapen der home Seite](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Ausganspunkt für unsere Daten nutzen wir die Home Webseite des TH-Nürnberg-Intranet. (https://intern.ohmportal.de/)\n",
    "Diese Website downloaden wir und suchen alle Links auf andere Webseiten und speichern diese Links in eine Liste.\n",
    "Als nächsten Schritt rufen wir alle Links aus dieser Liste auf und sammeln wiederum alle Links von jeder dieser Webseiten.\n",
    "In der daraus resultierenden Liste sortieren wir alle Links aus, die nicht auf die Webseite des TH-Intranet verweisen.\n",
    "Dann laden wir alle Dokumente herunter und speichern sie in der Datenbank.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Funktion, die eine URL als parameter nimmt und das HTML file zurückgibt, falls die Seite existiert.\n",
    "\n",
    "Dafür nutzen wir die requests Bibliothek.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da einige Seiten eine Umleitung auf die THN Webseite enthalten, überprüfen wir, ob die URL nach dem Umleiten noch zur ursprünglichen Domäne gehört.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hinweis** SSL Zertigikat prüfung wird ausgesetzt, da einige Seiten deshalb ein Fehler werfen.\n",
    "Bsp URL: \"https://intern.ohmportal.de/institutionen/fakultaeten/betriebswirtschaft/online-services/page.html\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_html_from_url(url):\n",
    "    \n",
    "    html = \"\"\n",
    "\n",
    "    try:   \n",
    "        res = requests.get(url,verify=False)\n",
    "    except Exception as error:\n",
    "        print(\"FEHLER:\" , error)\n",
    "        return html\n",
    "    \n",
    "    if res.status_code == 200:\n",
    "        if res.url.startswith(url):\n",
    "            html = res.text\n",
    "        else:\n",
    "            print(f\"Umleitung zu externer Seite verhindert. URL: {res.url}\")\n",
    "    else:\n",
    "        print(f\"Kein Inhalt heruntergeladen. Statuscode: {res.status_code}\")\n",
    "    \n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Funktion, die ein HTML file nach Links durchsucht und alle gefundenen externen und internen Links zurückgibt.\n",
    "Dafür nutzen wir die Bibliothek Beautifulsoup, mit dem lxml parser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_one_html(html):\n",
    "    soup = BeautifulSoup(html,\"lxml\")\n",
    "    links = [a[\"href\"] for a in soup.find_all('a', href=True)]\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt geben wir unsere initiale URL an und extrahieren alle Links aus dieser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://intern.ohmportal.de/\"\n",
    "html = download_html_from_url(BASE_URL)\n",
    "links = get_links_from_one_html(html)\n",
    "df = pd.DataFrame({\"link\": links})\n",
    "print(*df[\"link\"])\n",
    "print(len(df[\"link\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Links filtern](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst können wir alle Links überprüfen, ob sie Parameter oder sections mit\n",
    "\n",
    "- _?param1=hallo_\n",
    "- _#section_\n",
    "\n",
    "enthalten. Beide Attribute sind für den Download der Webseiten nicht notwendig und werden ausgefiltert. Dies spart uns HTML Duplikate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_links(links):\n",
    "    cleaned_links = [link.split('#')[0].split('?')[0] for link in links]\n",
    "    return cleaned_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun speichern wir das zwischen Ergebnis der gefilterten Links.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"link\"]=clean_links(df[\"link\"])\n",
    "print(*df[\"link\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun löschen wir alle Links, die kein Inhalt haben (leere Links).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_links(links):\n",
    "    cleaned_links = [link for link in links if link.strip()]\n",
    "    return cleaned_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"link\": remove_empty_links(df[\"link\"])})\n",
    "print(*df[\"link\"])\n",
    "print(len(df[\"link\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man sieht, dass wir 0 Links ausgefiltert haben, wenn man die Anzahl der Links vor und nach der Filterung vergleicht.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Externe Links finden- nur zur Visualisierung (optionale Funktion)](#toc0_)###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir Mal nachschauen, auf welche externen Seiten die Startseite des THN-Intranet verweist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_extern_urls(urls):\n",
    "    external_links = []\n",
    "    for link in urls:\n",
    "        if link.startswith(\"http\"):\n",
    "            external_links.append(link)\n",
    "\n",
    "    return external_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_links = find_extern_urls(df[\"link\"])\n",
    "print(\"Anzahl externer Links: \", len(external_links))\n",
    "print(external_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Seite verweist also auf elearning, auf jobboerse und ein Forum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_'></a>[Interne Links filtern](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir filtern nun noch alle Links heraus, die keine HTML-inhalte besitzen, wie z.B. pdf oder xml Dateien, die mail Links enthalten oder die nicht auf die THN Webseite referieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_intern_links(urls):\n",
    "    filtered_links = []\n",
    "    for url in urls:\n",
    "        if url.startswith(\"http\"):\n",
    "            continue\n",
    "        elif url.startswith(\"mailto:\"):\n",
    "            continue\n",
    "        elif url.startswith(\"javascript:\"):\n",
    "            continue\n",
    "        elif url.startswith(\"&#\"): # is encoded mailto\n",
    "            continue\n",
    "        elif \".xml\" in url:\n",
    "            continue\n",
    "        elif \".docx\" in url:\n",
    "            continue\n",
    "        elif \".pdf\" in url:\n",
    "            continue\n",
    "        elif url == \"/\":\n",
    "            continue\n",
    "        elif url == \"&\":\n",
    "            continue\n",
    "        else:\n",
    "            filtered_links.append(url)\n",
    "    return filtered_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intern_links = filter_intern_links(df[\"link\"])\n",
    "print(\"Anzahl interner Links: \", len(intern_links))\n",
    "print(intern_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun haben wir 4 Links entfernt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Dublikate entfernen und sortieren](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir die duplikate entfernen und anschließend alphabetisch sortieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_and_remove_dublicates(df):\n",
    "    if 'html' in df.columns:\n",
    "        df = df.sort_values(by='html', ascending=False)\n",
    "        df = df.drop_duplicates(subset='link', keep='first')\n",
    "        df = df.reset_index(drop=True)\n",
    "    else:\n",
    "        df = df.sort_values(by='link', ascending=False)\n",
    "        df = df.drop_duplicates(subset='link', keep='first')\n",
    "        df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intern_links = filter_intern_links(df[\"link\"])\n",
    "df = pd.DataFrame({\"link\": intern_links})\n",
    "df = sort_and_remove_dublicates(df)\n",
    "print(\"Anzahl interner Links (ohne Dublikate): \", len(df[\"link\"]))\n",
    "print(*df[\"link\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anhand der Anzahl der Links vor und nach der Duplikaten Entfernung, sieht man das 14 doppelt vorkommende Links entfernet werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Links abspeichern](#toc0_) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für unsere weiteren Schritte werden wir immer nur interne Links verwenden, deshalb speichern wir an dieser Stelle mal die internen Links ab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"only_links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[Downloaden der files](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir mit dem downloaden anfangen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Funktion lädt nun alle Html files zu den Links herunter und speichert sie im Dataframe neben den \"link\" in einer Spalte \"html\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all_urls(links):\n",
    "    htmls = []\n",
    "    for link in tqdm(links):\n",
    "        url = \"https://intern.ohmportal.de/\" + link\n",
    "        html = download_html_from_url(url)\n",
    "        htmls.append(html)\n",
    "    return htmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"html\"]=download_all_urls(df[\"link\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rows_with_empty_html(df):\n",
    "    df = df[df[\"html\"] != \"\"]  \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_rows_with_empty_html(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun sind 12 Reihen entfernt, die keinen HTML Inhalt hatten, da sie zum Beispiel auf eine andere Domäne verweisen wurden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können die Daten an dieser Stelle abspeichern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"intranet_html_iter_01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[Weitere Iterationsstufen](#toc0_) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir ab diesem Abschnitt starten können wir die vorher gesammelten Daten neu laden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"intranet_html_iter_01\")\n",
    "print(len(df[\"link\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_none_html_rows = df[df[\"html\"].notnull()]  # Filtere die Zeilen, in denen \"html\" nicht \"None\" ist\n",
    "print(len(non_none_html_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir die heruntergeladenen HTML files nach weiteren Links durchsuchen und Sie dem Dataframe hinzufügen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_links_in_html(htmls):\n",
    "    all_links = []\n",
    "    for html in tqdm(htmls):\n",
    "        links = get_links_from_one_html(html)\n",
    "        links= clean_links(links)\n",
    "        links= remove_empty_links(links)\n",
    "        links= filter_intern_links(links)\n",
    "        [all_links.append(link) for link in links]\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = find_all_links_in_html(df[\"html\"])\n",
    "len(all_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben jetzt also 979 Links gesammelt, davon sind aber viele Dublikate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun führen wir die neu gesammelten Links mit den ursprünglichen Links zusammen, wobei die neuen Links ein \"None\" Wert für die \"html\" Spalte bekommen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame({\"link\": all_links, \"html\": None})\n",
    "df = pd.concat([df, df_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sort_and_remove_dublicates(df)\n",
    "len(df[\"link\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gefiltert nach dublikaten haben wir nun also noch 285 Links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*df[\"link\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unique_link_endings(df):\n",
    "    endings = set() \n",
    "    \n",
    "    for link in df[\"link\"]:\n",
    "        parts = link.split(\"/\") \n",
    "        if len(parts) > 0:\n",
    "            ending = parts[-1]  \n",
    "            endings.add(ending)\n",
    "    \n",
    "    for ending in endings:\n",
    "        print(ending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_unique_link_endings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_none_html_rows = df[df[\"html\"].notnull()]  # Filtere die Zeilen, in denen \"html\" nicht \"None\" ist\n",
    "print(len(non_none_html_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den Daten nun weitere Webseiten hinzuzufügen, können wir für jede weitere URL schauen, ob sie schon heruntergeladen wurde. Wenn nicht, dann laden wir sie jetzt herunter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_df_with_html(df):\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        if pd.isna(row['html']) or row['html'] == '':\n",
    "            url = \"https://intern.ohmportal.de/\" + row[\"link\"]\n",
    "            html = download_html_from_url(url)\n",
    "            df.at[index, 'html'] = html\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = update_df_with_html(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun entfernen wir die leeren HTML Inhalte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=remove_rows_with_empty_html(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_none_html_rows = df[  df[\"html\"]==\"\" ]  # Filtere die Zeilen, in denen \"html\" nicht \"None\" ist\n",
    "print(len(non_none_html_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun speichern wir die 2. Iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"intranet_html_iter_02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"intranet_html_iter_02\")\n",
    "print(len(df[\"link\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_broken_link(df):\n",
    "    df = df[df['link'] != '/seitenbaum/studierende/einrichtungen-beratung/language-center/page.html']\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[Iterationen](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Iteration: Startseite TH-Intranet bzw. neue Links\n",
    "2. Iteration: Links von startseite aufrufen und HTML scrapen\n",
    "3. Iteration: Links von diesen Seiten aufrufen und HTML scrapen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration(df):\n",
    "    all_links = find_all_links_in_html(df[\"html\"])\n",
    "    df_new = pd.DataFrame({\"link\": all_links, \"html\": None})\n",
    "    df = pd.concat([df, df_new])\n",
    "    df = sort_and_remove_dublicates(df)\n",
    "    print(f\"got {len(df[df['html'].isna()])} new links\")\n",
    "    df=remove_broken_link(df)\n",
    "    df = update_df_with_html(df)\n",
    "    df=remove_rows_with_empty_html(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"intranet_html_iter_03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=iteration(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = find_all_links_in_html(df[\"html\"])\n",
    "df_new = pd.DataFrame({\"link\": all_links, \"html\": None})\n",
    "df = pd.concat([df, df_new])\n",
    "df = sort_and_remove_dublicates(df)\n",
    "print(f\"got {len(df[df['html'].isna()])} new links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=remove_broken_link(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = update_df_with_html(df)\n",
    "df=remove_rows_with_empty_html(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"intranet_html_iter_04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[Texte extrahieren](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes müssen wir aus den rohen HTML Dokumenten die unrelevanten Daten aussortieren, wie holen die letzten verfügbaren Iterations Daten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"intranet_html_iter_04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die nachfolgende Funktion bestimmt, ob ein Beautifulsoup geparstes HTML Element sichtbar ist oder nicht.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt bestimmen wir eine Funktion, die ein HTML als Input bekommt und daraus die Texte und Titeln bestimmt.\n",
    "\n",
    "Die HTML Seiten haben folgenden Strukturen:\n",
    "\n",
    "- \"main\" (https://intern.ohmportal.de/seitenbaum/home/page.html)\n",
    "\n",
    "main>div#main-coloumn>dicv>#idcontent-coloumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_content(file):\n",
    "    soup = BeautifulSoup(file, \"lxml\")\n",
    "\n",
    "    title = soup.find(\"title\")\n",
    "    if title:\n",
    "        title = title.text\n",
    "    else:\n",
    "        title = \"\"\n",
    "\n",
    "    main = soup.find(\"div\", {'id': 'main'})\n",
    "    \n",
    "    visible_texts = \"\"\n",
    "    if main:\n",
    "        container = main.find(\"div\", {'id': 'contentColumn'})\n",
    "        if container:\n",
    "            texts = container.find_all(text=True)\n",
    "            visible_texts = filter(tag_visible, texts)\n",
    "            visible_texts = [t.strip() for t in visible_texts if t.strip()]  # Entferne Leerzeichen und leere Zeichenfolgen\n",
    "            visible_texts = ' '.join(visible_texts)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"text\": visible_texts\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun speichern wir die Texte sowie die dazugehörigen Titeln in den Dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1686 [00:00<01:37, 17.24it/s]C:\\Users\\lizab\\AppData\\Local\\Temp\\ipykernel_10120\\1933498786.py:25: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  texts = container.find_all(text=True)\n",
      "100%|██████████| 1686/1686 [00:10<00:00, 159.05it/s]\n"
     ]
    }
   ],
   "source": [
    "parsed_texts = []\n",
    "titles = []\n",
    "for html in tqdm(df[\"html\"]):\n",
    "    content = get_content(html)\n",
    "    parsed_texts.append(content[\"text\"])\n",
    "    titles.append(content[\"title\"])\n",
    "\n",
    "df[\"text\"] = parsed_texts\n",
    "df[\"title\"] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                        \n",
      "1                                                        \n",
      "2       Donnerstag, 04. Juli 2019 finanziert neues eBo...\n",
      "3       Öffnungszeiten Ab hier finden Sie die Öffnungs...\n",
      "4       Ältestenrat Aufgaben Der Ältestenrat unterstüt...\n",
      "                              ...                        \n",
      "1681    Ausgabe der Abschlussarbeit Auf dieser und den...\n",
      "1682    Anmeldung der Abschlussarbeit 1. Zur Erfassung...\n",
      "1683    Abgabe der Abschlussarbeit 1. rechtzeitige Abg...\n",
      "1684    Wiederholung der Abschlussarbeit Wird eine Bac...\n",
      "1685    Bewertung der Abschlussarbeit Zur Bewertung de...\n",
      "Name: text, Length: 1686, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                        \n",
      "1                                                        \n",
      "2                          Technische Hochschule Nürnberg\n",
      "3       Technische Hochschule Nürnberg: Öffnungszeiten...\n",
      "4             Technische Hochschule Nürnberg: Ältestenrat\n",
      "                              ...                        \n",
      "1681    Technische Hochschule Nürnberg: Ausgabe der Ab...\n",
      "1682    Technische Hochschule Nürnberg: Anmeldung der ...\n",
      "1683    Technische Hochschule Nürnberg: Abgabe der Abs...\n",
      "1684    Technische Hochschule Nürnberg: Wiederholung d...\n",
      "1685    Technische Hochschule Nürnberg: Bewertung der ...\n",
      "Name: title, Length: 1686, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"intranet_html_attributes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
