{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Die Webseite der TH Nürnberg- Intranet Scrapen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einleitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Webseite der TH-Nürnberg wirkt als Ausgangspunkt für die Wissensgrundlage des Chatbots.\n",
    "\n",
    "Conventions: \n",
    "- pandas Spaltennamen im Sigular\n",
    "- Die meißten Links sind keine URLs, da sie lokal sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import pandas as pd\n",
    "from db_init import db_get_df, db_save_df\n",
    "import glob\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapen der home Seite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Ausganspunkt für unsere Daten nutzen wir die Home Webseite der TH-Nürnberg. (https://www.th-nuernberg.de/)\n",
    "Diese Website downloaden wir und suchen alle Links auf andere Webseiten und speichern diese Links in eine Liste.\n",
    "Als nächsten Schritt rufen wir alle Links aus dieser Liste auf und sammeln wiederum alle Links von jeder dieser Webseiten.\n",
    "In der daraus resultierenden Liste sortieren wir alle Links aus, die nicht auf die Webseite der TH verweisen.\n",
    "Dann laden wir alle Dokumente herunter und speichern sie in der Datenbank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Funktion, die eine URL als parameter nimmt und das HTML file zurückgibt, falls die Seite existiert.\n",
    "\n",
    "Dafür nutzen wir die requests Bibliothek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_html_from_url(url):\n",
    "    res = requests.get(url)\n",
    "    html = \"\"\n",
    "    if res.ok:\n",
    "        html =  res.text\n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Funktion, die ein HTML file nach Links durchsucht und alle gefundenen externen und internen Links zurückgibt.\n",
    "Dafür nutzen wir die Bibliothek Beautifulsoup, mit dem lxml parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_one_html(html):\n",
    "    soup = BeautifulSoup(html,\"lxml\")\n",
    "    links = [a[\"href\"] for a in soup.find_all('a', href=True)]\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt geben wir unsere initiale URL an und extrahieren alle Links aus dieser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://intern.ohmportal.de/\"\n",
    "html = download_html_from_url(BASE_URL)\n",
    "links = get_links_from_one_html(html)\n",
    "df = pd.DataFrame({\"link\": links})\n",
    "print(*df[\"link\"])\n",
    "print(len(df[\"link\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links filtern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst können wir alle Links überprüfen, ob sie Parameter oder sections mit \n",
    "- *?param1=hallo*\n",
    "- *#section* \n",
    "\n",
    "enthalten. Beide Attribute sind für den Download der Webseiten nicht notwendig und werden ausgefiltert. Dies spart uns HTML Duplikate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_links(links):\n",
    "    cleaned_links = [link.split('#')[0].split('?')[0] for link in links]\n",
    "    return cleaned_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun speichern wir das zwischen Ergebnis der gefilterten Links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"link\"]=clean_links(df[\"link\"])\n",
    "print(*df[\"link\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun löschen wir alle Links, die kein Inhalt haben (leere Links). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_links(links):\n",
    "    cleaned_links = [link for link in links if link.strip()]\n",
    "    return cleaned_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"link\": remove_empty_links(df[\"link\"])})\n",
    "print(*df[\"link\"])\n",
    "print(len(df[\"link\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man sieht, dass wir 7 Links ausgefiltert haben, wenn man die Anzahl der Links vor und nach der Filterung vergleicht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Externe Links finden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir Mal nachschauen, auf welche externen Seiten die Startseite der THN verweist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_extern_urls(urls):\n",
    "    external_links = []\n",
    "    for link in urls:\n",
    "        if link.startswith(\"http\"):\n",
    "            external_links.append(link)\n",
    "\n",
    "    return external_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_links = find_extern_urls(df[\"link\"])\n",
    "print(\"Anzahl externer Links: \", len(external_links))\n",
    "print(external_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Seite verweist also auf alle gängigen Sozial Media Seiten, wie Twitter, youtube, tiktok (der nicht existiert), instagram (der nicht existiert), xing, oft auf die jobbörse mit mehreren Links ins Intranet und auf die Efi fakultät,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interne Links filtern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir filtern nun noch alle Links heraus, die keine HTML-inhalte besitzen, wie z.B. pdf oder xml Dateien, die mail Links enthalten oder die nicht auf die THN Webseite referieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_intern_links(urls):\n",
    "    filtered_links = []\n",
    "    for url in urls:\n",
    "        if url.startswith(\"http\"):\n",
    "            continue\n",
    "        elif url.startswith(\"mailto:\"):\n",
    "            continue\n",
    "        elif url.startswith(\"javascript:\"):\n",
    "            continue\n",
    "        elif url.startswith(\"&#\"): # is encoded mailto\n",
    "            continue\n",
    "        elif \".xml\" in url:\n",
    "            continue\n",
    "        elif \".pdf\" in url:\n",
    "            continue\n",
    "        elif url == \"/\":\n",
    "            continue\n",
    "        elif url == \"&\":\n",
    "            continue\n",
    "        else:\n",
    "            filtered_links.append(url)\n",
    "    return filtered_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intern_links = filter_intern_links(df[\"link\"])\n",
    "print(\"Anzahl interner Links: \", len(intern_links))\n",
    "print(intern_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun haben wir weitere 55 Links entfernt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dublikate entfernen und sortieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir die duplikate entfernen und anschließend alphabetisch sortieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_and_remove_dublicates(df):\n",
    "    df = df.sort_values(\"link\")\n",
    "    df = df.drop_duplicates(subset=\"link\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intern_links = filter_intern_links(df[\"link\"])\n",
    "df = pd.DataFrame({\"link\": intern_links})\n",
    "df = sort_and_remove_dublicates(df)\n",
    "print(\"Anzahl interner Links (ohne Dublikate): \", len(intern_links))\n",
    "print(*df[\"link\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anhand der Anzahl der Links vor und nach der Duplikaten Entfernung, sieht man das die THN Startseite, keine doppelt vorkommende Links enthält."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links abspeichern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für unsere weiteren Schritte werden wir immer nur interne Links verwenden, deshalb speichern wir an dieser Stelle mal die internen Links ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"only_links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloaden der files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir mit dem downloaden anfangen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Funktion lädt nun alle Html files zu den Links herunter und speichert sie im Dataframe neben den \"link\" in einer Spalte \"html\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all_urls(links):\n",
    "    htmls = []\n",
    "    for link in tqdm(links):\n",
    "        url = \"https://www.th-nuernberg.de\" + link\n",
    "        html = download_html_from_url(url)\n",
    "        htmls.append(html)\n",
    "    return htmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"html\"]=download_all_urls(df[\"link\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können die Daten an dieser Stelle abspeichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"intranet_html_iter_01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weitere Iterationsstufen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir ab diesem Abschnitt starten können wir die vorher gesammelten Daten neu laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"intranet_html_iter_01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir die heruntergeladenen HTML files nach weiteren Links durchsuchen und Sie dem Dataframe hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_links_in_html(htmls):\n",
    "    all_links = []\n",
    "    for html in tqdm(htmls):\n",
    "        links = get_links_from_one_html(html)\n",
    "        links= clean_links(links)\n",
    "        links= remove_empty_links(links)\n",
    "        links= filter_intern_links(links)\n",
    "        [all_links.append(link) for link in links]\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = find_all_links_in_html(df[\"html\"])\n",
    "len(all_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben jetzt also 296 Links gesammelt, davon sind aber viele Dublikate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame({\"link\": all_links, \"html\": None})\n",
    "df = pd.concat([df, df_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sort_and_remove_dublicates(df)\n",
    "len(df[\"link\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gefiltert nach dublikaten haben wir nun also noch 277 Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*df[\"link\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteratives Downloaden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den Daten nun weitere Webseiten hinzuzufügen, können wir für jede weitere URL schauen, ob sie schon heruntergeladen wurde. Wenn nicht, dann laden wir sie jetzt herunter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_df_with_html(df):\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        if pd.isna(row['html']) or row['html'] == '':\n",
    "            url = \"https://intern.ohmportal.de/\" + row[\"link\"]\n",
    "            html = download_html_from_url(url)\n",
    "            df.at[index, 'html'] = html\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = update_df_with_html(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterationen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Iteration: Startseite THN (~300 Links)\n",
    "2. Iteration: Links von startseite aufrufen und HTML scrapen (~2000 Links)\n",
    "3. Iteration: Links von diesen Seiten aufrufen und HTML scrapen (~ Links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration(df):\n",
    "    all_links = find_all_links_in_html(df[\"html\"])\n",
    "    df_new = pd.DataFrame({\"link\": all_links, \"html\": None})\n",
    "    df = pd.concat([df, df_new])\n",
    "    df = sort_and_remove_dublicates(df)\n",
    "    df=update_df_with_html(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=iteration(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texte extrahieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes müssen wir aus den rohen HTML Dokumenten die unrelevanten Daten aussortieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"html_iter_01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die nachfolgende Funktion bestimmt, ob ein Beautifulsoup geparstes HTML Element sichtbar ist oder nicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt bestimmen wir eine Funktion, die ein HTML als Input bekommt und daraus die Texte und Titeln bestimmt.\n",
    "\n",
    "Die HTML Seiten haben 3 verschiedene Strukturen:\n",
    "\n",
    "- \"main\" (https://www.th-nuernberg.de/hochschule-region/organisation-und-struktur/hochschulleitung-und-gremien/)\n",
    "- \"div\", {'class': 'portal'} (https://www.th-nuernberg.de/fakultaeten/bi/)\n",
    "- \"div\", {'class': 'page-wrap'} (https://www.th-nuernberg.de/studium-karriere/studien-und-bildungsangebot/duale-studienmodelle/studium-mit-vertiefter-praxis/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(file):\n",
    "    soup = BeautifulSoup(file,\"lxml\")\n",
    "\n",
    "    title = soup.find(\"title\")\n",
    "    if title:\n",
    "        title = title.text\n",
    "    else:\n",
    "        title = \"\"\n",
    "\n",
    "    main = soup.find(\"main\")\n",
    "    portal = soup.find(\"div\", {'class': 'portal'})\n",
    "    page_container = soup.find(\"div\", {'class': 'page-wrap'})\n",
    "    \n",
    "    visible_texts = \"\"\n",
    "    if main:\n",
    "        container = main.find(\"div\" ,{'class': 'container'}, recursive=False)\n",
    "        if container:\n",
    "            texts = container.find_all(text=True)\n",
    "            visible_texts = filter(tag_visible, texts)\n",
    "\n",
    "    elif portal:\n",
    "        texts = portal.find_all(text=True)\n",
    "        visible_texts = filter(tag_visible, texts)\n",
    "\n",
    "    elif page_container:\n",
    "        container = page_container.find(\"div\" ,{'class': 'container'}, recursive=False)\n",
    "        if container:\n",
    "            texts = container.find_all(text=True)\n",
    "            visible_texts = filter(tag_visible, texts)\n",
    "\n",
    "    return {\n",
    "        \"title\":    title,\n",
    "        \"text\":     u\" \".join(t.strip() for t in visible_texts)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun speichern wir die Texte sowie die dazugehörigen Titeln in den Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_texts = []\n",
    "titles = []\n",
    "for html in tqdm(df[\"html\"]):\n",
    "    content = get_content(html)\n",
    "    parsed_texts.append(content[\"text\"])\n",
    "    titles.append(content[\"title\"])\n",
    "\n",
    "df[\"text\"] = parsed_texts\n",
    "df[\"title\"] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df[df[\"text\"] != \"\"][\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"html_attribute\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
