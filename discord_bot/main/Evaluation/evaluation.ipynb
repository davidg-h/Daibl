{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einleitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Notebook vergleichen wir verschiedene Ansätze, um sicherzustellen, dass unser Bot die am besten passenden Antworten generiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die 5 Fragen, die verglichen sollen folgende Eigenschaften besitzen:\n",
    "- Die Fragen sollen Fakultät übergreifend sein\n",
    "- Die Fragen sollen TH-spezifisch sein\n",
    "- Die Fragen sollen TH-Intranet-spezifisch sein\n",
    "- Die Fragen sollen Realitätsnah sein\n",
    "\n",
    "Die 5 Fragen, die verglichen werden sind:\n",
    "\n",
    "1. Wie ist die Email Adresse von Professor Gallwitz?\n",
    "2. Was soll ich beachten, wenn ich eine Prüfung anmelden will?\n",
    "3. Welche voraussetzungen, muss ich für den Mater Studiengang erfüllen?\n",
    "4. Welche Professoren gibt es an der Fakultät Soziale Arbeit?\n",
    "5. Wann und was muss ich im IT-Projekt machen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beispiel Tabelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun werden fogende Parameter verglichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| LLM Modell    | Kontext Variation | Word Embedding Model | Frage 1 | Antwort 1 | Frage 2 | Antwort 2 | Frage 3 | Antwort 3 | Frage 4 | Antwort 4 | Frage 5 | Antwort 5 | Durchschnitt |\n",
    "|---------------|-------------------|----------------------|---------|-----------|---------|-----------|---------|-----------|---------|-----------|---------|-----------|--------------|\n",
    "| Vicuna groß   | 1 Dokument        | a(MiniLM)            | 2       |           | 4       |           | 5       |           | 8       |           | 7       |           | 5.2          |\n",
    "| Vicuna groß   | 1 Dokument        | b(xy)                 | 10      |           | 10      |           | 10      |           | 10      |           | 10      |           | 10           |\n",
    "| Vicuna groß   | 1 Dokument        | c(TF-IDF)             |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 1 Dokument        | d(MiniLM*TF-IDF)      |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 1 Dokument        | e(xy*TF-IDF)          |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 5 Dokumente       | a                     |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 5 Dokumente       | b                     |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 5 Dokumente       | c                     |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 5 Dokumente       | d                     |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 5 Dokumente       | e                     |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 10 Dokumente      | a                     |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 10 Dokumente      | b                     |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 10 Dokumente      | c                     |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 10 Dokumente      | d                     |         |           |         |           |         |           |         |           |         |           | 0            |\n",
    "| Vicuna groß   | 10 Dokumente      | e                     |         |           |         |           |         |           |         |           |         |           | 0            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun soll ein script ersteltt werden, der die Antwort Spalten generiert.\n",
    "Nacher werden die Antworten manuell bewertet von allen Projekt Teilnehmer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/br/Projects/IT-Ptojekt-Chatbot/daibl-slim/daibl/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "from scrap.query_crafter import construct_prompt\n",
    "from scrap.embedding_algorithms.tdIdfDistance import get_most_similar_articles_tf_idf\n",
    "from scrap.embedding_algorithms.question_embedding_MiniLM import (\n",
    "    get_most_similar_articles_MiniLM,\n",
    ")\n",
    "from LLM.ServerCommunicator import server_get_answer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "import os\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from transformers import pipeline  # loading of hf LLMs\n",
    "import gc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verschiedene Embedding Modelle für Kontextgenerierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_articles_for_specified_embeding_model(embedding_model,question,document_amount):\n",
    "    \n",
    "    embeddings_model_dict = {\n",
    "        'MiniLM': get_most_similar_articles_MiniLM,\n",
    "        'TF-IDF': get_most_similar_articles_tf_idf\n",
    "    }\n",
    "     \n",
    "    get_most_similar_articles = embeddings_model_dict.get(embedding_model)\n",
    "    result = get_most_similar_articles(question,document_amount)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Model laden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_load(model_id):\n",
    "  HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "  login(token=HUGGINGFACEHUB_API_TOKEN)\n",
    "  device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "  model = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto',\n",
    "    temperature=0.3,\n",
    "    top_p=0.15,\n",
    "    top_k=15,\n",
    "    repetition_penalty=1.1,\n",
    "    num_return_sequences=1,\n",
    "    max_new_tokens=128,\n",
    "    #max_length=256,\n",
    "  )\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_unload(model):\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def return_prompt_answer(model, query_prompt):\n",
    "    \"\"\" process and return answer of LLM \"\"\"\n",
    "    \n",
    "    answer = model(query_prompt, do_sample=True)\n",
    "    return answer[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to forward the port to the cluster for the vicuna_70b model.\n",
    "To do so, copy the template into the terminal, change the username:  \n",
    "> ssh -N -L localhost:8080:localhost:8087 \\<mustermannm12345\\>@141.75.89.6 \n",
    "\n",
    "and enter your password afterwards.\n",
    "The Terminal has to be kept open for the tunneling to work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_answer_from_model(model_id, query_prompt,model):\n",
    "    if model_id == 'vicuna_70b':\n",
    "        answer = server_get_answer(query_prompt)\n",
    "    elif model_id == 'lmsys/vicuna-13b-v1.5': \n",
    "        answer = return_prompt_answer(model,query_prompt)\n",
    "    elif model_id  == 'meta-llama/Llama-2-13b-chat-hf':\n",
    "        answer = return_prompt_answer(model,query_prompt)\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_response(model_id,document_amount,embedding_model,question,model):\n",
    "    anwser=\"\"\n",
    "    documents = get_most_similar_articles_for_specified_embeding_model(embedding_model,question,document_amount)\n",
    "    query_prompt = construct_prompt(documents, question) \n",
    "    anwser = await get_answer_from_model(model_id,query_prompt,model)\n",
    "    \n",
    "    return anwser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await generate_response(\"vicuna_70b\",1,\"MiniLM\",\"Wie ist die Email Adresse von Professor Gallwitz?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alle Variationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = [\"Model\", \"Document Amount\", \"Embeddings Model\", \"Question\", \"Response\"]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "model_ids = [\"vicuna_70b\",\"vicuna_13b\",\"meta-llama/Llama-2-13b-chat-hf\"]\n",
    "document_amounts = [1, 5, 10]\n",
    "embeddings_models=[\"MiniLM\",\"TF-IDF\"]\n",
    "questions=[\"Wie ist die Email Adresse von Professor Gallwitz?\",\n",
    "           \"Was soll ich beachten, wenn ich eine Prüfung anmelden will?\",\n",
    "           \"Welche voraussetzungen, muss ich für den Master Studiengang erfüllen?\",\n",
    "           \"Welche Professoren gibt es an der Fakultät Soziale Arbeit?\",\n",
    "           \"Wann und was muss ich im IT-Projekt machen?\"]\n",
    "\n",
    "data = []\n",
    "model= None\n",
    "for model_id in model_ids:\n",
    "    if(model_id != \"vicuna_70b\"):\n",
    "       model= model_load(model_id)\n",
    "    for document_amount in document_amounts:\n",
    "        for embeddings_model in embeddings_models:\n",
    "            for question in questions:\n",
    "                response = await generate_response(model_id, document_amount, embeddings_model, question,model)\n",
    "                data.append([model_id, document_amount, embeddings_model, question, response])\n",
    "    \n",
    "\n",
    "df = pd.concat([df, pd.DataFrame(data, columns=columns)], ignore_index=True)\n",
    "\n",
    "df.to_excel('Evaluation.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"meta-llama/Llama-2-13b-chat-hf\"\n",
    "model_id=model_load(model_id)\n",
    "####################### Pipeline for model loading and unloading ###############################\n",
    "import os\n",
    "import gc\n",
    "from multiprocessing import Process\n",
    "\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from transformers import pipeline \n",
    "\n",
    "def run_model(model_id, question):\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Torch allocated memory: {torch.cuda.memory_allocated()}; {device}\")\n",
    "    model = pipeline(\n",
    "        task=\"text-generation\", \n",
    "        model=model_id,\n",
    "        torch_dtype=torch.bfloat16, \n",
    "        device_map='auto',\n",
    "        temperature=0.3,\n",
    "        top_p=0.15,\n",
    "        top_k=15,\n",
    "        repetition_penalty=1.1,\n",
    "        num_return_sequences=1,\n",
    "        #max_new_tokens=128,\n",
    "        max_length=64,\n",
    "    )\n",
    "    answer = model(question, do_sample=True)\n",
    "    print(answer[0]['generated_text'])\n",
    "    \n",
    "    # free resources and memory after process terminates\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Torch allocated memory: {torch.cuda.memory_allocated()}; {device}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    login(token=\"token\")\n",
    "\n",
    "    question = \"Hallo wer bist du?\"\n",
    "\n",
    "    model_id = [\"meta-llama/Llama-2-13b-chat-hf\", \"meta-llama/Llama-2-7b-chat-hf\", \"meta-llama/Llama-2-7b-chat-hf\"]\n",
    "    for i in range(2):\n",
    "        for name in model_id:\n",
    "            p = Process(target=run_model, args=(name, question))\n",
    "            p.start()\n",
    "            p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten automatisch evaluieren lassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "from scrap.db_init import db_get_df, db_save_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"~/Desktop/output.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_answer(question):\n",
    "    if question == \"Wie ist die Email Adresse von Professor Gallwitz?\":\n",
    "        return \"Die Email Adresse von Prof. Dr. Gallwitz ist: florian.gallwitz@th-nuernberg.de\"\n",
    "    elif question == \"Was soll ich beachten, wenn ich eine Prüfung anmelden will?\":\n",
    "        return \"Um eine Prüfung anzumelden, müssen Sie folgende Kriterien beachten: Anmeldefristen,Zulassungsvoraussetzungen, Anmeldeverfahren, Prüfungstermin, Prüfungsvorbereitung.\"\n",
    "    elif (\n",
    "        question\n",
    "        == \"Welche voraussetzungen, muss ich für den Master Studiengang erfüllen?\"\n",
    "    ):\n",
    "        return \"Für den MIN Master Studiengang an der Technische Hochschule Nürnberg , müssen Sie folgende Voraussetzungen erfüllen: Bachelorabschluss in IN/MIN/WIN oder verwandete Fächer, Notendurchschnitt von 2,5, Sprachkenntnisse von C1, Bewerbungsunterlagen bereit stellen.\"\n",
    "    elif question == \"Welche Professoren gibt es an der Fakultät Soziale Arbeit?\":\n",
    "        return \"An der Fakultät für Soziale Arbeit gibt es folgende Proffessoren: Johannes Bach, Steffen Brockmann, Michael Domes, Simone Emmert, Carolin Freier, Sabine Fromm …\"\n",
    "    elif question == \"Wann und was muss ich im IT-Projekt machen?\":\n",
    "        return \"Der Praxisbeauftragter der Fakultät Informatik ist Prof. Dr. Wolfgang Bremer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.iterrows():\n",
    "    row = row[1]\n",
    "    question = row[\"Question\"]\n",
    "    sample_answer = get_sample_answer(question)\n",
    "    model_answer = row[\"Response\"]\n",
    "    print(question, sample_answer, model_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [02:18,  2.30s/it]\n"
     ]
    }
   ],
   "source": [
    "from gpt_evaluate import gpt_evaluate_outputs\n",
    "\n",
    "df = db_get_df(\"evaluation\")\n",
    "df = gpt_evaluate_outputs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if isinstance(row['Score'], dict):\n",
    "        s1 = row['Score'].get('Inhaltliche Korrektheit', 0)\n",
    "        s2 = row['Score'].get('Sprachliche Gestaltung', 0)\n",
    "        s3 = row['Score'].get('Fokus', 0)\n",
    "        df.at[index, 'Score'] = s1 + s2 + s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Document Amount</th>\n",
       "      <th>Embeddings Model</th>\n",
       "      <th>Question</th>\n",
       "      <th>Response</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmsys/vicuna-13b-v1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>MiniLM</td>\n",
       "      <td>Wie ist die Email Adresse von Professor Gallwitz?</td>\n",
       "      <td>\\n        \\n        Die Antwort lautet: [galwi...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmsys/vicuna-13b-v1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>MiniLM</td>\n",
       "      <td>Was soll ich beachten, wenn ich eine Prüfung a...</td>\n",
       "      <td>\\n\\n        Antwort: B) Unterlagen für die Anm...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lmsys/vicuna-13b-v1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>MiniLM</td>\n",
       "      <td>Welche voraussetzungen, muss ich für den Maste...</td>\n",
       "      <td>\\n        \\n        Um produktiv zu bleiben, h...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lmsys/vicuna-13b-v1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>MiniLM</td>\n",
       "      <td>Welche Professoren gibt es an der Fakultät Soz...</td>\n",
       "      <td>\\n        \\n        Es tut mir leid, aber ich ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lmsys/vicuna-13b-v1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>MiniLM</td>\n",
       "      <td>Wann und was muss ich im IT-Projekt machen?</td>\n",
       "      <td>\\n        \\n        IT-Projekt:\\n        Proje...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Document Amount Embeddings Model  \\\n",
       "0  lmsys/vicuna-13b-v1.5                1           MiniLM   \n",
       "1  lmsys/vicuna-13b-v1.5                1           MiniLM   \n",
       "2  lmsys/vicuna-13b-v1.5                1           MiniLM   \n",
       "3  lmsys/vicuna-13b-v1.5                1           MiniLM   \n",
       "4  lmsys/vicuna-13b-v1.5                1           MiniLM   \n",
       "\n",
       "                                            Question  \\\n",
       "0  Wie ist die Email Adresse von Professor Gallwitz?   \n",
       "1  Was soll ich beachten, wenn ich eine Prüfung a...   \n",
       "2  Welche voraussetzungen, muss ich für den Maste...   \n",
       "3  Welche Professoren gibt es an der Fakultät Soz...   \n",
       "4        Wann und was muss ich im IT-Projekt machen?   \n",
       "\n",
       "                                            Response Score  \n",
       "0  \\n        \\n        Die Antwort lautet: [galwi...     7  \n",
       "1  \\n\\n        Antwort: B) Unterlagen für die Anm...     2  \n",
       "2  \\n        \\n        Um produktiv zu bleiben, h...     5  \n",
       "3  \\n        \\n        Es tut mir leid, aber ich ...     1  \n",
       "4  \\n        \\n        IT-Projekt:\\n        Proje...     7  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('Begründung', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"evaluation_gpt3.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"'evaluation_gpt3.5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('Evaluation_gpt3.5.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
