{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Processing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einleitung und Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Einleitung"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ziel dieses Notebooks ist, die gescrapeten Daten so zu verarbeiten, dass Sie später thematisch durchsucht werden können. Ein Embeddingvektor liefert genau diese Funktion. Auf eine Anfrage hin wird der Abstand zwischen dem Embedding der Frage und den Embeddings aller anderen Dokumenten berechnet. Dann werden die Dokumente mit den kleinsten Abständen ausgewählt und dem LLM als Kontext mitgegeben. Mithilfe des Wissens dieser Dokumente soll dass LLM dann in der Lage sein die Frage korrekt zu beantworten.\n",
    "Für die Embeddings benutzen wir [Google Bert](https://blog.google/products/search/search-language-understanding-bert/)\n",
    "\n",
    "Beispiel:\n",
    "\n",
    "User: Welche Dozenten unterrichten das Fach Grundlagen der Informatik?\n",
    "\n",
    "System wählt besten 5 Dokumente aus \n",
    "\n",
    "> <Dokument 1>: ... betreute Prof. Dr. Löhr eine Batchelorarbeit in Grundlagen der Informatik... <br>\n",
    "> <Dokument 2>: Prof. Dr. Weber tel.: 013882664 email: weber@th.de Raum: HQ: 403, Fächer: Grundlagen der Informatik ... <br>\n",
    "> <Dokument 3> ... <br>\n",
    "> <Dokument 4> ... <br>\n",
    "> <Dokument 5> ... <br>\n",
    "    \n",
    "\n",
    "Aus der Nutzeranfrage und den Dokumenten wird eine neue Query erstellt, die dem LLM dann final bereitgetellt wird. Diese sieht in etwa so aus:\n",
    "\n",
    "    \n",
    "> <Dokument 1>: ... betreute Prof. Dr. Löhr eine Batchelorarbeit in Grundlagen der Informatik... <br>\n",
    "> <Dokument 2>: Prof. Dr. Weber tel.: 013882664 email: weber@th.de Raum: HQ: 403, Fächer: Grundlagen der Informatik ... <br>\n",
    "> <Dokument 3> ... <br>\n",
    "> <Dokument 4> ... <br>\n",
    "> <Dokument 5> ... <br>\n",
    "> Bitte beantworte folgende Frage unter der Berücksichtigung obiger Dokumente:\n",
    "> Welche Dozenten unterrichten das Fach Grundlagen der Informatik?\n",
    "\n",
    "\n",
    "Das LLM wird daraufhin hoffentlich korrekt eine Antwort liefern die ähnlich ist zu:\n",
    "\n",
    "> A: An der TH Nürnberg Georg Simon Ohm unterichten die Professoren Prof. Dr. Löhr und Prof. Dr. Weber das Fach Grundlagen der Informatik.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from db_init import db_get_df, db_save_df\n",
    "import json\n",
    "\n",
    "# import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from embedding_algorithms.question_embedding import question_embeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laden der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Laden aus der sqlite db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst laden wir die Daten aus der Datenbank. Dabei besitzt jedes Dokument als Metadaten den Titel der Webseite, den filenamen und den Text. Diese speichern wir uns in einen Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"html_attrs\", [\"filename\", \"title\", \"text\"])\n",
    "\n",
    "print(df.dtypes)\n",
    "print(df[\"text\"][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtern der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sortieren zunächst alle Dokumete aus, die keinen Text beinhalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ungefiltert sind es {len(df)} Dokumente\")\n",
    "df = df[df[\"text\"].apply(len) != 0]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(f\"gefiltert sind es {len(df)} Dokumente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"filename\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### nach Sprachen filtern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt filtern wir die Seiten noch in Englisch und deutsche Seiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en = df[df['filename'].str.startswith('data/htmlfiles/file_en')]\n",
    "len(df_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_de = df[df['filename'].str.startswith('data/htmlfiles/file_en') == False]\n",
    "len(df_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Beispiel Keywort suche"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur überprüfung der Texte können wir nun einmal eine Keywordsuche starten. Dieser Ansatz wird außerdem tiefer im Notebook [spacy_keywordextraction](./spacy_keywordextraction.ipynb) verfolgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Gallwitz\"\n",
    "\n",
    "[text for text in df[\"text\"] if word in text][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings berechnen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Einleitung zu BERT Tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt werden wir für jedes Dokument ein eigenes Word embeddings erstellen. Dazu müssen wir zunächst das BERT Model laden. <br>\n",
    "Das BERT Model ist ein von Google trainierter Encoder, welcher ursprünglich dafür Entwickelt wurde, dass er maskierte Wörter erraten kann (Masked Language Modelling) oder Vorhersagen kann, ob ein Satz auf einen anderen Satz folgt.(Next Sentence Prediction)\n",
    "- es ist trainiert auf 10.000+ Büchern\n",
    "- es gibt Modelle \"base\" und \"large\"\n",
    "uncased heißt ohne klein - Großschreibung\n",
    "\n",
    "Wir brauchen zur Vorbereitung die zusätzlichen Token\n",
    "- [SEP] um das Ende eines Satzes zu markieren\n",
    "- [CLS] am Anfang des Texten\n",
    "- [PAD] zum auffüllen der Token \n",
    "Außerdem\n",
    "TokenIDs\n",
    "MaskIDs - zum filtern der [PAD]\n",
    "Segment IDs um verschiedene Sätze zu unterscheiden\n",
    "Posititional Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Laden des BERT Modells und des BERT tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO try better model\n",
    "# BertModel.from_pretrained('bert-base-german-cased',output_hidden_states = True) \n",
    "model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True) \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testen des Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSentence = \"In der Bibliothek gibt es 40 Bücher zum Thema Animes\"\n",
    "tokens_question = tokenizer.tokenize(testSentence)\n",
    "print(*tokens_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokens im dataframe speichern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun erstellen wir eine neue Spalte [\"tokens\"], in der wir für jedes Dokument die Tokens abspeichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] = [tokenizer.tokenize(text) for text in tqdm(df[\"text\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der dataframe hat nun eine Spalte mehr und wir können uns ein Beispiel der Tokens ansehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "print(df[\"tokens\"][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokens in IDs umwandeln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Tokens müssen nun in IDs umgewandelt werden, damit sie das BERT Model für die Erstellung eines Embedding Vectors benutzen kann. Dafür benutzen wir eine Funktion des Tokenizers ```tokenizer.convert_tokens_to_ids()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"token_ids\"] = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tqdm(df[\"tokens\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_symbols = [\"[CLS]\", \"[SEP]\", \"[PAD]\"]\n",
    "print(tokenizer.convert_tokens_to_ids(special_symbols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokens splitten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben jetzt also die Tokens IDs für unsere 2400 verschiedenen Dokumente gebildet. Der nächste Schritt wäre nun, diese Token IDs dem BERT Model zu übergeben, sodass es uns ein Embedding daraus errechnet. Leider kann das BERT Model nur 512 Tokens (~1300 Zeichen) als Input nehmen. Die meißten der gescrapeten Webseiten sind aber wesentlich länger. <br> \n",
    "Der Naheliegendste Ansatz ist dabei, die Tokens einfach in 510 token große Chunks aufzusplitten (Wir brauchen noch 2 Tokens extra für jeden Chunk) und für jeden Chunk ein extra Embedding zu erstellen. <br> \n",
    "Dabei gibt es entweder die Möglichkeit die Chunks überlappend, oder einfach hard cut zu gestalten. <br>\n",
    "\n",
    "Wir werden hier zunächst den hard cut Ansatz verfolgen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_text_and_tokens(row):\n",
    "    text = row['text']\n",
    "    tokens_ids = row['token_ids']\n",
    "    filename = row['filename']\n",
    "\n",
    "    if len(tokens_ids) > 510:\n",
    "        for i in range(0, len(tokens_ids), 510):\n",
    "            chunk_tokens = tokens_ids[i:i + 510]\n",
    "            # adding the [CLS] and the [SEP] token\n",
    "            chunk_tokens = [101] + chunk_tokens + [102]\n",
    "            chunk_text = tokenizer.decode(chunk_tokens)\n",
    "\n",
    "            new_row = {'filename': filename, 'chunk_id': i/510, 'chunk_text': chunk_text, 'chunk_tokens_json': json.dumps(chunk_tokens)}\n",
    "            new_rows.append(new_row)\n",
    "    else:\n",
    "        # adding the [CLS] and the [SEP] token\n",
    "        tokens_ids = [101] + tokens_ids + [102]\n",
    "        tokens_ids = tokens_ids + [0] * (512 -len(tokens_ids))\n",
    "        text = \"[CLS]\" + text + \"[SEP]\"\n",
    "        new_row = {'filename': filename, 'chunk_id': 0, 'chunk_text': text, 'chunk_tokens_json': json.dumps(tokens_ids) }\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "new_rows = []\n",
    "df.apply(split_text_and_tokens, axis=1)\n",
    "chunk_df = pd.DataFrame(new_rows)\n",
    "chunk_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns an dieser Stelle die gesplitteten chunks anschauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunk_df.sample(2).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ab jetzt werden wir mit dem neuen chunk_df weiterarbeiten. Nun erstellen wir für jeden Chunk ein eigenes Embedding, welches dann die Semantik dieses chunks enthalten soll. Dafür müssen wir uns nun das BERT Model etwas genauer anschauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proccessSentence(tokens, model, tokenizer):\n",
    "    if len(tokens) == 0:\n",
    "        return torch.zeros(768)\n",
    "\n",
    "    tokens = [\"CLS\"] + tokens + [\"SEP\"]\n",
    "\n",
    "    attention_mask = [1 if token != \"[PAD]\" else 0  for token in tokens]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    token_ids_tensor = torch.tensor([token_ids], dtype=torch.int64)\n",
    "    attetion_mask_tensor = torch.tensor([attention_mask], dtype=torch.int64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(token_ids_tensor, attetion_mask_tensor)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # stack the layer list \n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    # remove the batches dim\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    # average all token embeds\n",
    "    layer_vecs = torch.mean(token_embeddings, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate the average of layer 3 to 13\n",
    "    embed = torch.mean(layer_vecs[2:], dim=0)\n",
    "\n",
    "\n",
    "    return embed\n",
    "\n",
    "df[\"chunk_embeddings\"] = [proccessSentence(tokens).tolist() for tokens in tqdm(df[\"tokens\"])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diser Code kann sehr lange brauchen um die Embeddings zu berechnen. Der Code An dieser Stelle sollte man dann die Embeddings am besten abspeichern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "db_save_df(df, \"chunk_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"chunk_embeddings\", [\"*\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_t_sne(embed_id):\n",
    "    df_embed=[json.loads(embedding) for embedding in tqdm(df[f\"chunk_embeddings_{embed_id}\"])]\n",
    "    word_embeddings = np.array(df_embed)\n",
    "    tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "    X_embedded = tsne.fit_transform(word_embeddings)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X_embedded[:, 0], X_embedded[:, 1], s=5)\n",
    "    plt.title(f\"t-SNE Visualization of Word Embeddings {embed_id}\")\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    embed_t_sne(i +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Extraktion von Word Embeddings für die Frage\n",
    "def get_word_embedding(question, model_name='bert-base-uncased'):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    tokens = tokenizer(question, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        question_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return question_embedding\n",
    "\n",
    "# Laden der Word Embeddings\n",
    "df = db_get_df(\"chunk_embeddings\", [\"chunk_embeddings_2\"])\n",
    "df = [json.loads(embedding) for embedding in tqdm(df[\"chunk_embeddings_2\"])]\n",
    "word_embeddings = np.array(df)\n",
    "\n",
    "# Extrahieren der Embeddings für die Frage\n",
    "question_text = \"n?\"\n",
    "question_embedding = get_word_embedding(question_text)\n",
    "\n",
    "# Berechnen der Kosinus-Ähnlichkeit zwischen der Frage und den anderen Word Embeddings\n",
    "similarities = cosine_similarity(word_embeddings, [question_embedding])\n",
    "\n",
    "# 'similarities' ist jetzt ein Array mit den Kosinus-Ähnlichkeiten zwischen der Frage und den anderen Word Embeddings.\n",
    "\n",
    "# Kombinieren Sie die t-SNE-Komponenten mit den Kosinus-Ähnlichkeiten\n",
    "combined_features = np.column_stack((X_embedded, similarities))\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(combined_features[:, 0], combined_features[:, 1], s=5)\n",
    "plt.scatter(question_embedding[0], question_embedding[1], color='red', s=50, label='Ihre Frage')\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings with Cosine Similarity\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annahme: X_embedded ist Ihre t-SNE-Visualisierung\n",
    "# Annahme: word_embeddings ist Ihre Matrix der Word Embeddings\n",
    "# Annahme: question_text ist Ihre Frage\n",
    "# Annahme: n_clusters ist die Anzahl der gewünschten Cluster\n",
    "question_text = \"Welche Kompetenzen hat Pr. Gallwitz?\" #wann ist der Bewerbungszeitraum  Für das Wintersemester\n",
    "\n",
    "# Schritt 1: Clustering durchführen\n",
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "cluster_labels = kmeans.fit_predict(word_embeddings)\n",
    "\n",
    "# Schritt 3: Berechnen der Ähnlichkeit zur Frage\n",
    "question_embedding = get_word_embedding(question_text)  # Verwenden Sie Ihre get_word_embedding Funktion\n",
    "similarities = cosine_similarity(word_embeddings, [question_embedding])\n",
    "\n",
    "# Schritt 4: Visualisierung aktualisieren\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(5):\n",
    "    plt.scatter(X_embedded[cluster_labels == i, 0], X_embedded[cluster_labels == i, 1], s=5, label=f'Cluster {i}')\n",
    "\n",
    "# Farben entsprechend des Clusters für die Frage aktualisieren\n",
    "question_cluster = np.argmax(similarities)\n",
    "plt.scatter(X_embedded[question_cluster, 0], X_embedded[question_cluster, 1], s=50, color='red', label='Ihre Frage')\n",
    "\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings with Clusters\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans = KMeans(n_clusters=5)  # Specify the number of clusters you want\n",
    "cluster_labels = kmeans.fit_predict(word_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(len(np.unique(cluster_labels))):\n",
    "    plt.scatter(X_embedded[cluster_labels == i, 0], X_embedded[cluster_labels == i, 1], s=5, label=f'Cluster {i}')\n",
    "\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings with Clusters\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the indices of data points in Cluster 0\n",
    "cluster0_indices = np.where(cluster_labels == 1)\n",
    "\n",
    "# Get the corresponding rows from the DataFrame 'df'\n",
    "cluster0_data_rows = df.iloc[cluster0_indices]\n",
    "\n",
    "# Print the 'text' column for the data points in Cluster 0\n",
    "for text in cluster0_data_rows['text']:\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Annahme: Ihr DataFrame 'df' enthält eine Spalte 'text' mit den Textdaten.\n",
    "\n",
    "# Anzahl der Cluster (angenommen, es sind 5 Cluster)\n",
    "num_clusters = 5\n",
    "\n",
    "for cluster_id in range(num_clusters):\n",
    "    # Filtern Sie die Zeilen für den aktuellen Cluster\n",
    "    cluster_data_rows = df\n",
    "\n",
    "    # Laden des spaCy-Modells für die Textverarbeitung\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    # Benutzerdefinierte Stoppwortliste\n",
    "    stopwords = {'www', 'th-nuernberg', 'nürnberg', 'nuernberg', 'th', 'technische', 'hochschule', 'ohm', 'de', 'punkt', 'simon'}\n",
    "\n",
    "    # Tokenisieren und Lemmatisieren der Texte, Entfernen der Stoppwörter und Konvertieren in Strings\n",
    "    processed_texts = []\n",
    "    for text in cluster_data_rows['text']:\n",
    "        doc = nlp(text)\n",
    "        processed_tokens = []\n",
    "        for token in doc:\n",
    "            if token.text.lower() not in stopwords and token.pos_ in {'NOUN', 'PROPN'}:\n",
    "                processed_tokens.append(token.text)\n",
    "        processed_texts.append(' '.join(processed_tokens))\n",
    "\n",
    "    # Erstellen eines Wörterbuchs und einer Textkorpus für das LDA-Modell\n",
    "    text_tokens = [text.split() for text in processed_texts]\n",
    "    dictionary = gensim.corpora.Dictionary(text_tokens)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in text_tokens]\n",
    "\n",
    "    # Anwendung des LDA-Modells\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "    # Anzeigen der Hauptthemen für den aktuellen Cluster\n",
    "    print(f\"Cluster {cluster_id} Topics:\")\n",
    "    for topic_id, topic in lda_model.print_topics():\n",
    "        print(f\"Topic {topic_id}: {topic}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_text = ' '.join(processed_texts)  # 'processed_texts' ist die Liste der bereinigten Texte\n",
    "\n",
    "# Erstellen der Word Cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "\n",
    "# Anzeigen der Word Cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Erstellen einer Liste von Stoppwörtern, einschließlich der URL und der benutzerdefinierten Wörter\n",
    "stopwords = set(['www', 'th-nuernberg', 'nürnberg', 'nuernberg', 'th', 'technische', 'hochschule', 'ohm', 'de', 'punkt', 'simon','https','http','nuremberg','telefon','email','fax','Prof Dr','studium'])\n",
    "stopwords = set(word.lower() for word in stopwords)  # In Kleinbuchstaben umwandeln\n",
    "\n",
    "# Anzahl der Cluster (angenommen, es sind 5 Cluster)\n",
    "num_clusters = 5\n",
    "\n",
    "for cluster_id in range(num_clusters):\n",
    "    if cluster_id == 0:\n",
    "        continue\n",
    "    # Filter the indices of data points in the current cluster\n",
    "    cluster_indices = np.where(cluster_labels == cluster_id)\n",
    "\n",
    "    # Get the corresponding rows from the DataFrame 'df'\n",
    "    cluster_data_rows = df.iloc[cluster_indices]\n",
    "\n",
    "    # Extract and preprocess text data\n",
    "    texts = cluster_data_rows['text']\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "    processed_texts = [' '.join([token.text for token in nlp(text) if not token.is_stop and token.text.lower() not in stopwords]) for text in texts]\n",
    "\n",
    "    # Create a Word Cloud for the current cluster\n",
    "    all_text = ' '.join(processed_texts)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "\n",
    "    # Display the Word Cloud for the current cluster\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Word Cloud for Cluster {cluster_id}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF-IDF Algorithmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "question=\"Welche Kompetenzen hat Prf. Gallwitz?\"\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n",
    "question_tfidf = tfidf_vectorizer.transform([question])\n",
    "similarities = cosine_similarity(question_tfidf, tfidf_matrix)\n",
    "\n",
    "similarity_df = pd.DataFrame({\n",
    "    'Similarity': similarities[0],\n",
    "    'Text': df['text']\n",
    "})\n",
    "sorted_similarity_df = similarity_df.sort_values(by='Similarity', ascending=False)\n",
    "\n",
    "# Print the top N most relevant documents (e.g., top 5)\n",
    "top_n = 5\n",
    "relevant_documents = sorted_similarity_df.head(top_n)\n",
    "\n",
    "# Print the relevant documents and their similarities to the question\n",
    "for index, row in relevant_documents.iterrows():\n",
    "    print(f\"Similarity: {row['Similarity']}\")\n",
    "    print(row['Text'])\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dokument--> bert anwenden für jeden dokument\n",
    "# question-bert anwenden\n",
    "question=\"was macht Gallwitz?\"\n",
    "document=df[\"text\"][1]\n",
    "\n",
    "tokens_question = tokenizer.tokenize(question)\n",
    "tokens_document = tokenizer.tokenize(document)\n",
    "attetion_mask_question = [1] * len(tokens_question)\n",
    "attention_mask_dokument = [1] * len(tokens_document)\n",
    "\n",
    "token_idss = tokenizer.convert_tokens_to_ids(tokens_question)\n",
    "tokenDocument_idss = tokenizer.convert_tokens_to_ids(tokens_document)\n",
    "\n",
    "\n",
    "tokens_tensor = torch.tensor([token_idss])\n",
    "segments_tensors = torch.tensor([attetion_mask_question])\n",
    "\n",
    "tokensDocument_tensor = torch.tensor([tokenDocument_idss])\n",
    "segmentsDocument_tensors = torch.tensor([attention_mask_dokument])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokensDocument_tensor, segmentsDocument_tensors)\n",
    "    hiddenDocuments_states = outputs[2]\n",
    "\n",
    "# print(token_idss)\n",
    "# print(tokenDocument_idss)\n",
    "\n",
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_vecs_sum = []\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding)\n",
    "\n",
    "\n",
    "tokenDocuments_vecs = hiddenDocuments_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentenceDocument_embedding = torch.mean(tokenDocuments_vecs, dim=0)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(sentence_embedding, sentenceDocument_embedding)\n",
    "\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Fachhochschulgesetz\"\n",
    "\n",
    "df.loc[df[\"text\"].str.contains(word)][\"text\"]\n",
    "# [text for text in df[\"text\"] if word in text][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"chunk_embeddings\")\n",
    "questions = [\"Was besagt das Fachhochschulgesetz?\", \n",
    "             \"Wo befindet sich die Mensa\", \n",
    "             \"Welche Professoren gibt es an der Technischen Hochschule Nürnberg?\",\n",
    "             \"Gib mir alle Infos zum Studienstart\",\n",
    "             \"Was gibt es Neues im Bezug auf Künstliche Intelligenz an der Hochschule?\",\n",
    "             \"Themen für eine Batchelorarbeit\",\n",
    "             \"Where is the Language office?\",\n",
    "             \"Where can i find the Mensa\",\n",
    "             \"Give me information on beginning of Semester\",\n",
    "             \"How many professors are there at the TH?\"]\n",
    "question = questions[random.randint(0,9)]\n",
    "question_embedding = question_embeddings(question)\n",
    "\n",
    "df[\"distance\"] = [1 - cosine(json.loads(embedding), question_embedding) for embedding in df[\"chunk_embeddings_2\"]]\n",
    "most_similar_documents = df.nsmallest(5, \"distance\")\n",
    "# print(f\"question embedding: {question_embedding[:10]}\")\n",
    "print(question)\n",
    "print(most_similar_documents[\"chunk_text\"].to_markdown())\n",
    "\n",
    "\n",
    "df[\"distance\"].plot(kind='hist', bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"word_embeddings\", [\"filename\", \"title\", \"text\", \"tokens\"])\n",
    "df[\"token_ids\"] = [tokenizer.convert_tokens_to_ids(json.loads(tokens)) for tokens in df[\"tokens\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splice dokuments in 512 token chunks\n",
    "\n",
    "# Initialize an empty list to store rows for the new DataFrame\n",
    "new_rows = []\n",
    "\n",
    "# Function to split text and tokens into chunks of 512 tokens\n",
    "def split_text_and_tokens(row):\n",
    "    text = row['text']\n",
    "    tokens_ids = row['token_ids']\n",
    "    filename = row['filename']\n",
    "\n",
    "    if len(tokens_ids) > 512:\n",
    "        # Split into multiple chunks\n",
    "        for i in range(0, len(tokens_ids), 512):\n",
    "            chunk_tokens = tokens_ids[i:i + 512]\n",
    "            chunk_text = tokenizer.decode(chunk_tokens)\n",
    "\n",
    "            # Create a new row with a reference to the original row\n",
    "            new_row = {'filename': filename, 'chunk_id': i/512, 'chunk_text': chunk_text, 'chunk_tokens_json': json.dumps(chunk_tokens)}\n",
    "            new_rows.append(new_row)\n",
    "    else:\n",
    "        # If the row has 512 tokens or fewer, keep it as is\n",
    "        new_row = {'filename': filename, 'chunk_id': 0, 'chunk_text': text, 'chunk_tokens_json': json.dumps(tokens_ids) }\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "# Apply the function to each row in the original DataFrame\n",
    "df.apply(split_text_and_tokens, axis=1)\n",
    "\n",
    "# Create a new DataFrame from the list of new rows\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Reset the index of the new DataFrame if needed\n",
    "new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(new_df.to_markdown())\n",
    "\n",
    "# tokenizer.convert_tokens_to_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus den 2433 Dokumenten die wir eigentlich gescraped haben, sind nun 6945 chunks entstanden es hat sich fast verdreifacht. Wenn man die 787 Seiten ohne Inhalt abzieht, hat sich die Anzahl von 1646 auf 6158 fast vervierfacht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"chunk_tokens_json\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(new_df, \"chunk_word_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "database_path = os.getenv(\"DATABASE_PATH\")\n",
    "\n",
    "def merge_db_tables():\n",
    "    # Connect to your SQLite database\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Create the new table using the structure of the first table (chunk_word_embeddings_0)\n",
    "    cursor.execute('''CREATE TABLE chunk_word_embeddings_all AS SELECT * FROM chunk_word_embeddings_0 WHERE 0''')\n",
    "    # Insert data from the other tables into the new table\n",
    "    for i in range(1, 8):\n",
    "        cursor.execute(f'INSERT INTO chunk_word_embeddings_all SELECT * FROM chunk_word_embeddings_{i}')\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "merge_db_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate embeddings with SentenceTransformer from huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### paraphrase-MiniLM-L6-v2 with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "df = db_get_df(\"html_attrs_de\", [\"filename\", \"title\", \"text\"])\n",
    "# Load spaCy for sentence segmentation\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "#TODO speichere embeddings in databank\n",
    "sentences = []\n",
    "embeddings = []\n",
    "\n",
    "for text in df['text']:\n",
    "    # Split the text into sentences using spaCy\n",
    "    doc = nlp(text)\n",
    "    sentence_list = [sent.text for sent in doc.sents]\n",
    "    sentences.append(sentence_list)\n",
    "    \n",
    "    # Encode each sentence using Sentence Transformers\n",
    "    sentence_embeddings = [model.encode(sentence) for sentence in sentence_list]\n",
    "    embeddings.append(sentence_embeddings)\n",
    "    print(embeddings)\n",
    "\n",
    "# Now, 'sentences' is a list where each element is a list of sentences, and 'embeddings' is a list of corresponding sentence embeddings.\n",
    "# filenames = df.head(3)\n",
    "# print(filenames.to_markdown())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### embeddings with sentence-transformers/all-MiniLM-L6-v2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the pre-trained MiniLM model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Create a list to store the embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Assuming 'text' column holds your HTML content\n",
    "for text in df['text']:\n",
    "    # Encode the text and append the resulting embedding to the list\n",
    "    text_embedding = model.encode(text)\n",
    "    embeddings.append(text_embedding)\n",
    "\n",
    "# Create a new DataFrame to store the embeddings\n",
    "embeddings_df1 = pd.DataFrame(embeddings)\n",
    "\n",
    "# Now 'embeddings_df' contains the embeddings for each text in your DataFrame\n",
    "db_save_df(embeddings_df1, \"embeddings1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### embeddings with 'sentence-transformers/msmarco-distilbert-base-tas-b' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the pre-trained Multi-QA MiniLM model\n",
    "model = SentenceTransformer('sentence-transformers/msmarco-distilbert-base-tas-b')\n",
    "\n",
    "# Create a list to store the embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Assuming 'text' column holds your HTML content\n",
    "for text in tqdm(df['text'], desc=\"Encoding Texts\"):\n",
    "    # Encode the text and append the resulting embedding to the list\n",
    "    text_embedding = model.encode(text)\n",
    "    embeddings.append(text_embedding)\n",
    "\n",
    "# Create a new DataFrame to store the embeddings\n",
    "embeddings_df3 = pd.DataFrame(embeddings)\n",
    "db_save_df(embeddings_df3, \"embeddings3\")\n",
    "\n",
    "# Now 'embeddings_df' contains the embeddings for each text in your DataFrame using the multi-qa model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### embeddings with 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the pre-trained Multi-QA MiniLM model\n",
    "model = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Create a list to store the embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Assuming 'text' column holds your HTML content\n",
    "for text in tqdm(df['text'], desc=\"Encoding Texts\"):\n",
    "    # Encode the text and append the resulting embedding to the list\n",
    "    text_embedding = model.encode(text)\n",
    "    embeddings.append(text_embedding)\n",
    "\n",
    "# Create a new DataFrame to store the embeddings\n",
    "embeddings_df4 = pd.DataFrame(embeddings)\n",
    "db_save_df(embeddings_df4, \"embeddings4\")\n",
    "\n",
    "# Now 'embeddings_df' contains the embeddings for each text in your DataFrame using the multi-qa model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate embeddings with 'paraphrase-MiniLM-L6-v2' model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir laden das Modell \"paraphrase-MiniLM-L6-v2\" von HuggingFace herunter. Daraus werden Embeddings für die jeweiligen TH-Webseiten berechnet. Das Modell erstellt Embeddings der Größe 384 Floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = db_get_df(\"html_attrs\", [\"filename\", \"title\", \"text\"])\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "text_embeddings = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating embeddings\"):\n",
    "    text_embedding = model.encode(row['text'])\n",
    "    text_embeddings.append({'filename': row['filename'],\n",
    "                            'title': row['title'],\n",
    "                            'text': row['text'],\n",
    "                            'text_embedding': text_embedding})\n",
    "\n",
    "embeddings_df = pd.DataFrame(text_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sqlite kann keine Listen speichern, daher werde hier json Objekte erstellt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df[\"text_embedding_json\"] = [json.dumps(text_embedding.tolist()) for text_embedding in embeddings_df[\"text_embedding\"]]\n",
    "db_save_df(embeddings_df[[\"filename\",\"title\",\"text\",\"text_embedding_json\" ]], \"embeddings_paraphrase_MiniLM_L6_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarities(query,model,embeddings_df):\n",
    "    query_embedding = model.encode(query, show_progress_bar=True)\n",
    "    similarities = []\n",
    "\n",
    "    for index, row in tqdm(embeddings_df.iterrows(), total=len(embeddings_df), desc=\"Calculating similarities\"):\n",
    "        \n",
    "        similarity = cosine_similarity([query_embedding], [row[\"text_embedding\"]])[0][0]\n",
    "\n",
    "        similarities.append({'filename': row['filename'],\n",
    "                            'title': row['title'],\n",
    "                            'text': row['text'], \n",
    "                            'similarity': similarity})\n",
    "\n",
    "    similarities_df = pd.DataFrame(similarities)\n",
    "    return similarities_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarities(similarities_df):\n",
    "    similarities_df['similarity'].plot(kind='hist', bins=200)\n",
    "    plt.xlabel('Similarity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Similarity Distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir schauen uns die similarities für unterschiedliche Fragen an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "embeddings_df=db_get_df(\"embeddings_paraphrase_MiniLM_L6_v2\")\n",
    "# Convert back to lists.\n",
    "embeddings_df[\"text_embedding\"] = [json.loads(text_embedding_json) for text_embedding_json in embeddings_df[\"text_embedding_json\"]]\n",
    "queries = [\"Was besagt das Fachhochschulgesetz?\", \n",
    "             \"Wo befindet sich die Mensa\", \n",
    "             \"Welche Professoren gibt es an der Technischen Hochschule Nürnberg?\",\n",
    "             \"Gib mir alle Infos zum Studienstart\",\n",
    "             \"Was gibt es Neues im Bezug auf Künstliche Intelligenz an der Hochschule?\",\n",
    "             \"Themen für eine Batchelorarbeit\",\n",
    "             \"Where is the Language office?\",\n",
    "             \"Where can i find the Mensa\",\n",
    "             \"Give me information on beginning of Semester\",\n",
    "             \"How many professors are there at the TH?\"]\n",
    "\n",
    "most_similar_articles=\"\"\n",
    "\n",
    "for query in queries:\n",
    "    similarities_df=calculate_similarities(query,model,embeddings_df)\n",
    "    most_similar_articles = similarities_df.nlargest(5, 'similarity')['text']\n",
    "    print(\"prompt: \", query)\n",
    "    print(\"most_similar_articles: \\n\"+most_similar_articles.to_markdown())\n",
    "    plot_similarities(similarities_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Intranet HTML and TH-Nuernberg.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_init import db_get_df, db_save_df\n",
    "\n",
    "df_th_nuernberg = db_get_df(\"html_attrs_2086\")\n",
    "df_intranet = db_get_df(\"intranet_html_attrs_1686\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link     object\n",
      "html     object\n",
      "text     object\n",
      "title    object\n",
      "dtype: object\n",
      "link     object\n",
      "html     object\n",
      "text     object\n",
      "title    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_th_nuernberg.dtypes)\n",
    "print(df_intranet.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_th_nuernberg, df_intranet], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"html_attr_combined\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
