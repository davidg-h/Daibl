{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Processing</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ziel dieses Notebooks ist, die gescrapeten Daten so zu verarbeiten, dass Sie später thematisch durchsucht werden können. Ein Embeddingvektor liefert genau diese Funktion. Auf eine Anfrage hin wird der Abstand zwischen dem Embedding der Frage und den Embeddings aller anderen Dokumenten berechnet. Dann werden die Dokumente mit den kleinsten Abständen ausgewählt und dem LLM als Kontext mitgegeben. Mithilfe des Wissens dieser Dokumente soll dass LLM dann in der Lage sein die Frage korrekt zu beantworten.\n",
    "Für die Embeddings benutzen wir [Google Bert](https://blog.google/products/search/search-language-understanding-bert/)\n",
    "\n",
    "Beispiel:\n",
    "\n",
    "User: Welche Dozenten unterrichten das Fach Grundlagen der Informatik?\n",
    "\n",
    "System wählt besten 5 Dokumente aus \n",
    "\n",
    "    <Dokument 1>: ... betreute Prof. Dr. Löhr eine Batchelorarbeit in Grundlagen der Informatik...\n",
    "    <Dokument 2>: Prof. Dr. Weber tel.: 013882664 email: weber@th.de Raum: HQ: 403, Fächer: Grundlagen der Informatik ...\n",
    "    <Dokument 3> ...\n",
    "    <Dokument 4> ...\n",
    "    <Dokument 5> ...\n",
    "    \n",
    "\n",
    "Aus der Nutzeranfrage und den Dokumenten wird eine neue Query erstellt, die dem LLM dann final bereitgetellt wird. Diese sieht in etwa so aus:\n",
    "\n",
    "    \n",
    "    <Dokument 1>: ... betreute Prof. Dr. Löhr eine Batchelorarbeit in Grundlagen der     Informatik...\n",
    "    <Dokument 2>: Prof. Dr. Weber tel.: 013882664 email: weber@th.de Raum: HQ: 403, Fächer: Grundlagen der Informatik ...\n",
    "    <Dokument 3> ...\n",
    "    <Dokument 4> ...\n",
    "    <Dokument 5> ...\n",
    "\n",
    "    Bitte beantworte folgende Frage unter der Berücksichtigung obiger Dokumente:\n",
    "    Welche Dozenten unterrichten das Fach Grundlagen der Informatik?\n",
    "\n",
    "\n",
    "Das LLM wird daraufhin hoffentlich korrekt eine Antwort liefern die ähnlich ist zu:\n",
    "\n",
    "    A: An der TH Nürnberg Georg Simon Ohm unterichten die Professoren Prof. Dr. Löhr und Prof. Dr. Weber das Fach Grundlagen der Informatik.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cosine\n",
    "from db_init import db_get_df, db_save_df\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst laden wir die Daten aus der Datenbank. Dabei besitzt jedes Dokument als Metadaten den Titel der Webseite, den filenamen und den Text. Diese speichern wir uns in einen Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"html_attrs\", [\"filename\", \"title\", \"text\"])\n",
    "\n",
    "print(df.dtypes)\n",
    "print(df[\"text\"][3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur überprüfung der Texte können wir nun einmal eine Keywordsuche starten. Dieser Ansatz wird außerdem tiefer im Notebook [spacy_keywordextraction](./spacy_keywordextraction.ipynb) verfolgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Gallwitz\"\n",
    "\n",
    "[text for text in df[\"text\"] if word in text][:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt werden wir für jedes Dokument ein eigenes Word embeddings erstellen. Dazu müssen wir zunächst das BERT Model laden.\n",
    "Das BERT Model ist ein von Google trainiertes mehrschichtiges neuronales Netz, welches ursprünglich dafür entwickelt wurde, dass ???\n",
    "es ist trainiert auf 10.000+ Büchern\n",
    "es gibt Modelle \"base\" und \"large\"\n",
    "uncased heißt ohne klein - Großschreibung\n",
    "\n",
    "Wir brauchen zur vorbereitung die zusätzlichen Token\n",
    "[SEP] um das Ende eines Satzes zu markieren\n",
    "[CLS] am Anfang des Texten\n",
    "[PAD] zum auffüllen der Token \n",
    "Außerdem\n",
    "TokenIDs\n",
    "MaskIDs - zum filtern der [PAD]\n",
    "Segment IDs um verschiedene Sätze zu unterscheiden\n",
    "Posititional Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased') #TODO try better model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "testSentence = \"In der Bibliothek gibt es 40 Bücher zu Thema Animes\"\n",
    "tokens_question = tokenizer.tokenize(testSentence)\n",
    "tokens_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] = [tokenizer.tokenize(text) for text in df[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "print(df[\"tokens\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_question = ['[CLS]'] + tokens_question + ['[SEP]']\n",
    "attention_mask = [1 if token != \"[PAD]\" else 0  for token in tokens_question]\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens_question)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "\n",
    "attention_mask = torch.tensor(attention_mask).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(token_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proccessSentence(tokens):\n",
    "    if len(tokens) == 0:\n",
    "        # Handle the case when the token list is empty, for example, return a default embedding or raise an exception.\n",
    "        # For demonstration purposes, we'll return a zero tensor as the default embedding.\n",
    "        return torch.zeros(768)\n",
    "\n",
    "    # Ensure the token sequence length is no longer than the maximum sequence length the model can handle (512)\n",
    "    if len(tokens) > 512:\n",
    "        tokens = tokens[:512]\n",
    "\n",
    "    # Padding the token sequence to the maximum sequence length if it's shorter\n",
    "    if len(tokens) < 512:\n",
    "        tokens += ['[PAD]'] * (512 - len(tokens))\n",
    "\n",
    "    attention_mask = [1 if token != \"[PAD]\" else 0  for token in tokens]\n",
    "    tokenDocument_idss = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    tokensDocument_tensor = torch.tensor([tokenDocument_idss], dtype=torch.int64)\n",
    "    segmentsDocument_tensors = torch.tensor([attention_mask], dtype=torch.int64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokensDocument_tensor, segmentsDocument_tensors)\n",
    "        hiddenDocuments_states = outputs[2]\n",
    "\n",
    "    tokenDocuments_vecs = hiddenDocuments_states[-2][0]\n",
    "    sentenceDocument_embedding = torch.mean(tokenDocuments_vecs, dim=0)\n",
    "    #print(\"Our final sentence embedding vector of shape:\", sentenceDocument_embedding)\n",
    "\n",
    "    return sentenceDocument_embedding\n",
    "\n",
    "df[\"word_embeddings\"] = [proccessSentence(tokens).tolist() for tokens in tqdm(df[\"tokens\"])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "Here is an example how to calculate the tf-idf of the articles and find the 5 most similar(cosine similarity) artciles to a question tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "question=\"Welche Kompetenzen hat Prf. Gallwitz?\"\n",
    "# Assume you have a DataFrame df with a 'text' column that contains your documents\n",
    "# Also, assume you have a 'question' string for which you want to find relevant documents.\n",
    "\n",
    "# Create a TF-IDF vectorizer with the same parameters as before\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on your documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Transform the question string into TF-IDF representation\n",
    "question_tfidf = tfidf_vectorizer.transform([question])\n",
    "\n",
    "# Calculate cosine similarities between the question and all documents\n",
    "similarities = cosine_similarity(question_tfidf, tfidf_matrix)\n",
    "\n",
    "# Create a DataFrame to store similarities and document texts\n",
    "similarity_df = pd.DataFrame({\n",
    "    'Similarity': similarities[0],\n",
    "    'Text': df['text']\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by similarity in descending order\n",
    "sorted_similarity_df = similarity_df.sort_values(by='Similarity', ascending=False)\n",
    "\n",
    "# Print the top N most relevant documents (e.g., top 5)\n",
    "top_n = 5\n",
    "relevant_documents = sorted_similarity_df.head(top_n)\n",
    "\n",
    "# Print the relevant documents and their similarities to the question\n",
    "for index, row in relevant_documents.iterrows():\n",
    "    print(f\"Similarity: {row['Similarity']}\")\n",
    "    print(row['Text'])\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database = 'discord_bot/scrap/html.sqlite'\n",
    "\n",
    "# with sqlite3.connect(database) as con:\n",
    "#     html_df.to_sql('html_with_embeddings', con, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dokument--> bert anwenden für jeden dokument\n",
    "# question-bert anwenden\n",
    "question=\"was macht Gallwitz?\"\n",
    "document=df[\"text\"][1]\n",
    "\n",
    "tokens_question = tokenizer.tokenize(question)\n",
    "tokens_document = tokenizer.tokenize(document)\n",
    "attetion_mask_question = [1] * len(tokens_question)\n",
    "attention_mask_dokument = [1] * len(tokens_document)\n",
    "\n",
    "token_idss = tokenizer.convert_tokens_to_ids(tokens_question)\n",
    "tokenDocument_idss = tokenizer.convert_tokens_to_ids(tokens_document)\n",
    "\n",
    "\n",
    "tokens_tensor = torch.tensor([token_idss])\n",
    "segments_tensors = torch.tensor([attetion_mask_question])\n",
    "\n",
    "tokensDocument_tensor = torch.tensor([tokenDocument_idss])\n",
    "segmentsDocument_tensors = torch.tensor([attention_mask_dokument])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokensDocument_tensor, segmentsDocument_tensors)\n",
    "    hiddenDocuments_states = outputs[2]\n",
    "\n",
    "# print(token_idss)\n",
    "# print(tokenDocument_idss)\n",
    "\n",
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_vecs_sum = []\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding)\n",
    "\n",
    "\n",
    "tokenDocuments_vecs = hiddenDocuments_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentenceDocument_embedding = torch.mean(tokenDocuments_vecs, dim=0)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(sentence_embedding, sentenceDocument_embedding)\n",
    "\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'html.sqlite'\n",
    "sql = \"\"\"\n",
    "SELECT filename, title, text, word_embeddings FROM word_embeddings\n",
    "\"\"\"\n",
    "\n",
    "con = sqlite3.connect(database)\n",
    "df = pd.read_sql_query(sql, con)\n",
    "con.close()\n",
    "\n",
    "print(df.dtypes)\n",
    "print(df[\"text\"][3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(len(df[\"text\"][4]))\n",
    "# print(json.loads(df[\"text\"][0]))\n",
    "diff_bank = 1 - cosine(json.loads(df[\"word_embeddings\"][3]), json.loads(df[\"word_embeddings\"][6]))\n",
    "# for embed in df[\"word_embeddings\"]:\n",
    "# print(embed)\n",
    "\n",
    "print(diff_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"][1746]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Fachhochschulgesetz\"\n",
    "\n",
    "df.loc[df[\"text\"].str.contains(word)][\"text\"]\n",
    "# [text for text in df[\"text\"] if word in text][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/br/Projects/IT-Ptojekt-Chatbot/daibl/discord_bot')\n",
    "from question_embedding import question_embeddings\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# TODO 10 Fragen\n",
    "# TODO TSNE\n",
    "\n",
    "df = db_get_df(\"chunk_word_embeddings_all\")\n",
    "print(df.dtypes)\n",
    "question = \"TH\"\n",
    "question_embedding = question_embeddings(question)\n",
    "\n",
    "df[\"distance\"] = [1 - cosine(json.loads(embedding), question_embedding) for embedding in df[\"chunk_word_embeddings\"]]\n",
    "most_similar_documents = df.nsmallest(5, \"distance\")\n",
    "print(f\"question embedding: {question_embedding[:10]}\")\n",
    "print(most_similar_documents[\"distance\"])\n",
    "\n",
    "\n",
    "df[\"distance\"].plot(kind='hist', bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"word_embeddings\", [\"filename\", \"title\", \"text\", \"tokens\"])\n",
    "df[\"token_ids\"] = [tokenizer.convert_tokens_to_ids(json.loads(tokens)) for tokens in df[\"tokens\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splice dokuments in 512 token chunks\n",
    "\n",
    "# Initialize an empty list to store rows for the new DataFrame\n",
    "new_rows = []\n",
    "\n",
    "# Function to split text and tokens into chunks of 512 tokens\n",
    "def split_text_and_tokens(row):\n",
    "    text = row['text']\n",
    "    tokens_ids = row['token_ids']\n",
    "    filename = row['filename']\n",
    "\n",
    "    if len(tokens_ids) > 512:\n",
    "        # Split into multiple chunks\n",
    "        for i in range(0, len(tokens_ids), 512):\n",
    "            chunk_tokens = tokens_ids[i:i + 512]\n",
    "            chunk_text = tokenizer.decode(chunk_tokens)\n",
    "\n",
    "            # Create a new row with a reference to the original row\n",
    "            new_row = {'filename': filename, 'chunk_id': i/512, 'chunk_text': chunk_text, 'chunk_tokens_json': json.dumps(chunk_tokens)}\n",
    "            new_rows.append(new_row)\n",
    "    else:\n",
    "        # If the row has 512 tokens or fewer, keep it as is\n",
    "        new_row = {'filename': filename, 'chunk_id': 0, 'chunk_text': text, 'chunk_tokens_json': json.dumps(tokens_ids) }\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "# Apply the function to each row in the original DataFrame\n",
    "df.apply(split_text_and_tokens, axis=1)\n",
    "\n",
    "# Create a new DataFrame from the list of new rows\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Reset the index of the new DataFrame if needed\n",
    "new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(new_df.to_markdown())\n",
    "\n",
    "# tokenizer.convert_tokens_to_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus den 2433 Dokumenten die wir eigentlich gescraped haben, sind nun 6945 chunks entstanden es hat sich fast verdreifacht. Wenn man die 787 Seiten ohne Inhalt abzieht, hat sich die Anzahl von 1646 auf 6158 fast vervierfacht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"chunk_tokens_json\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(new_df, \"chunk_word_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "database_path = os.getenv(\"DATABASE_PATH\")\n",
    "\n",
    "def merge_db_tables():\n",
    "    # Connect to your SQLite database\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Create the new table using the structure of the first table (chunk_word_embeddings_0)\n",
    "    cursor.execute('''CREATE TABLE chunk_word_embeddings_all AS SELECT * FROM chunk_word_embeddings_0 WHERE 0''')\n",
    "    # Insert data from the other tables into the new table\n",
    "    for i in range(1, 8):\n",
    "        cursor.execute(f'INSERT INTO chunk_word_embeddings_all SELECT * FROM chunk_word_embeddings_{i}')\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "merge_db_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate embeddings with SentenceTransformer from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "df = db_get_df(\"html_attrs_de\", [\"filename\", \"title\", \"text\"])\n",
    "# Load spaCy for sentence segmentation\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "#TODO speichere embeddings in databank\n",
    "sentences = []\n",
    "embeddings = []\n",
    "\n",
    "for text in df['text']:\n",
    "    # Split the text into sentences using spaCy\n",
    "    doc = nlp(text)\n",
    "    sentence_list = [sent.text for sent in doc.sents]\n",
    "    sentences.append(sentence_list)\n",
    "    \n",
    "    # Encode each sentence using Sentence Transformers\n",
    "    sentence_embeddings = [model.encode(sentence) for sentence in sentence_list]\n",
    "    embeddings.append(sentence_embeddings)\n",
    "    print(embeddings)\n",
    "\n",
    "# Now, 'sentences' is a list where each element is a list of sentences, and 'embeddings' is a list of corresponding sentence embeddings.\n",
    "# filenames = df.head(3)\n",
    "# print(filenames.to_markdown())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
